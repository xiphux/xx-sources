Return-Path: <linux-kernel-owner+akpm=40zip.com.au-S266620AbUFWTdc@vger.kernel.org>
Received: from localhost (bix [127.0.0.1])
	by localhost.localdomain (8.12.10/8.12.10) with ESMTP id i5NJZd5M014767
	for <akpm@localhost>; Wed, 23 Jun 2004 12:35:41 -0700
Received: from bix [127.0.0.1]
	by localhost with POP3 (fetchmail-6.2.0)
	for akpm@localhost (single-drop); Wed, 23 Jun 2004 12:35:41 -0700 (PDT)
Received: by mangalore (mbox akpm)
 (with Cubic Circle's cucipop (v1.31 1998/05/13) Thu Jun 24 05:36:36 2004)
X-From_: linux-kernel-owner+akpm=40zip.com.au-S266620AbUFWTdc@vger.kernel.org  Thu Jun 24 05:35:28 2004
Received: from mailin2.pacific.net.au (mailin2.pacific.net.au [61.8.0.81])
	by mangalore.zipworld.com.au (8.12.3/8.12.3/Debian-6.6) with ESMTP id i5NJZS3R013718
	for <akpm@zip.com.au>; Thu, 24 Jun 2004 05:35:28 +1000
Received: from vger.kernel.org (vger.kernel.org [12.107.209.244])
	by mailin2.pacific.net.au (8.12.3/8.12.3/Debian-6.6) with ESMTP id i5NJZQxg005825
	for <akpm@zip.com.au>; Thu, 24 Jun 2004 05:35:27 +1000
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S266620AbUFWTdc (ORCPT <rfc822;akpm@zip.com.au>);
	Wed, 23 Jun 2004 15:33:32 -0400
Received: (majordomo@vger.kernel.org) by vger.kernel.org id S266618AbUFWTdc
	(ORCPT <rfc822;linux-kernel-outgoing>);
	Wed, 23 Jun 2004 15:33:32 -0400
Received: from fmr03.intel.com ([143.183.121.5]:26496 "EHLO
	hermes.sc.intel.com") by vger.kernel.org with ESMTP id S266628AbUFWTdC
	(ORCPT <rfc822;linux-kernel@vger.kernel.org>);
	Wed, 23 Jun 2004 15:33:02 -0400
Received: from talaria.sc.intel.com (talaria.sc.intel.com [10.3.253.5])
	by hermes.sc.intel.com (8.12.9-20030918-01/8.12.9/d: major-outer.mc,v 1.15 2004/01/30 18:16:28 root Exp $) with ESMTP id i5NJY4N4020781
	for <linux-kernel@vger.kernel.org>; Wed, 23 Jun 2004 19:34:04 GMT
Received: from unix-os.sc.intel.com (unix-os.sc.intel.com [172.25.110.7])
	by talaria.sc.intel.com (8.12.9-20030918-01/8.12.9/d: major-inner.mc,v 1.10 2004/03/01 19:21:36 root Exp $) with ESMTP id i5NJY7pb032222
	for <linux-kernel@vger.kernel.org>; Wed, 23 Jun 2004 19:34:08 GMT
Received: from kwchenmobl (kwchen-mobl.amr.corp.intel.com [143.183.251.218])
	by unix-os.sc.intel.com (8.11.6/8.11.2) with ESMTP id i5NJVeY10040
	for <linux-kernel@vger.kernel.org>; Wed, 23 Jun 2004 12:31:40 -0700
Message-Id: <200406231931.i5NJVeY10040@unix-os.sc.intel.com>
From: "Chen, Kenneth W" <kenneth.w.chen@intel.com>
To: <linux-kernel@vger.kernel.org>
Subject: More bug fix in mm/hugetlb.c - fix try_to_free_low()
Date: Wed, 23 Jun 2004 12:33:00 -0700
X-Mailer: Microsoft Office Outlook, Build 11.0.5510
Thread-Index: AcRZWOSfZF1L1+w9Q9qh2U2PoW6zWA==
X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2800.1409
X-Scanned-By: MIMEDefang 2.31 (www . roaringpenguin . com / mimedefang)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Level: 
X-Spam-Checker-Version: SpamAssassin 2.60 (1.212-2003-09-23-exp) on bix
X-Spam-Status: No, hits=-4.8 required=1.0 tests=BAYES_00,MISSING_OUTLOOK_NAME 
	autolearn=no version=2.60

Is it just me having bad luck with hugetlb for x86 lately?  Take base
2.6.7, turn on CONFIG_HIGHMEM and CONFIG_HUGETLBFS.  Try to config
the hugetlb pool:

[root@quokka]# echo 100 > /proc/sys/vm/nr_hugepages
[root@quokka]# grep HugePage /proc/meminfo
HugePages_Total:   100
HugePages_Free:    100

[root@quokka]# echo 20 > /proc/sys/vm/nr_hugepages
[root@quokka]# grep HugePage /proc/meminfo
HugePages_Total:     0
HugePages_Free:      0

[root@quokka]# echo 100 > /proc/sys/vm/nr_hugepages
[root@quokka]# grep HugePage /proc/meminfo
HugePages_Total:   100
HugePages_Free:    100

[root@quokka]# echo 0 > /proc/sys/vm/nr_hugepages
[root@quokka]# grep HugePage /proc/meminfo
HugePages_Total:    31
HugePages_Free:     31

The argument "count" passed to try_to_free_low() is the config parameter
for desired hugetlb page pool size.  But the implementation took that
input argument as number of pages to free. It also decrement the config
parameter as well.  All give random behavior depend on how many hugetlb
pages are in normal/highmem zone.

A two line fix in try_to_free_low() would be:

-			if (!--count)
-				return 0;
+			if (count >= nr_huge_pages)
+				return count;

But more appropriately, that function shouldn't return anything.

diff -Nurp linux-2.6.7.orig/mm/hugetlb.c linux-2.6.7/mm/hugetlb.c
--- linux-2.6.7.orig/mm/hugetlb.c	2004-06-15 22:19:37.000000000 -0700
+++ linux-2.6.7/mm/hugetlb.c	2004-06-23 12:11:31.000000000 -0700
@@ -130,7 +130,7 @@ static void update_and_free_page(struct
 }

 #ifdef CONFIG_HIGHMEM
-static int try_to_free_low(unsigned long count)
+static void try_to_free_low(unsigned long count)
 {
 	int i;
 	for (i = 0; i < MAX_NUMNODES; ++i) {
@@ -141,16 +141,14 @@ static int try_to_free_low(unsigned long
 			list_del(&page->lru);
 			update_and_free_page(page);
 			--free_huge_pages;
-			if (!--count)
-				return 0;
+			if (count >= nr_huge_pages)
+				return;
 		}
 	}
-	return count;
 }
 #else
-static inline int try_to_free_low(unsigned long count)
+static inline void try_to_free_low(unsigned long count)
 {
-	return count;
 }
 #endif

@@ -170,7 +168,8 @@ static unsigned long set_max_huge_pages(
 		return nr_huge_pages;

 	spin_lock(&hugetlb_lock);
-	for (count = try_to_free_low(count); count < nr_huge_pages; --free_huge_pages) {
+	try_to_free_low(count);
+	for (; count < nr_huge_pages; --free_huge_pages) {
 		struct page *page = dequeue_huge_page();
 		if (!page)
 			break;


-
To unsubscribe from this list: send the line "unsubscribe linux-kernel" in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
