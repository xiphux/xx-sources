---

 linux-2.6.7-rc3-xx5-xiphux/include/linux/sched.h |    4 +-
 linux-2.6.7-rc3-xx5-xiphux/kernel/sched.c        |   41 ++++++++++++++++++-----
 2 files changed, 36 insertions(+), 9 deletions(-)

diff -puN include/linux/sched.h~ck-iso include/linux/sched.h
--- linux-2.6.7-rc3-xx5/include/linux/sched.h~ck-iso	2004-06-17 23:20:58.866913232 -0400
+++ linux-2.6.7-rc3-xx5-xiphux/include/linux/sched.h	2004-06-17 23:20:58.880911104 -0400
@@ -137,8 +137,9 @@ extern unsigned long nr_iowait(void);
 
 #ifdef CONFIG_STAIRCASE
 #define SCHED_BATCH		3
+#define SCHED_ISO		4
 #define SCHED_MIN		0
-#define SCHED_MAX		3
+#define SCHED_MAX		4
 
 #define SCHED_RANGE(policy)	((policy) >= SCHED_MIN && \
 					(policy) <= SCHED_MAX)
@@ -336,6 +337,7 @@ struct signal_struct {
 #endif
 #ifdef CONFIG_STAIRCASE
 #define batch_task(p)		((p)->policy == SCHED_BATCH)
+#define iso_task(p)		((p)->policy == SCHED_ISO)
 #endif
 
 /*
diff -puN kernel/sched.c~ck-iso kernel/sched.c
--- linux-2.6.7-rc3-xx5/kernel/sched.c~ck-iso	2004-06-17 23:20:58.873912168 -0400
+++ linux-2.6.7-rc3-xx5-xiphux/kernel/sched.c	2004-06-17 23:21:21.886413736 -0400
@@ -599,6 +599,18 @@ static inline int preemption_warranted(u
 		return 0;
 }
 #elif defined(CONFIG_STAIRCASE)
+static int rr_interval(task_t * p)
+{
+	int rr_interval = RR_INTERVAL;
+	if (batch_task(p))
+		rr_interval *= 10;
+	else if (iso_task(p))
+		rr_interval /= 2;
+	if (!rr_interval)
+		rr_interval = 1;
+	return rr_interval;
+}
+
 static int task_preempts_curr(struct task_struct *p, runqueue_t *rq)
 {
 	if (p->prio >= rq->curr->prio)
@@ -814,6 +826,8 @@ static unsigned int burst(task_t *p)
 	if (rt_task(p))
 		return p->burst;
 	task_user_prio = TASK_USER_PRIO(p);
+	if (iso_task(p))
+		task_user_prio /= 2;
 	if (likely(task_user_prio < 40))
 		return 39 - task_user_prio;
 	else
@@ -860,7 +874,7 @@ int interactive = 1;
  */
 static int effective_prio(task_t *p)
 {
-	int prio;
+	int prio, rr;
 	unsigned int full_slice, used_slice, first_slice;
 	unsigned int best_burst;
 	if (rt_task(p))
@@ -870,10 +884,11 @@ static int effective_prio(task_t *p)
 
 	best_burst = burst(p);
 	full_slice = slice(p);
+	rr = rr_interval(p);
 	used_slice = full_slice - p->slice;
 	if (p->burst > best_burst)
 		p->burst = best_burst;
-	first_slice = RR_INTERVAL;
+	first_slice = rr;
 	if (interactive && !compute)
 		first_slice *= (p->burst + 1);
 	prio = MAX_PRIO - 2 - best_burst;
@@ -881,7 +896,7 @@ static int effective_prio(task_t *p)
 	if (used_slice < first_slice)
 		return prio;
 	if (p->mm)
-		prio += 1 + (used_slice - first_slice) / RR_INTERVAL;
+		prio += 1 + (used_slice - first_slice) / rr;
 	if (prio > MAX_PRIO - 2)
 		prio = MAX_PRIO - 2;
 	return prio;
@@ -892,7 +907,7 @@ static void recalc_task_prio(task_t *p, 
 	unsigned long sleep_time = now - p->timestamp;
 	unsigned long run_time = NS_TO_JIFFIES(p->runtime);
 	unsigned long total_run = NS_TO_JIFFIES(p->totalrun) + run_time;
-	if (!run_time && NS_TO_JIFFIES(p->runtime + sleep_time) < RR_INTERVAL &&
+	if (!run_time && NS_TO_JIFFIES(p->runtime + sleep_time) < rr_interval(p) &&
 		!batch_task(p)) {
 		if (p->slice - total_run < 1) {
 			p->totalrun = 0;
@@ -1144,7 +1159,7 @@ activate_task(task_t *p, runqueue_t *rq,
 
 #ifdef CONFIG_STAIRCASE
 	p->prio = effective_prio(p);
-	p->time_slice = RR_INTERVAL;
+	p->time_slice = rr_interval(p);
 	if (batch_task(p))
 		p->time_slice *= 10;
 #else
@@ -3085,19 +3100,19 @@ void scheduler_tick(int user_ticks, int 
 		dec_burst(p);
 		p->slice = slice(p);
 		p->prio = effective_prio(p);
-		p->time_slice = RR_INTERVAL;
+		p->time_slice = rr_interval(p);
 		enqueue_task(p, rq);
 		goto out_unlock;
 	}
 	/*
 	 * Tasks that run out of time_slice but still have slice left get
-	 * requeued with a lower priority && RR_INTERVAL time_slice.
+	 * requeued with a lower priority && rr_interval time_slice.
 	 */
 	if (!--p->time_slice) {
 		set_tsk_need_resched(p);
 		dequeue_task(p, rq);
 		p->prio = effective_prio(p);
-		p->time_slice = RR_INTERVAL;
+		p->time_slice = rr_interval(p);
 		enqueue_task(p, rq);
 		goto out_unlock;
 	}
@@ -4103,7 +4118,15 @@ static int setscheduler(pid_t pid, int p
 	(policy == SCHED_FIFO || policy == SCHED_RR) &&
 #endif
 	    !capable(CAP_SYS_NICE))
+#ifdef CONFIG_STAIRCASE
+		/*
+		 * If the caller requested an RT policy without having the
+		 * necessary rights, we downgrade the policy to SCHED_ISO.
+		 */
+		policy = SCHED_ISO;
+#else
 		goto out_unlock;
+#endif
 	if ((current->euid != p->euid) && (current->euid != p->uid) &&
 	    !capable(CAP_SYS_NICE))
 		goto out_unlock;
@@ -4599,6 +4622,7 @@ asmlinkage long sys_sched_get_priority_m
 	case SCHED_NORMAL:
 #ifdef CONFIG_STAIRCASE
 	case SCHED_BATCH:
+	case SCHED_ISO:
 #endif
 		ret = 0;
 		break;
@@ -4625,6 +4649,7 @@ asmlinkage long sys_sched_get_priority_m
 	case SCHED_NORMAL:
 #ifdef CONFIG_STAIRCASE
 	case SCHED_BATCH:
+	case SCHED_ISO:
 #endif
 		ret = 0;
 	}

_
