---

 linux-2.6.7-xx1-xiphux/fs/proc/array.c           |    4 
 linux-2.6.7-xx1-xiphux/include/linux/init_task.h |    7 
 linux-2.6.7-xx1-xiphux/include/linux/sched.h     |    9 
 linux-2.6.7-xx1-xiphux/kernel/sched.c            |  315 ++++++++++++++++++-----
 4 files changed, 273 insertions(+), 62 deletions(-)

diff -puN fs/proc/array.c~spa-ia-bonus-v0.1 fs/proc/array.c
--- linux-2.6.7-xx1/fs/proc/array.c~spa-ia-bonus-v0.1	2004-06-22 05:28:48.986970920 -0400
+++ linux-2.6.7-xx1-xiphux/fs/proc/array.c	2004-06-22 05:28:49.060959672 -0400
@@ -155,7 +155,9 @@ static inline char * task_state(struct t
 	read_lock(&tasklist_lock);
 	buffer += sprintf(buffer,
 		"State:\t%s\n"
+#ifndef CONFIG_SPA
 		"SleepAVG:\t%lu%%\n"
+#endif
 		"Tgid:\t%d\n"
 		"Pid:\t%d\n"
 		"PPid:\t%d\n"
@@ -163,7 +165,9 @@ static inline char * task_state(struct t
 		"Uid:\t%d\t%d\t%d\t%d\n"
 		"Gid:\t%d\t%d\t%d\t%d\n",
 		get_task_state(p),
+#ifndef CONFIG_SPA
 		(p->sleep_avg/1024)*100/(1020000000/1024),
+#endif
 	       	p->tgid,
 		p->pid, p->pid ? p->real_parent->pid : 0,
 		p->pid && p->ptrace ? p->parent->pid : 0,
diff -puN include/linux/init_task.h~spa-ia-bonus-v0.1 include/linux/init_task.h
--- linux-2.6.7-xx1/include/linux/init_task.h~spa-ia-bonus-v0.1	2004-06-22 05:28:49.052960888 -0400
+++ linux-2.6.7-xx1-xiphux/include/linux/init_task.h	2004-06-22 05:28:49.061959520 -0400
@@ -73,6 +73,12 @@ extern struct group_info init_groups;
 #define SCHED_STATIC_PRIO .static_prio = MAX_PRIO-20,
 #define SCHED_TIME_SLICE .time_slice = HZ,
 
+#ifdef CONFIG_SPA
+#define SCHED_TIMESTAMP .sched_timestamp = ((INITIAL_JIFFIES * NSEC_PER_SEC) / HZ),
+#else
+#define SCHED_TIMESTAMP
+#endif
+
 #define INIT_TASK(tsk)	\
 {									\
 	.state		= 0,						\
@@ -121,6 +127,7 @@ extern struct group_info init_groups;
 	.proc_lock	= SPIN_LOCK_UNLOCKED,				\
 	.switch_lock	= SPIN_LOCK_UNLOCKED,				\
 	.journal_info	= NULL,						\
+	SCHED_TIMESTAMP							\
 }
 
 
diff -puN include/linux/sched.h~spa-ia-bonus-v0.1 include/linux/sched.h
--- linux-2.6.7-xx1/include/linux/sched.h~spa-ia-bonus-v0.1	2004-06-22 05:28:49.054960584 -0400
+++ linux-2.6.7-xx1-xiphux/include/linux/sched.h	2004-06-22 05:28:49.064959064 -0400
@@ -419,10 +419,19 @@ struct task_struct {
 	prio_array_t *array;
 #endif
 
+#ifdef CONFIG_SPA
+	u64 timestamp;
+	u64 sched_timestamp;
+	u64 avg_sleep_per_cycle;
+	u64 avg_delay_per_cycle;
+	u64 avg_cpu_per_cycle;
+	unsigned int interactive_bonus, sub_cycle_count;
+#else
 	unsigned long sleep_avg;
 	long interactive_credit;
 	unsigned long long timestamp;
 	int activated;
+#endif
 
 	unsigned long policy;
 	cpumask_t cpus_allowed;
diff -puN kernel/sched.c~spa-ia-bonus-v0.1 kernel/sched.c
--- linux-2.6.7-xx1/kernel/sched.c~spa-ia-bonus-v0.1	2004-06-22 05:28:49.056960280 -0400
+++ linux-2.6.7-xx1-xiphux/kernel/sched.c	2004-06-22 05:29:17.712603960 -0400
@@ -16,8 +16,9 @@
  *		by Davide Libenzi, preemptible kernel bits by Robert Love.
  *  2003-09-03	Interactivity tuning by Con Kolivas.
  *  2004-04-02	Scheduler domains code by Nick Piggin
- *  2004-06-03	Single priority array by Peter Williams
- * 		(Courtesy of Aurema Pty Ltd, www.aurema.com)
+ *  2004-06-03	Single priority array and simplified interactive bonus
+ *		mechanism by Peter Williams
+ *		(Courtesy of Aurema Pty Ltd, www.aurema.com)
  */
 
 #include <linux/mm.h>
@@ -80,45 +81,77 @@
 			(MAX_PRIO-1-NICE_TO_PRIO(0))/(MAX_USER_PRIO - 1)))
 #endif
 
+#ifndef CONFIG_SPA
 /*
  * Some helpers for converting nanosecond timing to jiffy resolution
  */
 #define NS_TO_JIFFIES(TIME)	((TIME) / (1000000000 / HZ))
 #define JIFFIES_TO_NS(TIME)	((TIME) * (1000000000 / HZ))
+#endif
 
 #ifdef CONFIG_SPA
 /*
  * These are the 'tuning knobs' of the scheduler:
  */
-#define ON_RUNQUEUE_WEIGHT	 30
-#define CHILD_PENALTY		 95
-#define PARENT_PENALTY		100
-#define PRIO_BONUS_RATIO	 25
-#define MAX_BONUS		(MAX_USER_PRIO * PRIO_BONUS_RATIO / 100)
-#define INTERACTIVE_DELTA	  2
-#define MAX_SLEEP_AVG		(TIME_SLICE_TICKS * MAX_BONUS)
-#define NS_MAX_SLEEP_AVG	(JIFFIES_TO_NS(MAX_SLEEP_AVG))
-#define CREDIT_LIMIT		100
+#define MAX_IA_BONUS 10
+#define MAX_BONUS MAX_IA_BONUS
 
-#define CURRENT_BONUS(p) \
-	(NS_TO_JIFFIES((p)->sleep_avg) * MAX_BONUS / \
-		MAX_SLEEP_AVG)
+/*
+ * Define some mini Kalman filter for estimating various averages, etc.
+ * To make it more efficient the denominator of the fixed point rational
+ * numbers used to store the averages and the response half life will
+ * be chosen so that the fixed point rational number reperesentation
+ * of (1 - alpha) * i (where i is an integer) will be i.
+ * Some of this is defined in linux/sched.h
+ */
 
-#define SCALE(v1,v1_max,v2_max) \
-	(v1) * (v2_max) / (v1_max)
+/*
+ * Fixed denominator rational numbers for use by the CPU scheduler
+ */
+#define SCHED_AVG_OFFSET 8
+/*
+ * Get the rounded integer value of a scheduling statistic average field
+ * i.e. those fields whose names begin with avg_
+ */
+#define SCHED_AVG_RND(x) \
+	(((x) + (1 << (SCHED_AVG_OFFSET - 1))) >> (SCHED_AVG_OFFSET))
+#define SCHED_AVG_ALPHA ((1 << SCHED_AVG_OFFSET) - 1)
+#define SCHED_AVG_MUL(a, b) (((a) * (b)) >> SCHED_AVG_OFFSET)
+#define SCHED_AVG_REAL(a) ((a) << SCHED_AVG_OFFSET)
+#define SCHED_IA_BONUS_OFFSET 8
+#define SCHED_IA_BONUS_ALPHA ((1 << SCHED_IA_BONUS_OFFSET) - 1)
+#define SCHED_IA_BONUS_MUL(a, b) (((a) * (b)) >> SCHED_IA_BONUS_OFFSET)
+/*
+ * Get the rounded integer value of the interactive bonus
+ */
+#define SCHED_IA_BONUS_RND(x) \
+	(((x) + (1 << (SCHED_IA_BONUS_OFFSET - 1))) >> (SCHED_IA_BONUS_OFFSET))
 
-#define DELTA(p) \
-	(SCALE(TASK_NICE(p), 40, MAX_BONUS) + INTERACTIVE_DELTA)
+static inline void apply_sched_avg_decay(u64 *valp)
+{
+	*valp = SCHED_AVG_MUL(*valp, SCHED_AVG_ALPHA);
+}
 
-#define INTERACTIVE_SLEEP(p) \
-	(JIFFIES_TO_NS(MAX_SLEEP_AVG * \
-		(MAX_BONUS / 2 + DELTA((p)) + 1) / MAX_BONUS - 1))
+static inline void update_sched_ia_bonus(struct task_struct *p, unsigned int incr)
+{
+	p->interactive_bonus = SCHED_AVG_MUL(p->interactive_bonus, SCHED_AVG_ALPHA);
+	p->interactive_bonus += incr;
+}
 
-#define HIGH_CREDIT(p) \
-	((p)->interactive_credit > CREDIT_LIMIT)
+#define CURRENT_BONUS(p) SCHED_IA_BONUS_RND((p)->interactive_bonus)
 
-#define LOW_CREDIT(p) \
-	((p)->interactive_credit < -CREDIT_LIMIT)
+/*
+ * Tasks with a CPU usage rate greater than 50% are considered to be CPU bound
+ */
+#define SCHED_CPU_BOUND_THRESHOLD ((MAX_IA_BONUS * (100 - 50)) / 100)
+/*
+ * Tasks that would sleep for more than 90% of the time if they had the CPU to
+ * themselves are considered to be interactive provided that their average
+ * sleep duration per scheduling cycle isn't too long
+ */
+#define SCHED_IA_THRESHOLD ((MAX_IA_BONUS * 90) / 100)
+#define LOWER_MAX_IA_SLEEP SCHED_AVG_REAL(15 * 60LL * NSEC_PER_SEC)
+#define UPPER_MAX_IA_SLEEP SCHED_AVG_REAL(2 * 60 * 60LL * NSEC_PER_SEC)
 
 #define PRIO_PREEMPTS_CURR(p, rq) \
 	((p) < (rq)->current_prio_slot->prio)
@@ -682,6 +715,92 @@ static inline void __activate_idle_task(
 }
 #endif
 
+#ifdef CONFIG_SPA
+/*
+ * Update various statistics for the end of a
+ * ((on_run_queue :-> on_cpu)* :-> sleep) cycle.
+ * We can't just do this in activate_task() as every invocation of that
+ * function is not the genuine end of a cycle.
+ */
+static void update_stats_for_cycle(task_t *p, const runqueue_t *rq)
+{
+	apply_sched_avg_decay(&p->avg_delay_per_cycle);
+	apply_sched_avg_decay(&p->avg_cpu_per_cycle);
+	p->avg_sleep_per_cycle += (rq->timestamp_last_tick - p->sched_timestamp);
+	/*
+	 * Do this second so that averages for all measures are for
+	 * the current cycle
+	 */
+	apply_sched_avg_decay(&p->avg_sleep_per_cycle);
+	p->sched_timestamp = rq->timestamp_last_tick;
+	p->sub_cycle_count = 0;
+}
+
+#if BITS_PER_LONG < 64
+/*
+ * Assume that there's no 64 bit divide available
+ */
+static inline unsigned long sched_div_64(u64 a, u64 b)
+{
+	if (a < b)
+		return 0;
+	/*
+	 * Scale a and b down to less than 32 bits so that we can do a divide
+	 */
+	while (a > ULONG_MAX) { a >>= 1; b >>= 1; }
+
+	return ((unsigned long)a / (unsigned long)b);
+}
+#else
+#define sched_div_64(a, b) ((a) / (b))
+#endif
+#define BONUS_RND(mb, e, d) \
+	((unsigned int)sched_div_64((e) * ((mb) * 2 + 1), ((e) + (d)) * 2))
+
+static void reassess_cpu_boundness(task_t *p)
+{
+	unsigned int bonus;
+	u64 off_cpu_avg;
+
+	/*
+	 * No cpu use means not cpu bound
+	 * NB this also prevents divide by zero later if cpu is also zero
+	 */
+	if (p->avg_cpu_per_cycle == 0)
+		return;
+	off_cpu_avg = p->avg_sleep_per_cycle + p->avg_delay_per_cycle;
+	bonus = BONUS_RND(MAX_IA_BONUS, off_cpu_avg, p->avg_cpu_per_cycle);
+	if (bonus < SCHED_CPU_BOUND_THRESHOLD)
+		update_sched_ia_bonus(p, bonus);
+}
+
+static void reassess_interactiveness(task_t *p)
+{
+	unsigned int bonus;
+
+	/*
+	 * No sleep means not interactive (in most cases), but
+	 * NB this also prevents divide by zero later if cpu is also zero
+	 */
+	if (p->avg_sleep_per_cycle == 0) {
+		if (p->avg_cpu_per_cycle == 0)
+			update_sched_ia_bonus(p, MAX_IA_BONUS);
+		return;
+	} else if (p->avg_sleep_per_cycle > LOWER_MAX_IA_SLEEP) {
+		/*
+		 * Really long sleeps mean it's probably not interactive
+		 */
+		if (p->avg_sleep_per_cycle > UPPER_MAX_IA_SLEEP)
+			update_sched_ia_bonus(p, 0);
+		return;
+	}
+	bonus = BONUS_RND(MAX_IA_BONUS, p->avg_sleep_per_cycle, p->avg_cpu_per_cycle);
+	if (bonus > SCHED_IA_THRESHOLD)
+		update_sched_ia_bonus(p, bonus);
+	else if (p->sub_cycle_count == 0)
+		reassess_cpu_boundness(p);
+}
+#else
 static void recalc_task_prio(task_t *p, unsigned long long now)
 {
 	unsigned long long __sleep_time = now - p->timestamp;
@@ -702,11 +821,7 @@ static void recalc_task_prio(task_t *p, 
 		if (p->mm && p->activated != -1 &&
 			sleep_time > INTERACTIVE_SLEEP(p)) {
 				p->sleep_avg = JIFFIES_TO_NS(MAX_SLEEP_AVG -
-#ifdef CONFIG_SPA
-						TIME_SLICE_TICKS
-#else
 						AVG_TIMESLICE
-#endif
 						);
 				if (!HIGH_CREDIT(p))
 					p->interactive_credit++;
@@ -757,10 +872,9 @@ static void recalc_task_prio(task_t *p, 
 			}
 		}
 	}
-#ifndef CONFIG_SPA
 	p->prio = effective_prio(p);
-#endif
 }
+#endif
 
 /*
  * activate_task - move a task to the runqueue and do priority recalculation
@@ -777,7 +891,7 @@ void
 activate_task(task_t *p, runqueue_t *rq, int local)
 {
 #ifdef CONFIG_SPA
-	int prio;
+	int prio = effective_prio(p);
 #endif
 	unsigned long long now;
 
@@ -791,10 +905,8 @@ activate_task(task_t *p, runqueue_t *rq,
 	}
 #endif
 
+#ifndef CONFIG_SPA
 	recalc_task_prio(p, now);
-#ifdef CONFIG_SPA
-	prio = effective_prio(p);
-#endif
 
 	/*
 	 * This checks to make sure it's not an uninterruptible task
@@ -818,6 +930,7 @@ activate_task(task_t *p, runqueue_t *rq,
 			p->activated = 1;
 		}
 	}
+#endif
 	p->timestamp = now;
 
 #ifdef CONFIG_SPA
@@ -1200,13 +1313,22 @@ out_activate:
 #endif /* CONFIG_SMP */
 	if (old_state == TASK_UNINTERRUPTIBLE) {
 		rq->nr_uninterruptible--;
+#ifndef CONFIG_SPA
 		/*
 		 * Tasks on involuntary sleep don't earn
 		 * sleep_avg beyond just interactive state.
 		 */
 		p->activated = -1;
+#endif
 	}
-
+#ifdef CONFIG_SPA
+	/*
+	 * This is the end of one scheduling cycle and the start
+	 * of the next
+	 */
+	update_stats_for_cycle(p, rq);
+	reassess_interactiveness(p);
+#endif
 	/*
 	 * Sync wakeups (i.e. those types of wakeups where the waker
 	 * has indicated that it will leave the CPU in short order)
@@ -1283,6 +1405,15 @@ void fastcall sched_fork(task_t *p)
 	 * Give the child a new timeslice
 	 */
 	p->time_slice = task_timeslice(p);
+	/*
+	 * Initialize the scheduler statistics counters
+	 */
+	p->sched_timestamp = 0 /* set this to current time later */;
+	p->avg_sleep_per_cycle = 0;
+	p->avg_delay_per_cycle = 0;
+	p->avg_cpu_per_cycle = 0;
+	p->interactive_bonus = 0;
+	p->sub_cycle_count = 0;
 #else
 	/*
 	 * Share the timeslice between parent and child, thus the
@@ -1327,6 +1458,21 @@ void fastcall wake_up_forked_process(tas
 
 	BUG_ON(p->state != TASK_RUNNING);
 
+#ifdef CONFIG_SPA
+	set_task_cpu(p, smp_processor_id());
+
+	/*
+	 * Scheduling statistics compilation starts now
+	 */
+	p->sched_timestamp = rq->timestamp_last_tick;
+	/*
+	 * Now that the idle task is back on the run queue we need extra care
+	 * to make sure that its one and only fork() doesn't end up in the idle
+	 * priority slot.  Just testing for empty run list is no longer adequate.
+	 */
+	if (unlikely(!task_queued(current) || RUNQUEUE_IDLE(rq)))
+		__activate_task(p, rq, effective_prio(p));
+#else
 	/*
 	 * We decrease the sleep average of forking parents
 	 * and children as well, to keep max-interactive tasks
@@ -1338,22 +1484,11 @@ void fastcall wake_up_forked_process(tas
 	p->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(p) *
 		CHILD_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
 
-#ifndef CONFIG_SPA
 	p->interactive_credit = 0;
-#endif
 
 	p->prio = effective_prio(p);
 	set_task_cpu(p, smp_processor_id());
 
-#ifdef CONFIG_SPA
-	/*
-	 * Now that the idle task is back on the run queue we need extra care
-	 * to make sure that its one and only fork() doesn't end up in the idle
-	 * priority slot.  Just testing for empty run list is no longer adequate.
-	 */
-	if (unlikely(!task_queued(current) || RUNQUEUE_IDLE(rq)))
-		__activate_task(p, rq, effective_prio(p));
-#else
 	if (unlikely(!current->array))
 		__activate_task(p, rq);
 #endif
@@ -1670,6 +1805,7 @@ lock_again:
 		double_rq_unlock(this_rq, rq);
 		goto lock_again;
 	}
+#ifndef CONFIG_SPA
 	/*
 	 * We decrease the sleep average of forking parents
 	 * and children as well, to keep max-interactive tasks
@@ -1683,11 +1819,17 @@ lock_again:
 
 	p->interactive_credit = 0;
 
-#ifndef CONFIG_SPA
 	p->prio = effective_prio(p);
 #endif
 	set_task_cpu(p, cpu);
 
+#ifdef CONFIG_SPA
+	/*
+	 * Scheduling statistics compilation starts now
+	 */
+	p->sched_timestamp = rq->timestamp_last_tick;
+#endif
+
 	if (cpu == this_cpu) {
 #ifdef CONFIG_SPA
 		if (unlikely(!task_queued(current) || RUNQUEUE_IDLE(rq)))
@@ -1836,9 +1978,13 @@ void pull_task(runqueue_t *src_rq,
 	dequeue_task(p, src_array);
 #endif
 	src_rq->nr_running--;
+#ifdef CONFIG_SPA
+	p->avg_delay_per_cycle += (src_rq->timestamp_last_tick - p->sched_timestamp);
+#endif
 	set_task_cpu(p, this_cpu);
 	this_rq->nr_running++;
 #ifdef CONFIG_SPA
+	p->sched_timestamp = this_rq->timestamp_last_tick;
 	enqueue_task(p, this_rq, prio);
 #else
 	enqueue_task(p, this_array);
@@ -2648,6 +2794,15 @@ void scheduler_tick(int user_ticks, int 
 #endif
 		p->time_slice = task_timeslice(p);
 #ifdef CONFIG_SPA
+		p->avg_cpu_per_cycle += (rq->timestamp_last_tick - p->sched_timestamp);
+		p->sched_timestamp = rq->timestamp_last_tick;
+		reassess_cpu_boundness(p);
+		/*
+		 * Arguably the interactive bonus should be updated here
+		 * as well.  But depends on whether we wish to encourage
+		 * interactive tasks to maintain a high bonus or CPU bound
+		 * tasks to lose some of there bonus?
+		 */
 		rq->current_prio_slot = rq->queues + effective_prio(p);
 		enqueue_task(p, rq, rq->current_prio_slot->prio);
 #else
@@ -2784,6 +2939,12 @@ static inline int dependent_sleeper(int 
 	}
 	return ret;
 }
+#ifdef CONFIG_SPA
+static inline int dependent_idle(const runqueue_t *rq, const task_t *p)
+{
+	return p == rq->idle;
+}
+#endif
 #else
 static inline void wake_sleeping_dependent(int cpu, runqueue_t *rq)
 {
@@ -2793,6 +2954,12 @@ static inline int dependent_sleeper(int 
 {
 	return 0;
 }
+#ifdef CONFIG_SPA
+static inline int dependent_idle(const runqueue_t *rq, const task_t *p)
+{
+	return 0;
+}
+#endif
 #endif
 
 /*
@@ -2806,9 +2973,9 @@ asmlinkage void __sched schedule(void)
 #ifndef CONFIG_SPA
 	prio_array_t *array;
 	struct list_head *queue;
-#endif
 	unsigned long long now;
 	unsigned long run_time;
+#endif
 	int cpu;
 #ifndef CONFIG_SPA
 	int idx;
@@ -2833,6 +3000,7 @@ need_resched:
 
 	release_kernel_lock(prev);
  	schedstat_inc(rq, sched_cnt);
+#ifndef CONFIG_SPA
 	now = sched_clock();
 	if (likely(now - prev->timestamp < NS_MAX_SLEEP_AVG))
 		run_time = now - prev->timestamp;
@@ -2846,7 +3014,7 @@ need_resched:
 	 */
 	if (HIGH_CREDIT(prev))
 		run_time /= (CURRENT_BONUS(prev) ? : 1);
-
+#endif
 	spin_lock_irq(&rq->lock);
 
 	/*
@@ -2885,7 +3053,7 @@ need_resched:
 #ifdef CONFIG_SPA
 	rq->current_prio_slot = rq->queues + sched_find_first_bit(rq->bitmap);
 	next = list_entry(rq->current_prio_slot->queue.next, task_t, run_list);
-	if (next == rq->idle) {
+	if (dependent_idle(rq, next)) {
 		wake_sleeping_dependent(cpu, rq);
 		goto switch_tasks;
 	}
@@ -2916,27 +3084,32 @@ need_resched:
 		goto switch_tasks;
 	}
 
+#ifndef CONFIG_SPA
 	if (!rt_task(next) && next->activated > 0) {
 		unsigned long long delta = now - next->timestamp;
 
 		if (next->activated == 1)
 			delta = delta * (ON_RUNQUEUE_WEIGHT * 128 / 100) / 128;
 
-#ifndef CONFIG_SPA
 		array = next->array;
 		dequeue_task(next, array);
-#endif
 		recalc_task_prio(next, next->timestamp + delta);
-#ifndef CONFIG_SPA
 		enqueue_task(next, array);
-#endif
 	}
 	next->activated = 0;
+#endif
 switch_tasks:
 	prefetch(next);
 	clear_tsk_need_resched(prev);
 	RCU_qsctr(task_cpu(prev))++;
 
+#ifdef CONFIG_SPA
+	/*
+	 * Update estimate of average CPU time used per cycle
+	 */
+	prev->avg_cpu_per_cycle += (rq->timestamp_last_tick - prev->sched_timestamp);
+	prev->timestamp = prev->sched_timestamp = rq->timestamp_last_tick;
+#else
 	prev->sleep_avg -= run_time;
 	if ((long)prev->sleep_avg <= 0) {
 		prev->sleep_avg = 0;
@@ -2944,9 +3117,18 @@ switch_tasks:
 			prev->interactive_credit--;
 	}
 	prev->timestamp = now;
+#endif
 
 	if (likely(prev != next)) {
+#ifdef CONFIG_SPA
+		/*
+		 * Update estimate of average delay on run queue per cycle
+		 */
+		next->avg_delay_per_cycle += (rq->timestamp_last_tick - next->sched_timestamp);
+		next->timestamp = next->sched_timestamp = rq->timestamp_last_tick;
+#else
 		next->timestamp = now;
+#endif
 		rq->nr_switches++;
 		rq->curr = next;
 		++*switch_count;
@@ -4020,12 +4202,16 @@ void __devinit init_idle(task_t *idle, i
 	deactivate_task(idle, rq);
 #ifdef CONFIG_SPA
 	/*
-	 * Setting "activated" to zero will reduce testing required in schedule()
-	 */
-	idle->activated = 0;
-	/*
-	 * Should be no need to initialise other EBS fields as they shouldn't be used
-	 */
+	 * Initialize scheduling statistics counters as they may provide
+	 * valuable about the CPU e.g. avg_cpu_time_per_cycle for the idle
+	 * task will be an estimate of the average time the CPU is idle
+	 */
+	idle->sched_timestamp = rq->timestamp_last_tick;
+	idle->avg_sleep_per_cycle = 0;
+	idle->avg_delay_per_cycle = 0;
+	idle->avg_cpu_per_cycle = 0;
+	idle->interactive_bonus = 0;
+	idle->sub_cycle_count = 0;
 #else
 	idle->array = NULL;
 	idle->prio = MAX_PRIO;
@@ -4170,6 +4356,7 @@ static void __migrate_task(struct task_s
 		deactivate_task(p, rq_src);
 #ifdef CONFIG_SPA
 		set_task_cpu(p, dest_cpu);
+		p->avg_delay_per_cycle += (rq_dest->timestamp_last_tick - p->sched_timestamp);
 		if (PRIO_PREEMPTS_CURR(activate_task(p, rq_dest, 0), rq_dest))
 #else
 		activate_task(p, rq_dest, 0);
@@ -4178,8 +4365,11 @@ static void __migrate_task(struct task_s
 			resched_task(rq_dest->curr);
 	}
 #ifdef CONFIG_SPA
-	else
+	else {
 		set_task_cpu(p, dest_cpu);
+		p->avg_sleep_per_cycle += (rq_dest->timestamp_last_tick - p->sched_timestamp);
+	}
+	p->sched_timestamp = rq_dest->timestamp_last_tick;
 #endif
 
 out:
@@ -4758,6 +4948,7 @@ void __init sched_init(void)
 		// delimiter for bitsearch
 		__set_bit(IDLE_PRIO, rq->bitmap);
 		rq->current_prio_slot = rq->queues + (IDLE_PRIO - 20);
+		rq->timestamp_last_tick = sched_clock();
 		rq->next_prom_due = (jiffies + get_prom_interval(rq));
 #else
 		for (j = 0; j < 2; j++) {

_
