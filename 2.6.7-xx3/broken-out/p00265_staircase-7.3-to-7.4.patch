---

 linux-2.6.7-xx3-xiphux/include/linux/sched.h |    4 +++
 linux-2.6.7-xx3-xiphux/kernel/sched.c        |   33 ++++++++++++++++-----------
 2 files changed, 24 insertions(+), 13 deletions(-)

diff -puN include/linux/sched.h~staircase-7.3-to-7.4 include/linux/sched.h
--- linux-2.6.7-xx3/include/linux/sched.h~staircase-7.3-to-7.4	2004-06-24 18:16:31.733214280 -0400
+++ linux-2.6.7-xx3-xiphux/include/linux/sched.h	2004-06-24 18:17:34.629652568 -0400
@@ -631,6 +631,10 @@ do { if (atomic_dec_and_test(&(tsk)->usa
 #define PF_SWAPOFF	0x00080000	/* I am in swapoff */
 #define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */
 #define PF_SYNCWRITE	0x00200000	/* I am doing a sync write */
+#ifdef CONFIG_STAIRCASE
+#define PF_FORKED	0x00400000	/* I have just forked */
+#define PF_PREEMPTED	0x00800000	/* I have just been preempted */
+#endif
 
 #ifdef CONFIG_SMP
 #define SCHED_LOAD_SCALE	128UL	/* increase resolution of load */
diff -puN kernel/sched.c~staircase-7.3-to-7.4 kernel/sched.c
--- linux-2.6.7-xx3/kernel/sched.c~staircase-7.3-to-7.4	2004-06-24 18:16:34.442802360 -0400
+++ linux-2.6.7-xx3-xiphux/kernel/sched.c	2004-06-24 18:25:15.739553208 -0400
@@ -617,8 +617,10 @@ static int task_preempts_curr(struct tas
 		return 0;
 	if (!compute || rq->cache_ticks >= cache_decay_ticks ||
 		rt_task(p) || !p->mm || rq->curr == rq->idle ||
-		(batch_task(rq->curr) && !batch_task(p)))
+		(batch_task(rq->curr) && !batch_task(p))) {
+			rq->curr->flags |= PF_PREEMPTED;
 			return 1;
+	}
 	rq->preempted = 1;
 		return 0;
 }
@@ -684,7 +686,11 @@ static void enqueue_task(struct task_str
 		)
 {
 #ifdef CONFIG_STAIRCASE
-	list_add_tail(&p->run_list, rq->queue + p->prio);
+	if (rq->curr->flags & PF_PREEMPTED) {
+		rq->curr->flags &= ~PF_PREEMPTED;
+		list_add(&p->run_list, rq->queue + p->prio);
+	} else
+		list_add_tail(&p->run_list, rq->queue + p->prio);
 	__set_bit(p->prio, rq->bitmap);
 #elif defined(CONFIG_SPA)
 	list_add_tail(&p->run_list, &rq->queues[prio].queue);
@@ -906,15 +912,17 @@ static void recalc_task_prio(task_t *p, 
 	unsigned long sleep_time = now - p->timestamp;
 	unsigned long run_time = NS_TO_JIFFIES(p->runtime);
 	unsigned long total_run = NS_TO_JIFFIES(p->totalrun) + run_time;
-	if (!run_time && NS_TO_JIFFIES(p->runtime + sleep_time) < rr_interval(p) &&
+	if (((!run_time && NS_TO_JIFFIES(p->runtime + sleep_time) < rr_interval(p))
+		|| p->flags & PF_FORKED) &&
 		!batch_task(p)) {
-		if (p->slice - total_run < 1) {
-			p->totalrun = 0;
-			dec_burst(p);
-		} else {
-			p->totalrun += p->runtime;
-			p->slice -= NS_TO_JIFFIES(p->totalrun);
-		}
+			p->flags &= ~PF_FORKED;
+			if (p->slice - total_run < 1) {
+				p->totalrun = 0;
+				dec_burst(p);
+			} else {
+				p->totalrun += p->runtime;
+				p->slice -= NS_TO_JIFFIES(p->totalrun);
+			}
 	} else {
 		inc_burst(p);
 		p->runtime = 0;
@@ -1741,10 +1749,8 @@ void fastcall wake_up_forked_process(tas
 
 #ifdef CONFIG_STAIRCASE
 	/*
-	 * Forked process and it's parent have their burst dropped to prevent
-	 * priority elevation by forking and becoming fork bombs.
+	 * Forked process gets no burst to prevent fork bombs.
 	 */
-	current->burst = 0;
 	p->burst = 0;
 #endif
 
@@ -1753,6 +1759,7 @@ void fastcall wake_up_forked_process(tas
 #ifdef CONFIG_STAIRCASE
 	set_task_cpu(p, smp_processor_id());
 	__activate_task(p, rq);
+	current->flags |= PF_FORKED;
 #else
 #ifdef CONFIG_SPA
 	set_task_cpu(p, smp_processor_id());

_
