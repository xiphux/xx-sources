diff -uNr linux-2.6.8.1-mm4/include/linux/fs.h linux-2.6.8.1-mm4-cachefs/include/linux/fs.h
--- linux-2.6.8.1-mm4/include/linux/fs.h	2004-08-23 13:09:06.000000000 +0100
+++ linux-2.6.8.1-mm4-cachefs/include/linux/fs.h	2004-08-25 19:39:35.000000000 +0100
@@ -328,6 +328,10 @@
 	int (*releasepage) (struct page *, int);
 	ssize_t (*direct_IO)(int, struct kiocb *, const struct iovec *iov,
 			loff_t offset, unsigned long nr_segs);
+
+	/* notification that a previously read-only page is about to become
+	 * writable, if an error is returned it will cause a SIGBUS */
+	int (*page_becoming_writable)(struct page *page);
 };
 
 struct backing_dev_info;
diff -uNr linux-2.6.8.1-mm4/mm/memory.c linux-2.6.8.1-mm4-cachefs/mm/memory.c
--- linux-2.6.8.1-mm4/mm/memory.c	2004-08-23 13:09:09.000000000 +0100
+++ linux-2.6.8.1-mm4-cachefs/mm/memory.c	2004-08-25 19:40:52.000000000 +0100
@@ -1084,12 +1084,43 @@
 		int reuse = can_share_swap_page(old_page);
 		unlock_page(old_page);
 		if (reuse) {
+			if (vma->vm_file) {
+				struct address_space *mapping;
+
+				mapping = vma->vm_file->f_mapping;
+
+				/* if the page will be shareable, see if the
+				 * backing address space wants to know that the
+				 * page is about to become writable */
+				if (!mapping ||
+				    !mapping->a_ops ||
+				    !mapping->a_ops->page_becoming_writable)
+					goto no_wp_notification;
+
+				spin_unlock(&mm->page_table_lock);
+				if (mapping->a_ops->page_becoming_writable(old_page) < 0)
+					return VM_FAULT_SIGBUS;
+
+				spin_lock(&mm->page_table_lock);
+
+				/* need to revalidate the PTE
+				 * - if someone else changed it whilst the lock
+				 *   was not held, then we just return
+				 */
+				if (!pte_same(*page_table, pte))
+					goto minor_fault;
+
+			no_wp_notification:
+				;
+			}
+
 			flush_cache_page(vma, address);
 			entry = maybe_mkwrite(pte_mkyoung(pte_mkdirty(pte)),
 					      vma);
 			ptep_set_access_flags(vma, address, page_table, entry, 1);
 			update_mmu_cache(vma, address, entry);
 			pte_unmap(page_table);
+		minor_fault:
 			spin_unlock(&mm->page_table_lock);
 			return VM_FAULT_MINOR;
 		}
@@ -1531,18 +1562,30 @@
 	/*
 	 * Should we do an early C-O-W break?
 	 */
-	if (write_access && !(vma->vm_flags & VM_SHARED)) {
-		struct page *page;
+	if (write_access) {
+		if (!(vma->vm_flags & VM_SHARED)) {
+			struct page *page;
 
-		if (unlikely(anon_vma_prepare(vma)))
-			goto oom;
-		page = alloc_page_vma(GFP_HIGHUSER, vma, address);
-		if (!page)
-			goto oom;
-		copy_user_highpage(page, new_page, address);
-		page_cache_release(new_page);
-		new_page = page;
-		anon = 1;
+			if (unlikely(anon_vma_prepare(vma)))
+				goto oom;
+			page = alloc_page_vma(GFP_HIGHUSER, vma, address);
+			if (!page)
+				goto oom;
+			copy_user_highpage(page, new_page, address);
+			page_cache_release(new_page);
+			new_page = page;
+			anon = 1;
+		}
+		else {
+			/* if the page will be shareable, see if the backing
+			 * address space wants to know that the page is about
+			 * to become writable */
+			if (mapping &&
+			    mapping->a_ops &&
+			    mapping->a_ops->page_becoming_writable &&
+			    mapping->a_ops->page_becoming_writable(new_page) < 0)
+				return VM_FAULT_SIGBUS;
+		}
 	}
 
 	spin_lock(&mm->page_table_lock);
