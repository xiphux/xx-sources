---

 xx-sources-current-xiphux/fs/proc/array.c           |   10 
 xx-sources-current-xiphux/include/linux/init_task.h |   25 
 xx-sources-current-xiphux/include/linux/sched.h     |   25 
 xx-sources-current-xiphux/include/linux/sysctl.h    |    1 
 xx-sources-current-xiphux/kernel/Kconfig-extra.xx   |   26 
 xx-sources-current-xiphux/kernel/sched.c            |  551 +++++++++++++++++++-
 xx-sources-current-xiphux/kernel/sysctl.c           |   18 
 xx-sources-current-xiphux/mm/oom_kill.c             |    2 
 8 files changed, 634 insertions(+), 24 deletions(-)

diff -puN fs/proc/array.c~nicksched fs/proc/array.c
--- xx-sources-current/fs/proc/array.c~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/fs/proc/array.c	2004-08-10 10:48:07.045077712 -0400
@@ -159,7 +159,13 @@ static inline char * task_state(struct t
 	read_lock(&tasklist_lock);
 	buffer += sprintf(buffer,
 		"State:\t%s\n"
+#if defined(CONFIG_NICKSCHED)
+		"sleep_avg:\t%lu\n"
+		"sleep_time:\t%lu\n"
+		"total_time:\t%lu\n"
+#else
 		"SleepAVG:\t%lu%%\n"
+#endif
 		"Tgid:\t%d\n"
 		"Pid:\t%d\n"
 		"PPid:\t%d\n"
@@ -167,7 +173,11 @@ static inline char * task_state(struct t
 		"Uid:\t%d\t%d\t%d\t%d\n"
 		"Gid:\t%d\t%d\t%d\t%d\n",
 		get_task_state(p),
+#if defined(CONFIG_NICKSCHED)
+		p->sleep_avg, p->sleep_time, p->total_time,
+#else
 		(p->sleep_avg/1024)*100/(1020000000/1024),
+#endif
 	       	p->tgid,
 		p->pid, p->pid ? p->real_parent->pid : 0,
 		p->pid && p->ptrace ? p->parent->pid : 0,
diff -puN include/linux/init_task.h~nicksched include/linux/init_task.h
--- xx-sources-current/include/linux/init_task.h~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/include/linux/init_task.h	2004-08-10 10:48:04.904403144 -0400
@@ -67,6 +67,25 @@ extern struct group_info init_groups;
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
  */
+
+#if defined(CONFIG_NICKSCHED)
+#define SCHED_PRIO .prio = MAX_PRIO-29,
+#else
+#define SCHED_PRIO .prio = MAX_PRIO-20,
+#endif
+
+#if defined(CONFIG_NICKSCHED)
+#define SCHED_STATIC_PRIO .static_prio = MAX_PRIO-29,
+#else
+#define SCHED_STATIC_PRIO .static_prio = MAX_PRIO-20,
+#endif
+
+#if defined(CONFIG_NICKSCHED)
+#define SCHED_TIME_SLICE
+#else
+#define SCHED_TIME_SLICE .time_slice = HZ,
+#endif
+
 #define INIT_TASK(tsk)	\
 {									\
 	.state		= 0,						\
@@ -74,14 +93,14 @@ extern struct group_info init_groups;
 	.usage		= ATOMIC_INIT(2),				\
 	.flags		= 0,						\
 	.lock_depth	= -1,						\
-	.prio		= MAX_PRIO-20,					\
-	.static_prio	= MAX_PRIO-20,					\
+	SCHED_PRIO							\
+	SCHED_STATIC_PRIO						\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
 	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\
-	.time_slice	= HZ,						\
+	SCHED_TIME_SLICE						\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
 	.ptrace_list	= LIST_HEAD_INIT(tsk.ptrace_list),		\
diff -puN include/linux/sched.h~nicksched include/linux/sched.h
--- xx-sources-current/include/linux/sched.h~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/include/linux/sched.h	2004-08-10 10:48:07.047077408 -0400
@@ -317,7 +317,13 @@ struct signal_struct {
 #define MAX_USER_RT_PRIO	100
 #define MAX_RT_PRIO		MAX_USER_RT_PRIO
 
-#define MAX_PRIO		(MAX_RT_PRIO + 40)
+#if defined(CONFIG_NICKSCHED)
+#define PRIO_RANGE 59
+#else
+#define PRIO_RANGE 40
+#endif
+
+#define MAX_PRIO		(MAX_RT_PRIO + PRIO_RANGE)
 
 #define rt_task(p)		(unlikely((p)->prio < MAX_RT_PRIO))
 
@@ -435,6 +441,22 @@ struct task_struct {
 
 	int lock_depth;		/* Lock depth */
 
+#if defined(CONFIG_NICKSCHED)
+	int prio, static_prio;
+	struct list_head run_list;
+	prio_array_t *array;
+
+	/* Scheduler variables follow. kernel/sched.c */
+	unsigned long array_sequence;
+	unsigned long long timestamp;
+	int used_slice;
+
+	unsigned long total_time, sleep_time;
+	unsigned long sleep_avg;
+
+	unsigned long policy;
+	cpumask_t cpus_allowed;
+#else
 	int prio, static_prio;
 	struct list_head run_list;
 	prio_array_t *array;
@@ -447,6 +469,7 @@ struct task_struct {
 	unsigned long policy;
 	cpumask_t cpus_allowed;
 	unsigned int time_slice, first_time_slice;
+#endif
 
 #ifdef CONFIG_SCHEDSTATS
 	struct sched_info sched_info;
diff -puN include/linux/sysctl.h~nicksched include/linux/sysctl.h
--- xx-sources-current/include/linux/sysctl.h~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/include/linux/sysctl.h	2004-08-10 10:48:07.048077256 -0400
@@ -134,6 +134,7 @@ enum
 	KERN_SPARC_SCONS_PWROFF=64, /* int: serial console power-off halt */
 	KERN_HZ_TIMER=65,	/* int: hz timer on or off */
 	KERN_UNKNOWN_NMI_PANIC=66, /* int: unknown nmi panic flag */
+	KERN_SCHED_TIMESLICE=67, /* int: base timeslice for scheduler */
 };
 
 
diff -puN kernel/Kconfig-extra.xx~nicksched kernel/Kconfig-extra.xx
--- xx-sources-current/kernel/Kconfig-extra.xx~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/kernel/Kconfig-extra.xx	2004-08-10 10:48:07.058075736 -0400
@@ -17,6 +17,32 @@ config SCHED_NONE
 	  It contains the sched domains code by Nick Piggin and some tweaks
 	  to the scheduling code, but no significant changes.
 
+config NICKSCHED
+	bool "Nicksched"
+	help
+	  This is a scheduler written by Nick Piggin.  It is quite fast,
+	  responsive, and does well under heavy loads.
+
+	  In this scheduler, the architecture is still pretty similar
+	  to the original scheduler - there are still two priority
+	  arrays, for example.  The differences are in how the bonuses
+ 	  are given.
+
+	  In Nicksched, a task's priority and bonuses are based on its
+	  "sleep time."  The scheduler keeps track of a task's history -
+	  how long the task was running.  Each task is also assigned one
+	  of three running modes: sleeping (not active), running (using
+	  the CPU), and waiting (waiting for CPU time).  Tasks are given
+	  priority bonuses based on these two factors.  This is a lot
+	  simpler than the original interactivity/credit model, since
+	  it mostly uses linear functions and bit shifts to keep
+	  calculations simple and quick.  And with this method, it
+	  is a lot easier to scale timeslices - for example, the
+	  timeslice given is scaled against the priority of the other
+	  tasks, so a lower priority process can still get larger
+	  timeslices if there aren't higher priority processes using
+	  the CPU.
+
 endchoice
 
 endmenu
diff -puN kernel/sched.c~nicksched kernel/sched.c
--- xx-sources-current/kernel/sched.c~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/kernel/sched.c	2004-08-10 10:48:43.528531384 -0400
@@ -51,13 +51,17 @@
 
 /* Stuff for scheduler proc entry */
 const char *scheduler_name =
-#if 1
+#if defined(CONFIG_NICKSCHED)
+	"Nicksched"
+#else
 	"Default scheduler"
 #endif
 ;
 
 const char *scheduler_version =
-#if 1
+#if defined(CONFIG_NICKSCHED)
+	"v31-np2"
+#else
 	"NA"
 #endif
 ;
@@ -74,8 +78,14 @@ const char *scheduler_version =
  * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
  * and back.
  */
-#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
-#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
+#if defined(CONFIG_NICKSCHED)
+#define NICE_OFFSET 30
+#else
+#define NICE_OFFSET 20
+#endif
+
+#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + NICE_OFFSET)
+#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - NICE_OFFSET)
 #define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
 
 /*
@@ -87,6 +97,8 @@ const char *scheduler_version =
 #define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
 #define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
 
+#define US_TO_JIFFIES(x)	((x) * HZ / 1000000)
+#define JIFFIES_TO_US(x)	((x) * 1000000 / HZ)
 /*
  * Some helpers for converting nanosecond timing to jiffy resolution
  */
@@ -100,6 +112,27 @@ const char *scheduler_version =
  * default timeslice is 100 msecs, maximum timeslice is 800 msecs.
  * Timeslices get refilled after they expire.
  */
+#if defined(CONFIG_NICKSCHED)
+int sched_base_timeslice = 64; /* This gets divided by 8 */
+int sched_min_base = 1;
+int sched_max_base = 10000;
+
+#define RT_TIMESLICE		50		/* 50ms */
+#define BASE_TIMESLICE		(sched_base_timeslice)
+#define MIN_TIMESLICE		(BASE_TIMESLICE * HZ / 1000 / 8 / 8 ?: 1)
+#define MAX_TIMESLICE		(BASE_TIMESLICE * (MAX_USER_PRIO + 1) / 3 * 2 / 8)
+
+/* Maximum amount of history that will be used to calculate priority */
+#define MAX_SLEEP_SHIFT		19
+#define MAX_SLEEP		(1UL << MAX_SLEEP_SHIFT) /* roughly 0.52s */
+#define MAX_SLEEP_AFFECT	(MAX_SLEEP/8)
+#define MAX_RUN_AFFECT		(MAX_SLEEP/16)
+#define MIN_HISTORY		(MAX_SLEEP/8)
+#define FORKED_TS_MAX		(US_TO_JIFFIES(MIN_HISTORY) ?: 1)
+#define SLEEP_FACTOR		1024
+#define STIME_SLEEP		1	/* Sleeping */
+#define STIME_RUN		2	/* Using CPU */
+#else
 #define MIN_TIMESLICE		max(5 * HZ / 1000, 1)
 #define DEF_TIMESLICE		(100 * HZ / 1000)
 #define ON_RUNQUEUE_WEIGHT	 30
@@ -113,6 +146,7 @@ const char *scheduler_version =
 #define STARVATION_LIMIT	(MAX_SLEEP_AVG)
 #define NS_MAX_SLEEP_AVG	(JIFFIES_TO_NS(MAX_SLEEP_AVG))
 #define CREDIT_LIMIT		100
+#endif
 
 /*
  * If a task is 'interactive' then we reinsert it in the active
@@ -182,7 +216,7 @@ const char *scheduler_version =
  * it gets during one round of execution. But even the lowest
  * priority thread gets MIN_TIMESLICE worth of execution time.
  */
-
+#if !defined(CONFIG_NICKSCHED)
 #define SCALE_PRIO(x, prio) \
 	max(x * (MAX_PRIO - prio) / (MAX_USER_PRIO/2), MIN_TIMESLICE)
 
@@ -193,6 +227,7 @@ static unsigned int task_timeslice(task_
 	else
 		return SCALE_PRIO(DEF_TIMESLICE, p->static_prio);
 }
+#endif
 #define task_hot(p, now, sd) ((now) - (p)->timestamp < (sd)->cache_hot_time)
 
 enum idle_type
@@ -214,6 +249,10 @@ struct sched_domain;
 typedef struct runqueue runqueue_t;
 
 struct prio_array {
+#if defined(CONFIG_NICKSCHED)
+	int min_prio;
+	int min_nice;
+#endif
 	unsigned int nr_active;
 	unsigned long bitmap[BITMAP_SIZE];
 	struct list_head queue[MAX_PRIO];
@@ -237,6 +276,15 @@ struct runqueue {
 #ifdef CONFIG_SMP
 	unsigned long cpu_load;
 #endif
+#if defined(CONFIG_NICKSCHED)
+	unsigned long array_sequence;
+	unsigned long nr_uninterruptible;
+	unsigned long long nr_switches;
+	task_t *curr, *idle;
+	struct mm_struct *prev_mm;
+	atomic_t nr_iowait;
+	prio_array_t *active, *expired, arrays[2];
+#else
 	unsigned long long nr_switches;
 	unsigned long expired_timestamp, nr_uninterruptible;
 	unsigned long long timestamp_last_tick;
@@ -245,8 +293,12 @@ struct runqueue {
 	prio_array_t *active, *expired, arrays[2];
 	int best_expired_prio;
 	atomic_t nr_iowait;
+#endif
 
 #ifdef CONFIG_SMP
+#if defined(CONFIG_NICKSCHED)
+	unsigned long long timestamp_last_tick;
+#endif
 	struct sched_domain *sd;
 
 	/* For active balancing */
@@ -339,7 +391,9 @@ struct sched_domain {
 	unsigned int imbalance_pct;	/* No balance until over watermark */
 	unsigned long long cache_hot_time; /* Task considered cache hot (ns) */
 	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
+#if !defined(CONFIG_NICKSCHED)
 	unsigned int per_cpu_gain;	/* CPU % gained by adding domain cpus */
+#endif
 	int flags;			/* See SD_* */
 
 	/* Runtime fields. */
@@ -365,6 +419,24 @@ struct sched_domain {
 #endif
 };
 
+#if defined(CONFIG_NICKSCHED)
+#define CACHE_HOT_TIME_ONE .cache_hot_time = (5*1000/2),
+#define CACHE_HOT_TIME_TWO .cache_hot_time = (10*1000),
+#else
+#define CACHE_HOT_TIME_ONE .cache_hot_time = (5*1000000/2),
+#define CACHE_HOT_TIME_TWO .cache_hot_time = (10*1000000),
+#endif
+
+#if defined(CONFIG_NICKSCHED)
+#define PER_CPU_GAIN_ONE
+#define CPUPOWER_FLAG
+#define PER_CPU_GAIN_TWO
+#else
+#define PER_CPU_GAIN_ONE .per_cpu_gain = 25,
+#define CPUPOWER_FLAG | SD_SHARE_CPUPOWER
+#define PER_CPU_GAIN_TWO .per_cpu_gain = 100,
+#endif
+
 #ifndef ARCH_HAS_SCHED_TUNE
 #ifdef CONFIG_SCHED_SMT
 #define ARCH_HAS_SCHED_WAKE_IDLE
@@ -379,12 +451,12 @@ struct sched_domain {
 	.imbalance_pct		= 110,			\
 	.cache_hot_time		= 0,			\
 	.cache_nice_tries	= 0,			\
-	.per_cpu_gain		= 25,			\
+	PER_CPU_GAIN_ONE				\
 	.flags			= SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
 				| SD_WAKE_IDLE		\
-				| SD_SHARE_CPUPOWER,	\
+				CPUPOWER_FLAG,		\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 	.nr_balance_failed	= 0,			\
@@ -400,9 +472,9 @@ struct sched_domain {
 	.max_interval		= 4,			\
 	.busy_factor		= 64,			\
 	.imbalance_pct		= 125,			\
-	.cache_hot_time		= (5*1000000/2),	\
+	CACHE_HOT_TIME_ONE				\
 	.cache_nice_tries	= 1,			\
-	.per_cpu_gain		= 100,			\
+	PER_CPU_GAIN_TWO				\
 	.flags			= SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
@@ -422,9 +494,9 @@ struct sched_domain {
 	.max_interval		= 32,			\
 	.busy_factor		= 32,			\
 	.imbalance_pct		= 125,			\
-	.cache_hot_time		= (10*1000000),		\
+	CACHE_HOT_TIME_TWO				\
 	.cache_nice_tries	= 1,			\
-	.per_cpu_gain		= 100,			\
+	PER_CPU_GAIN_TWO				\
 	.flags			= SD_BALANCE_EXEC	\
 				| SD_WAKE_BALANCE,	\
 	.last_balance		= jiffies,		\
@@ -731,6 +803,19 @@ static void dequeue_task(struct task_str
 		__clear_bit(p->prio, array->bitmap);
 }
 
+#if defined(CONFIG_NICKSCHED)
+static void enqueue_task(struct task_struct *p, prio_array_t *array)
+{
+	struct list_head *entry = array->queue + p->prio;
+	sched_info_queued(p);
+	if (!list_empty(entry))
+		entry = entry->next;
+	list_add_tail(&p->run_list, entry);
+	__set_bit(p->prio, array->bitmap);
+	array->nr_active++;
+	p->array = array;
+}
+#else
 static void enqueue_task(struct task_struct *p, prio_array_t *array)
 {
 	sched_info_queued(p);
@@ -739,6 +824,7 @@ static void enqueue_task(struct task_str
 	array->nr_active++;
 	p->array = array;
 }
+#endif
 
 /*
  * Used by the migration code - we pull tasks from the head of the
@@ -753,6 +839,111 @@ static inline void enqueue_task_head(str
 	p->array = array;
 }
 
+static inline unsigned long long clock_us(void)
+{
+#if defined(CONFIG_NICKSCHED)
+	return sched_clock() >> 10;
+#else
+	return sched_clock();
+#endif
+}
+
+#if defined(CONFIG_NICKSCHED)
+static inline void add_task_time(task_t *p, unsigned long long time, unsigned long type)
+{
+	unsigned long ratio;
+	unsigned long max_affect;
+	unsigned long long tmp;
+
+	if (time == 0)
+		return;
+
+	if (type == STIME_SLEEP)
+		max_affect = MAX_SLEEP_AFFECT;
+	else
+		max_affect = MAX_RUN_AFFECT;
+
+	if (type == STIME_SLEEP)
+		time /= 2;
+
+	if (time > max_affect)
+		time = max_affect;
+
+	ratio = MAX_SLEEP - time;
+	tmp = (unsigned long long)ratio*p->total_time + MAX_SLEEP/2;
+	tmp >>= MAX_SLEEP_SHIFT;
+	p->total_time = (unsigned long)tmp;
+
+	tmp = (unsigned long long)ratio*p->sleep_time + MAX_SLEEP/2;
+	tmp >>= MAX_SLEEP_SHIFT;
+	p->sleep_time = (unsigned long)tmp;
+
+	p->total_time += time;
+	if (type == STIME_SLEEP)
+		p->sleep_time += time;
+
+	p->sleep_avg = (SLEEP_FACTOR * p->sleep_time) / p->total_time;
+}
+
+/*
+ * The higher a thread's priority, the bigger timeslices
+ * it gets during one round of execution. But even the lowest
+ * priority thread gets MIN_TIMESLICE worth of execution time.
+ *
+ * Timeslices are scaled, so if only low priority processes are running,
+ * they will all get long timeslices.
+ */
+static int task_timeslice(task_t *p, runqueue_t *rq)
+{
+	int idx, base, delta;
+	int timeslice;
+
+	if (rt_task(p))
+		return RT_TIMESLICE;
+
+	idx = min(p->prio, rq->expired->min_prio);
+	delta = p->prio - idx;
+	base = BASE_TIMESLICE * (MAX_USER_PRIO + 1) / (delta + 3);
+
+	idx = min(rq->expired->min_nice, p->static_prio);
+	delta = p->static_prio - idx;
+	timeslice = base * 2 / (delta + 2);
+
+	timeslice = timeslice * 30 / (60 - USER_PRIO(idx));
+
+	timeslice = timeslice * 1000 / HZ;
+	timeslice >>= 3;
+	if (timeslice < MIN_TIMESLICE)
+		timeslice = MIN_TIMESLICE;
+
+	return timeslice;
+}
+
+/*
+ * task_priority: calculates a task's priority based on previous running
+ * history (see add_task_time). The priority is just a simple linear function
+ * based on sleep_avg and static_prio.
+ */
+static inline int task_priority(task_t *p)
+{
+	int bonus, prio;
+
+	if (rt_task(p))
+		return p->prio;
+
+	prio = USER_PRIO(p->static_prio) + 10;
+	bonus = (((MAX_USER_PRIO + 1) / 3) * p->sleep_avg + (SLEEP_FACTOR / 2))
+					/ SLEEP_FACTOR;
+	prio = MAX_RT_PRIO + prio - bonus;
+
+	if (prio < MAX_RT_PRIO)
+		return MAX_RT_PRIO;
+	if (prio > MAX_PRIO-1)
+		return MAX_PRIO-1;
+
+ 	return prio;
+}
+#else
 /*
  * effective_prio - return the priority that is based on the static
  * priority but is modified by bonuses/penalties.
@@ -783,15 +974,30 @@ static int effective_prio(task_t *p)
 		prio = MAX_PRIO-1;
 	return prio;
 }
+#endif
 
 /*
  * __activate_task - move a task to the runqueue.
  */
+#if defined(CONFIG_NICKSCHED)
+static inline void __activate_task(task_t *p, runqueue_t *rq, prio_array_t *array)
+{
+	enqueue_task(p, array);
+	rq->nr_running++;
+	if (!rt_task(p)) {
+		if (p->prio < array->min_prio)
+			array->min_prio = p->prio;
+		if (p->static_prio < array->min_nice)
+			array->min_nice = p->static_prio;
+	}
+}
+#else
 static inline void __activate_task(task_t *p, runqueue_t *rq)
 {
 	enqueue_task(p, rq->active);
 	rq->nr_running++;
 }
+#endif
 
 /*
  * __activate_idle_task - move idle task to the _front_ of runqueue.
@@ -802,6 +1008,7 @@ static inline void __activate_idle_task(
 	rq->nr_running++;
 }
 
+#if !defined(CONFIG_NICKSCHED)
 static void recalc_task_prio(task_t *p, unsigned long long now)
 {
 	unsigned long long __sleep_time = now - p->timestamp;
@@ -875,6 +1082,7 @@ static void recalc_task_prio(task_t *p, 
 
 	p->prio = effective_prio(p);
 }
+#endif
 
 /*
  * activate_task - move a task to the runqueue and do priority recalculation
@@ -885,6 +1093,10 @@ static void recalc_task_prio(task_t *p, 
 static void activate_task(task_t *p, runqueue_t *rq, int local)
 {
 	unsigned long long now;
+#if defined(CONFIG_NICKSCHED)
+	unsigned long long sleep;
+	prio_array_t *array;
+#endif
 
 	now = sched_clock();
 #ifdef CONFIG_SMP
@@ -896,6 +1108,30 @@ static void activate_task(task_t *p, run
 	}
 #endif
 
+#if defined(CONFIG_NICKSCHED)
+	/*
+	 * If we have slept through an active/expired array switch, restart
+	 * our timeslice too.
+	 */
+	sleep = now - p->timestamp;
+	add_task_time(p, sleep, STIME_SLEEP);
+	p->prio = task_priority(p);
+	p->timestamp = now;
+
+	array = rq->active;
+	if (rq->array_sequence != p->array_sequence) {
+		p->used_slice = 0;
+	} else if (unlikely(p->used_slice == -1)) {
+		array = rq->expired;
+		p->used_slice = 0;
+	} else {
+		unsigned long tmp = MAX_SLEEP_AFFECT;
+		if (tmp > sleep)
+			tmp = sleep;
+		p->used_slice += US_TO_JIFFIES(tmp / 2);
+	}
+	__activate_task(p, rq, array);
+#else
 	recalc_task_prio(p, now);
 
 	/*
@@ -923,11 +1159,21 @@ static void activate_task(task_t *p, run
 	p->timestamp = now;
 
 	__activate_task(p, rq);
+#endif
 }
 
 /*
  * deactivate_task - remove a task from the runqueue.
  */
+#if defined(CONFIG_NICKSCHED)
+static inline void deactivate_task(struct task_struct *p, runqueue_t *rq)
+{
+	p->array_sequence = rq->array_sequence;
+	rq->nr_running--;
+ 	dequeue_task(p, p->array);
+ 	p->array = NULL;
+}
+#else
 static void deactivate_task(struct task_struct *p, runqueue_t *rq)
 {
 	rq->nr_running--;
@@ -936,6 +1182,7 @@ static void deactivate_task(struct task_
 	dequeue_task(p, p->array);
 	p->array = NULL;
 }
+#endif
 
 /*
  * resched_task - mark a task 'to be rescheduled now'.
@@ -1262,7 +1509,9 @@ out_activate:
 		 * Tasks on involuntary sleep don't earn
 		 * sleep_avg beyond just interactive state.
 		 */
+#if !defined(CONFIG_NICKSCHED)
 		p->activated = -1;
+#endif
 	}
 
 	/*
@@ -1312,6 +1561,9 @@ static int find_idlest_cpu(struct task_s
  */
 void fastcall sched_fork(task_t *p)
 {
+#if defined(CONFIG_NICKSCHED)
+	runqueue_t *rq;
+#endif
 	/*
 	 * We mark the process as running here, but have not actually
 	 * inserted it onto the runqueue yet. This guarantees that
@@ -1326,14 +1578,39 @@ void fastcall sched_fork(task_t *p)
 	memset(&p->sched_info, 0, sizeof(p->sched_info));
 #endif
 #ifdef CONFIG_PREEMPT
-	/*
-	 * During context-switch we hold precisely one spinlock, which
-	 * schedule_tail drops. (in the common case it's this_rq()->lock,
-	 * but it also can be p->switch_lock.) So we compensate with a count
-	 * of 1. Also, we want to start with kernel preemption disabled.
-	 */
+	/* Want to start with kernel preemption disabled. */
 	p->thread_info->preempt_count = 1;
 #endif
+#if defined(CONFIG_NICKSCHED)
+	preempt_disable();
+	rq = this_rq();
+
+	/*
+	 * Get MIN_HISTORY of history with the same sleep_avg as parent.
+	 */
+	p->sleep_avg = current->sleep_avg;
+	p->total_time = MIN_HISTORY;
+	p->sleep_time = p->total_time * p->sleep_avg / SLEEP_FACTOR;
+
+	/*
+	 * Parent loses 1/4 sleep_time for forking.
+	 */
+	current->sleep_time = 3 * current->sleep_time / 4;
+	if (likely(current->total_time != 0)) {
+		current->sleep_avg = (SLEEP_FACTOR * current->sleep_time)
+			/ current->total_time;
+	}
+
+	/*
+	 * Share the timeslice between parent and child.
+	 */
+	p->used_slice = 0;
+	local_irq_disable();
+	if (unlikely(current->used_slice == -1 || current == rq->idle))
+		p->used_slice = -1;
+	local_irq_enable();
+	preempt_enable();
+#else
 	/*
 	 * Share the timeslice between parent and child, thus the
 	 * total amount of pending timeslices in the system doesn't change,
@@ -1361,6 +1638,7 @@ void fastcall sched_fork(task_t *p)
 		preempt_enable();
 	} else
 		local_irq_enable();
+#endif
 }
 
 /*
@@ -1372,6 +1650,72 @@ void fastcall sched_fork(task_t *p)
  */
 void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
 {
+#if defined(CONFIG_NICKSCHED)
+	unsigned long flags;
+	int this_cpu, cpu;
+	runqueue_t *rq;
+	prio_array_t *array;
+
+	BUG_ON(p->state != TASK_RUNNING);
+
+	rq = task_rq_lock(p, &flags);
+	p->timestamp = clock_us();
+
+	cpu = task_cpu(p);
+	this_cpu = smp_processor_id();
+
+	p->prio = task_priority(p);
+
+	array = rq->active;
+	if (unlikely(p->used_slice == -1)) {
+		p->used_slice = 0;
+		array = rq->expired;
+	} else {
+		int total = task_timeslice(p, rq);
+		int ts = min(total / 4, MIN_TIMESLICE);
+		ts = min(ts, (int)FORKED_TS_MAX);
+		p->used_slice = total - ts;
+		current->used_slice += ts;
+	}
+
+	if (likely(cpu == this_cpu)) {
+		if (!(clone_flags & CLONE_VM) && likely(array == rq->active)) {
+			/*
+			 * The VM isn't cloned, so we're in a good position to
+			 * do child-runs-first in anticipation of an exec. This
+			 * usually avoids a lot of COW overhead.
+			 */
+			if (p->prio >= current->prio) {
+				p->prio = current->prio;
+				list_add_tail(&p->run_list, &current->run_list);
+				p->array = current->array;
+				p->array->nr_active++;
+				rq->nr_running++;
+			} else
+				__activate_task(p, rq, array);
+
+			set_need_resched();
+		} else {
+			/* Run child last */
+			__activate_task(p, rq, array);
+		}
+#ifdef CONFIG_SMP
+	} else {
+		runqueue_t *this_rq = this_rq();
+		/*
+		 * Not the local CPU - must adjust timestamp. This should
+		 * get optimised away in the !CONFIG_SMP case.
+		 */
+		p->timestamp = (p->timestamp - this_rq->timestamp_last_tick)
+					+ rq->timestamp_last_tick;
+
+		__activate_task(p, rq, array);
+		if (generic_task_preempts_curr(p, rq))
+			resched_task(rq->curr);
+#endif
+	}
+	task_rq_unlock(rq, &flags);
+#else
 	unsigned long flags;
 	int this_cpu, cpu;
 	runqueue_t *rq;
@@ -1441,6 +1785,7 @@ void fastcall wake_up_new_task(task_t * 
 	current->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(current) *
 		PARENT_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
 	task_rq_unlock(rq, &flags);
+#endif
 }
 
 /*
@@ -1454,6 +1799,7 @@ void fastcall wake_up_new_task(task_t * 
  */
 void fastcall sched_exit(task_t * p)
 {
+#if !defined(CONFIG_NICKSCHED)
 	unsigned long flags;
 	runqueue_t *rq;
 
@@ -1472,6 +1818,7 @@ void fastcall sched_exit(task_t * p)
 		(EXIT_WEIGHT + 1) * EXIT_WEIGHT + p->sleep_avg /
 		(EXIT_WEIGHT + 1);
 	task_rq_unlock(rq, &flags);
+#endif
 }
 
 /**
@@ -1780,6 +2127,14 @@ void pull_task(runqueue_t *src_rq, prio_
 	set_task_cpu(p, this_cpu);
 	this_rq->nr_running++;
 	enqueue_task(p, this_array);
+#if defined(CONFIG_NICKSCHED)
+	if (!rt_task(p)) {
+		if (p->prio < this_array->min_prio)
+			this_array->min_prio = p->prio;
+		if (p->static_prio < this_array->min_nice)
+			this_array->min_nice = p->static_prio;
+	}
+#endif
 	p->timestamp = (p->timestamp - src_rq->timestamp_last_tick)
 				+ this_rq->timestamp_last_tick;
 	/*
@@ -2363,6 +2718,7 @@ static inline void idle_balance(int cpu,
 }
 #endif
 
+#if !defined(CONFIG_NICKSCHED)
 static inline int wake_priority_sleeper(runqueue_t *rq)
 {
 #ifdef CONFIG_SCHED_SMT
@@ -2377,6 +2733,7 @@ static inline int wake_priority_sleeper(
 #endif
 	return 0;
 }
+#endif
 
 DEFINE_PER_CPU(struct kernel_stat, kstat);
 
@@ -2407,12 +2764,18 @@ EXPORT_PER_CPU_SYMBOL(kstat);
  */
 void scheduler_tick(int user_ticks, int sys_ticks)
 {
+#if defined(CONFIG_NICKSCHED)
+	enum idle_type cpu_status;
+	int ts;
+#endif
 	int cpu = smp_processor_id();
 	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
 	runqueue_t *rq = this_rq();
 	task_t *p = current;
 
-	rq->timestamp_last_tick = sched_clock();
+#if !defined(CONFIG_NICKSCHED) || defined(CONFIG_SMP)
+	rq->timestamp_last_tick = clock_us();
+#endif
 
 	if (rcu_pending(cpu))
 		rcu_check_callbacks(cpu, user_ticks);
@@ -2426,6 +2789,17 @@ void scheduler_tick(int user_ticks, int 
 		sys_ticks = 0;
 	}
 
+#if defined(CONFIG_NICKSCHED)
+	cpu_status = NOT_IDLE;
+	if (p == rq->idle) {
+		if (atomic_read(&rq->nr_iowait) > 0)
+			cpustat->iowait += sys_ticks;
+		else
+			cpustat->idle += sys_ticks;
+		cpu_status = IDLE;
+		goto out;
+	}
+#else
 	if (p == rq->idle) {
 		if (atomic_read(&rq->nr_iowait) > 0)
 			cpustat->iowait += sys_ticks;
@@ -2436,12 +2810,31 @@ void scheduler_tick(int user_ticks, int 
 		rebalance_tick(cpu, rq, IDLE);
 		return;
 	}
+#endif
 	if (TASK_NICE(p) > 0)
 		cpustat->nice += user_ticks;
 	else
 		cpustat->user += user_ticks;
 	cpustat->system += sys_ticks;
 
+#if defined(CONFIG_NICKSCHED)
+	if (unlikely(p->used_slice == -1))
+		goto out;
+	if (unlikely(p->policy == SCHED_FIFO))
+		goto out;
+
+	/* p was running during this tick. Update its time slice counter. */
+	p->used_slice++;
+	ts = task_timeslice(p, rq);
+	if (unlikely(p->used_slice >= ts)) {
+		if (p->comm[0] == 'c' && p->comm[1] == '\0')
+			printk(KERN_INFO "%d used %d of %d\n", p->pid, p->used_slice, ts);
+		p->used_slice = -1;
+		set_tsk_need_resched(p);
+	}
+out:
+	rebalance_tick(cpu, rq, cpu_status);
+#else
 	/* Task might have expired already, but not scheduled off yet */
 	if (generic_task_queued(p) != (int)rq->active) {
 		set_tsk_need_resched(p);
@@ -2522,8 +2915,10 @@ out_unlock:
 	spin_unlock(&rq->lock);
 out:
 	rebalance_tick(cpu, rq, NOT_IDLE);
+#endif
 }
 
+#if !defined(CONFIG_NICKSCHED)
 #ifdef CONFIG_SCHED_SMT
 static inline void wake_sleeping_dependent(int cpu, runqueue_t *rq)
 {
@@ -2608,6 +3003,7 @@ static inline int dependent_sleeper(int 
 	return 0;
 }
 #endif
+#endif
 
 /*
  * schedule() is the main scheduler function.
@@ -2628,12 +3024,20 @@ asmlinkage void __sched schedule(void)
 	 * schedule() atomically, we ignore that path for now.
 	 * Otherwise, whine if we are scheduling when we should not be.
 	 */
+#if defined(CONFIG_NICKSCHED)
+	if (unlikely(in_atomic()) &&
+			likely(!(current->state & (TASK_DEAD | TASK_ZOMBIE)))) {
+		printk(KERN_ERR "bad: scheduling while atomic!\n");
+		dump_stack();
+	}
+#else
 	if (likely(!(current->state & (TASK_DEAD | TASK_ZOMBIE)))) {
 		if (unlikely(in_atomic())) {
 			printk(KERN_ERR "bad: scheduling while atomic!\n");
 			dump_stack();
 		}
 	}
+#endif
 	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
 
 need_resched:
@@ -2652,7 +3056,12 @@ need_resched:
 
 	release_kernel_lock(prev);
 	schedstat_inc(rq, sched_cnt);
-	now = sched_clock();
+	now = clock_us();
+#if defined(CONFIG_NICKSCHED)
+	run_time = now - prev->timestamp;
+	prev->timestamp = now;
+	add_task_time(prev, run_time, STIME_RUN);
+#else
 	if (likely(now - prev->timestamp < NS_MAX_SLEEP_AVG))
 		run_time = now - prev->timestamp;
 	else
@@ -2665,6 +3074,7 @@ need_resched:
 	 */
 	if (HIGH_CREDIT(prev))
 		run_time /= (CURRENT_BONUS(prev) ? : 1);
+#endif
 
 	spin_lock_irq(&rq->lock);
 
@@ -2678,10 +3088,74 @@ need_resched:
 		if (unlikely((prev->state & TASK_INTERRUPTIBLE) &&
 				unlikely(signal_pending(prev))))
 			prev->state = TASK_RUNNING;
-		else
+		else {
 			deactivate_task(prev, rq);
+#if defined(CONFIG_NICKSCHED)
+			if (prev->state == TASK_UNINTERRUPTIBLE)
+				rq->nr_uninterruptible++;
+			goto no_check_expired;
+#endif
+		}
 	}
 
+#if defined(CONFIG_NICKSCHED)
+	if (unlikely(prev->used_slice == -1)) {
+		prev->used_slice = 0;
+		if (rt_task(prev)) {
+			if (prev->policy == SCHED_RR) {
+				dequeue_task(prev, prev->array);
+				enqueue_task(prev, rq->active);
+			}
+		} else {
+			dequeue_task(prev, prev->array);
+			prev->prio = task_priority(prev);
+			enqueue_task(prev, rq->expired);
+			if (prev->prio < rq->expired->min_prio)
+				rq->expired->min_prio = prev->prio;
+			if (prev->static_prio < rq->expired->min_nice)
+				rq->expired->min_nice = prev->static_prio;
+		}
+	}
+
+no_check_expired:
+	cpu = smp_processor_id();
+	if (unlikely(!rq->nr_running)) {
+		rq->array_sequence++;
+		idle_balance(cpu, rq);
+		if (!rq->nr_running) {
+			rq->arrays[0].min_prio = MAX_PRIO;
+			rq->arrays[0].min_nice = MAX_PRIO;
+			rq->arrays[1].min_prio = MAX_PRIO;
+			rq->arrays[1].min_nice = MAX_PRIO;
+			next = rq->idle;
+			goto switch_tasks;
+		}
+	}
+
+	array = rq->active;
+	if (unlikely(!array->nr_active)) {
+		/*
+		 * Switch the active and expired arrays.
+		 */
+		schedstat_inc(rq, sched_switch);
+		rq->array_sequence++;
+		rq->active = rq->expired;
+		rq->expired = array;
+		array = rq->active;
+		rq->expired->min_prio = MAX_PRIO;
+		rq->expired->min_nice = MAX_PRIO;
+	} else
+		schedstat_inc(rq, sched_noswitch);
+
+	idx = sched_find_first_bit(array->bitmap);
+	queue = array->queue + idx;
+	next = list_entry(queue->next, task_t, run_list);
+
+switch_tasks:
+	prefetch(next);
+	clear_tsk_need_resched(prev);
+	rcu_qsctr_inc(task_cpu(prev));
+#else
 	cpu = smp_processor_id();
 	if (unlikely(!rq->nr_running)) {
 		idle_balance(cpu, rq);
@@ -2741,6 +3215,7 @@ switch_tasks:
 			prev->interactive_credit--;
 	}
 	prev->timestamp = now;
+#endif
 
 	sched_info_switch(prev, next);
 	if (likely(prev != next)) {
@@ -3229,12 +3704,20 @@ static int setscheduler(pid_t pid, int p
 
 	array = p->array;
 	if (array)
+#if defined(CONFIG_NICKSCHED)
+		deactivate_task(p, rq);
+#else
 		deactivate_task(p, task_rq(p));
+#endif
 	retval = 0;
 	oldprio = p->prio;
 	__setscheduler(p, policy, lp.sched_priority);
 	if (array) {
+#if defined(CONFIG_NICKSCHED)
+		__activate_task(p, rq, array);
+#else
 		__activate_task(p, task_rq(p));
+#endif
 		/*
 		 * Reschedule if we are currently running on this runqueue and
 		 * our priority decreased, or if we are not currently running on
@@ -3481,6 +3964,13 @@ asmlinkage long sys_sched_yield(void)
 
 	dequeue_task(current, array);
 	enqueue_task(current, target);
+#if defined(CONFIG_NICKSCHED)
+	if (current->prio < target->min_prio)
+		target->min_prio = current->prio;
+	if (current->static_prio < target->min_nice)
+		target->min_nice = current->static_prio;
+	current->used_slice = 0;
+#endif
 
 	/*
 	 * Since we are going to call schedule() anyway, there's
@@ -3604,6 +4094,10 @@ long sys_sched_rr_get_interval(pid_t pid
 	int retval = -EINVAL;
 	struct timespec t;
 	task_t *p;
+#if defined(CONFIG_NICKSCHED)
+	unsigned long flags;
+	runqueue_t *rq;
+#endif
 
 	if (pid < 0)
 		goto out_nounlock;
@@ -3618,8 +4112,14 @@ long sys_sched_rr_get_interval(pid_t pid
 	if (retval)
 		goto out_unlock;
 
+#if defined(CONFIG_NICKSCHED)
+	rq = task_rq_lock(p, &flags);
+	jiffies_to_timespec(p->policy & SCHED_FIFO ? 0 : task_timeslice(p, rq), &t);
+	task_rq_unlock(rq, &flags);
+#else
 	jiffies_to_timespec(p->policy & SCHED_FIFO ?
 				0 : task_timeslice(p), &t);
+#endif
 	read_unlock(&tasklist_lock);
 	retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;
 out_nounlock:
@@ -3733,10 +4233,15 @@ void __devinit init_idle(task_t *idle, i
 	unsigned long flags;
 
 	idle->sleep_avg = 0;
+#if !defined(CONFIG_NICKSCHED)
 	idle->interactive_credit = 0;
+#endif
 	idle->array = NULL;
 	idle->prio = MAX_PRIO;
 	idle->state = TASK_RUNNING;
+#if defined(CONFIG_NICKSCHED)
+	idle->used_slice = 0;
+#endif
 	set_task_cpu(idle, cpu);
 
 	spin_lock_irqsave(&rq->lock, flags);
@@ -4546,7 +5051,9 @@ void __init sched_init(void)
 		spin_lock_init(&rq->lock);
 		rq->active = rq->arrays;
 		rq->expired = rq->arrays + 1;
+#if !defined(CONFIG_NICKSCHED)
 		rq->best_expired_prio = MAX_PRIO;
+#endif
 
 #ifdef CONFIG_SMP
 		rq->sd = &sched_domain_init;
@@ -4560,6 +5067,10 @@ void __init sched_init(void)
 
 		for (j = 0; j < 2; j++) {
 			array = rq->arrays + j;
+#if defined(CONFIG_NICKSCHED)
+			array->min_prio = MAX_PRIO;
+			array->min_nice = MAX_PRIO;
+#endif
 			for (k = 0; k < MAX_PRIO; k++) {
 				INIT_LIST_HEAD(array->queue + k);
 				__clear_bit(k, array->bitmap);
diff -puN kernel/sysctl.c~nicksched kernel/sysctl.c
--- xx-sources-current/kernel/sysctl.c~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/kernel/sysctl.c	2004-08-10 10:48:07.057075888 -0400
@@ -65,6 +65,11 @@ extern int sysctl_lower_zone_protection;
 extern int min_free_kbytes;
 extern int printk_ratelimit_jiffies;
 extern int printk_ratelimit_burst;
+#if defined(CONFIG_NICKSCHED)
+extern int sched_base_timeslice;
+extern int sched_min_base;
+extern int sched_max_base;
+#endif
 
 #if defined(CONFIG_X86_LOCAL_APIC) && defined(__i386__)
 int unknown_nmi_panic;
@@ -641,6 +646,19 @@ static ctl_table kern_table[] = {
 		.proc_handler   = &proc_unknown_nmi_panic,
 	},
 #endif
+#if defined(CONFIG_NICKSCHED)
+	{
+		.ctl_name	= KERN_SCHED_TIMESLICE,
+		.procname	= "base_timeslice",
+		.data		= &sched_base_timeslice,
+		.maxlen		= sizeof (int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.strategy	= &sysctl_intvec,
+		.extra1		= &sched_min_base,
+		.extra2		= &sched_max_base,
+	},
+#endif
 	{ .ctl_name = 0 }
 };
 
diff -puN mm/oom_kill.c~nicksched mm/oom_kill.c
--- xx-sources-current/mm/oom_kill.c~nicksched	2004-08-10 04:23:00.000000000 -0400
+++ xx-sources-current-xiphux/mm/oom_kill.c	2004-08-10 10:48:04.916401320 -0400
@@ -148,7 +148,9 @@ static void __oom_kill_task(task_t *p)
 	 * all the memory it needs. That way it should be able to
 	 * exit() and clear out its resources quickly...
 	 */
+#if !defined(CONFIG_NICKSCHED)
 	p->time_slice = HZ;
+#endif
 	p->flags |= PF_MEMALLOC | PF_MEMDIE;
 
 	/* This process has hardware access, be more careful. */

_
