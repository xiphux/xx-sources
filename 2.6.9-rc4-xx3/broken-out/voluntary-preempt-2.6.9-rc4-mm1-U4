Index: xx-sources/Makefile
===================================================================
--- xx-sources.orig/Makefile	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/Makefile	2004-10-16 22:00:09.025887520 -0400
@@ -486,8 +486,12 @@
 CFLAGS		+= -O2
 endif
 
-ifndef CONFIG_FRAME_POINTER
-CFLAGS		+= -fomit-frame-pointer
+ifdef CONFIG_MCOUNT
+CFLAGS		+= -pg
+else
+ ifndef CONFIG_FRAME_POINTER
+ CFLAGS		+= -fomit-frame-pointer
+ endif
 endif
 
 ifdef CONFIG_DEBUG_INFO
Index: xx-sources/arch/i386/Kconfig
===================================================================
--- xx-sources.orig/arch/i386/Kconfig	2004-10-16 20:56:47.000000000 -0400
+++ xx-sources/arch/i386/Kconfig	2004-10-16 22:00:55.549814808 -0400
@@ -359,12 +359,12 @@
 
 config RWSEM_GENERIC_SPINLOCK
 	bool
-	depends on M386
+	depends on M386 || PREEMPT_REALTIME
 	default y
 
 config RWSEM_XCHGADD_ALGORITHM
 	bool
-	depends on !M386
+	depends on !RWSEM_GENERIC_SPINLOCK
 	default y
 
 config X86_PPRO_FENCE
@@ -519,6 +519,63 @@
 	  Say Y here if you are building a kernel for a desktop system.
 	  Say N if you are unsure.
 
+config PREEMPT_VOLUNTARY
+	bool "Voluntary Kernel Preemption"
+	default y
+	help
+	  This option reduces the latency of the kernel by adding more
+	  "explicit preemption points" to the kernel code. These new
+	  preemption points have been selected to minimize the maximum
+	  latency of rescheduling, providing faster application reactions.
+
+	  Say Y here if you are building a kernel for a desktop system.
+	  Say N if you are unsure.
+
+config PREEMPT_SOFTIRQS
+	bool "Preempt Softirqs"
+	default y
+	help
+	  This option reduces the latency of the kernel by 'threading'
+          soft interrupts. This means that all softirqs will execute
+          in softirqd's context. While this helps latency, it can also
+          reduce performance.
+
+          The threading of softirqs can also be controlled via
+          /proc/sys/kernel/softirq_preemption runtime flag and the
+          sofirq-preempt=0/1 boot-time option.
+
+	  Say N if you are unsure.
+
+config PREEMPT_HARDIRQS
+	bool "Preempt Hardirqs"
+	default y
+	help
+	  This option reduces the latency of the kernel by 'threading'
+          hardirqs. This means that all (or selected) hardirqs will run
+          in their own kernel thread context. While this helps latency,
+          this feature can also reduce performance.
+
+          The threading of hardirqs can also be controlled via the
+          /proc/sys/kernel/hardirq_preemption runtime flag and the
+          hardirq-preempt=0/1 boot-time option. Per-irq threading can
+          be enabled/disable via the /proc/irq/<IRQ>/<handler>/threaded
+          runtime flags.
+
+	  Say N if you are unsure.
+
+config PREEMPT_REALTIME
+	bool "Real-Time Kernel Preemption"
+	default y
+	depends on PREEMPT && PREEMPT_VOLUNTARY && PREEMPT_SOFTIRQS && PREEMPT_HARDIRQS
+	help
+	  This option reduces the latency of the kernel by replacing
+	  almost every spinlock with semaphores and hence all but the
+	  most critical kernel code preemptible.
+
+	  Say Y here if you are building a kernel for a semi-hard-realtime
+	  system.
+	  Say N if you are unsure.
+
 config X86_UP_APIC
 	bool "Local APIC support on uniprocessors" if !SMP
 	depends on !(X86_VISWS || X86_VOYAGER)
@@ -900,7 +957,7 @@
 
 config REGPARM
 	bool "Use register arguments (EXPERIMENTAL)"
-	depends on EXPERIMENTAL
+	depends on EXPERIMENTAL && !MCOUNT
 	default n
 	help
 	Compile the kernel with -mregparm=3. This uses an different ABI
Index: xx-sources/arch/i386/boot/compressed/misc.c
===================================================================
--- xx-sources.orig/arch/i386/boot/compressed/misc.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/boot/compressed/misc.c	2004-10-16 20:56:50.000000000 -0400
@@ -15,6 +15,12 @@
 #include <video/edid.h>
 #include <asm/io.h>
 
+void mcount(void) __attribute ((no_instrument_function));
+
+void mcount(void)
+{
+}
+
 /*
  * gzip declarations
  */
Index: xx-sources/arch/i386/kernel/Makefile
===================================================================
--- xx-sources.orig/arch/i386/kernel/Makefile	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/Makefile	2004-10-16 20:56:50.000000000 -0400
@@ -21,6 +21,7 @@
 obj-$(CONFIG_APM)		+= apm.o
 obj-$(CONFIG_X86_SMP)		+= smp.o smpboot.o
 obj-$(CONFIG_X86_TRAMPOLINE)	+= trampoline.o
+obj-$(CONFIG_MCOUNT)		+= mcount-wrapper.o
 obj-$(CONFIG_X86_MPPARSE)	+= mpparse.o
 obj-$(CONFIG_X86_LOCAL_APIC)	+= apic.o nmi.o
 obj-$(CONFIG_X86_IO_APIC)	+= io_apic.o
Index: xx-sources/arch/i386/kernel/apic.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/apic.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/apic.c	2004-10-16 20:56:50.000000000 -0400
@@ -34,6 +34,7 @@
 #include <asm/desc.h>
 #include <asm/arch_hooks.h>
 #include <asm/hpet.h>
+#include <asm/i8253.h>
 
 #include <mach_apic.h>
 
@@ -863,7 +864,6 @@
  */
 static unsigned int __init get_8254_timer_count(void)
 {
-	extern spinlock_t i8253_lock;
 	unsigned long flags;
 
 	unsigned int count;
@@ -1171,7 +1171,7 @@
  *   interrupt as well. Thus we cannot inline the local irq ... ]
  */
 
-void smp_apic_timer_interrupt(struct pt_regs regs)
+void notrace smp_apic_timer_interrupt(struct pt_regs regs)
 {
 	int cpu = smp_processor_id();
 
@@ -1180,6 +1180,8 @@
 	 */
 	irq_stat[cpu].apic_timer_irqs++;
 
+        __trace((unsigned long)smp_apic_timer_interrupt, regs.eip);
+
 	/*
 	 * NOTE! We'd better ACK the irq immediately,
 	 * because timer handling can be slow.
Index: xx-sources/arch/i386/kernel/apm.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/apm.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/apm.c	2004-10-16 20:56:50.000000000 -0400
@@ -228,10 +228,10 @@
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/desc.h>
+#include <asm/i8253.h>
 
 #include "io_ports.h"
 
-extern spinlock_t i8253_lock;
 extern unsigned long get_cmos_time(void);
 extern void machine_real_restart(unsigned char *, int);
 
Index: xx-sources/arch/i386/kernel/cpu/mtrr/generic.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/cpu/mtrr/generic.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/cpu/mtrr/generic.c	2004-10-16 20:56:50.000000000 -0400
@@ -231,7 +231,7 @@
 
 static unsigned long cr4 = 0;
 static u32 deftype_lo, deftype_hi;
-static spinlock_t set_atomicity_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(set_atomicity_lock);
 
 static void prepare_set(void)
 {
Index: xx-sources/arch/i386/kernel/entry.S
===================================================================
--- xx-sources.orig/arch/i386/kernel/entry.S	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/entry.S	2004-10-16 20:56:50.000000000 -0400
@@ -189,6 +189,8 @@
 
 #ifdef CONFIG_PREEMPT
 ENTRY(resume_kernel)
+	cmpl $0, kernel_preemption
+	jz restore_all
 	cmpl $0,TI_preempt_count(%ebp)	# non-zero preempt_count ?
 	jnz restore_all
 need_resched:
Index: xx-sources/arch/i386/kernel/i386_ksyms.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/i386_ksyms.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/i386_ksyms.c	2004-10-16 20:56:50.000000000 -0400
@@ -16,6 +16,7 @@
 #include <linux/tty.h>
 #include <linux/highmem.h>
 #include <linux/time.h>
+#include <linux/mc146818rtc.h>
 
 #include <asm/semaphore.h>
 #include <asm/processor.h>
@@ -34,7 +35,6 @@
 #include <asm/kdebug.h>
 
 extern void dump_thread(struct pt_regs *, struct user *);
-extern spinlock_t rtc_lock;
 
 /* This is definitely a GPL-only symbol */
 EXPORT_SYMBOL_GPL(cpu_gdt_table);
@@ -175,7 +175,7 @@
 
 EXPORT_SYMBOL(register_die_notifier);
 #ifdef CONFIG_HAVE_DEC_LOCK
-EXPORT_SYMBOL(atomic_dec_and_lock);
+EXPORT_SYMBOL(_atomic_dec_and_spin_lock);
 #endif
 
 EXPORT_SYMBOL(__PAGE_KERNEL);
Index: xx-sources/arch/i386/kernel/i8259.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/i8259.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/i8259.c	2004-10-16 20:56:50.000000000 -0400
@@ -39,7 +39,7 @@
  * moves to arch independent land
  */
 
-spinlock_t i8259A_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_RAW_SPINLOCK(i8259A_lock);
 
 static void end_8259A_irq (unsigned int irq)
 {
@@ -371,7 +371,7 @@
  * New motherboards sometimes make IRQ 13 be a PCI interrupt,
  * so allow interrupt sharing.
  */
-static struct irqaction fpu_irq = { math_error_irq, 0, CPU_MASK_NONE, "fpu", NULL, NULL };
+static struct irqaction fpu_irq = { math_error_irq, SA_NODELAY, CPU_MASK_NONE, "fpu", NULL, NULL };
 
 void __init init_ISA_irqs (void)
 {
Index: xx-sources/arch/i386/kernel/io_apic.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/io_apic.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/io_apic.c	2004-10-16 20:56:50.000000000 -0400
@@ -37,6 +37,7 @@
 #include <asm/smp.h>
 #include <asm/desc.h>
 #include <asm/timer.h>
+#include <asm/i8259.h>
 
 #include <mach_apic.h>
 
@@ -44,7 +45,7 @@
 
 atomic_t irq_mis_count;
 
-static spinlock_t ioapic_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(ioapic_lock);
 
 /*
  *	Is the SiS APIC rmw bug present ?
@@ -139,6 +140,10 @@
 		reg &= ~disable;
 		reg |= enable;
 		io_apic_modify(entry->apic, 0x10 + pin*2, reg);
+		/*
+		 * Force POST flush by reading:
+	 	 */
+		reg = io_apic_read(entry->apic, 0x10 + pin*2);
 		if (!entry->next)
 			break;
 		entry = irq_2_pin + entry->next;
@@ -157,18 +162,6 @@
 	__modify_IO_APIC_irq(irq, 0, 0x00010000);
 }
 
-/* mask = 1, trigger = 0 */
-static void __mask_and_edge_IO_APIC_irq (unsigned int irq)
-{
-	__modify_IO_APIC_irq(irq, 0x00010000, 0x00008000);
-}
-
-/* mask = 0, trigger = 1 */
-static void __unmask_and_level_IO_APIC_irq (unsigned int irq)
-{
-	__modify_IO_APIC_irq(irq, 0x00008000, 0x00010000);
-}
-
 static void mask_IO_APIC_irq (unsigned int irq)
 {
 	unsigned long flags;
@@ -1556,7 +1549,6 @@
 
 void /*__init*/ print_PIC(void)
 {
-	extern spinlock_t i8259A_lock;
 	unsigned int v;
 	unsigned long flags;
 
@@ -1878,6 +1870,50 @@
 	return 0; /* don't check for pending */
 }
 
+/*
+ * In the preemptible case mask the IRQ first then handle it and ack it.
+ *
+ * (In the non-preemptible case we keep the IRQ unacked in the local APIC
+ * and dont need to do the masking, because the code executes atomically.)
+ */
+#ifdef CONFIG_PREEMPT_HARDIRQS
+static void mask_and_ack_level_ioapic_irq(unsigned int irq)
+{
+	move_irq(irq);
+	mask_IO_APIC_irq(irq);
+	ack_APIC_irq();
+}
+
+static void end_level_ioapic_irq(unsigned int irq)
+{
+	if (!(irq_desc[irq].status & (IRQ_DISABLED | IRQ_INPROGRESS)) &&
+							irq_desc[irq].action)
+		unmask_IO_APIC_irq(irq);
+}
+
+static void enable_level_ioapic_irq(unsigned int irq)
+{
+	unmask_IO_APIC_irq(irq);
+}
+
+#else /* !CONFIG_PREEMPT_HARDIRQS */
+
+static void mask_and_ack_level_ioapic_irq(unsigned int irq)
+{
+}
+
+/* mask = 1, trigger = 0 */
+static void __mask_and_edge_IO_APIC_irq (unsigned int irq)
+{
+	__modify_IO_APIC_irq(irq, 0x00010000, 0x00008000);
+}
+
+/* mask = 0, trigger = 1 */
+static void __unmask_and_level_IO_APIC_irq (unsigned int irq)
+{
+	__modify_IO_APIC_irq(irq, 0x00008000, 0x00010000);
+}
+
 static void end_level_ioapic_irq (unsigned int irq)
 {
 	unsigned long v;
@@ -1918,6 +1954,12 @@
 	}
 }
 
+static void enable_level_ioapic_irq(unsigned int irq)
+{
+	unmask_IO_APIC_irq(irq);
+}
+#endif /* !CONFIG_PREEMPT_HARDIRQS */
+
 #ifdef CONFIG_PCI_MSI
 static unsigned int startup_edge_ioapic_vector(unsigned int vector)
 {
@@ -1940,6 +1982,13 @@
 	return startup_level_ioapic_irq (irq);
 }
 
+static void mask_and_ack_level_ioapic_vector (unsigned int vector)
+{
+	int irq = vector_to_irq(vector);
+
+	mask_and_ack_level_ioapic_irq(irq);
+}
+
 static void end_level_ioapic_vector (unsigned int vector)
 {
 	int irq = vector_to_irq(vector);
@@ -1947,6 +1996,11 @@
 	end_level_ioapic_irq(irq);
 }
 
+static void enable_level_ioapic_vector(unsigned int vector)
+{
+	enable_level_ioapic_irq(vector_to_irq(vector));
+}
+
 static void mask_IO_APIC_vector (unsigned int vector)
 {
 	int irq = vector_to_irq(vector);
Index: xx-sources/arch/i386/kernel/irq.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/irq.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/irq.c	2004-10-16 20:56:50.000000000 -0400
@@ -45,7 +45,7 @@
  * SMP cross-CPU interrupts have their own specific
  * handlers).
  */
-asmlinkage unsigned int do_IRQ(struct pt_regs regs)
+asmlinkage notrace unsigned int do_IRQ(struct pt_regs regs)
 {	
 	/* high bits used in ret_from_ code */
 	int irq = regs.orig_eax & 0xff;
@@ -55,6 +55,8 @@
 #endif
 
 	irq_enter();
+	__trace((unsigned long)do_IRQ, regs.eip);
+	__trace((unsigned long)do_IRQ, irq);
 #ifdef CONFIG_DEBUG_STACKOVERFLOW
 	/* Debugging check for stack overflow: is there less than 1KB free? */
 	{
@@ -233,6 +235,7 @@
 
 		for (action=action->next; action; action = action->next)
 			seq_printf(p, ", %s", action->name);
+		seq_printf(p, "  %d/%d", irq_desc[i].irqs_unhandled, irq_desc[i].irq_count);
 
 		seq_putc(p, '\n');
 skip:
Index: xx-sources/arch/i386/kernel/mcount-wrapper.S
===================================================================
--- xx-sources.orig/arch/i386/kernel/mcount-wrapper.S	2004-10-14 14:49:41.073057592 -0400
+++ xx-sources/arch/i386/kernel/mcount-wrapper.S	2004-10-16 20:56:50.000000000 -0400
@@ -0,0 +1,27 @@
+/*
+ *  linux/arch/i386/mcount-wrapper.S
+ *
+ *  Copyright (C) 2004 Ingo Molnar
+ */
+
+.globl mcount
+mcount:
+
+	cmpl $0, trace_enabled
+	jz out
+
+	push %ebp
+	mov %esp, %ebp
+	pushl %eax
+	pushl %ecx
+	pushl %edx
+
+	call __mcount
+
+	popl %edx
+	popl %ecx
+	popl %eax
+	popl %ebp
+out:
+	ret
+
Index: xx-sources/arch/i386/kernel/nmi.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/nmi.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/nmi.c	2004-10-16 20:56:50.000000000 -0400
@@ -46,7 +46,7 @@
 #endif
 
 extern int unknown_nmi_panic;
-static unsigned int nmi_hz = HZ;
+static unsigned int nmi_hz = 10000;
 static unsigned int nmi_perfctr_msr;	/* the MSR to reset in NMI handler */
 static unsigned int nmi_p4_cccr_val;
 extern void show_registers(struct pt_regs *regs);
@@ -122,7 +122,7 @@
 	for (cpu = 0; cpu < NR_CPUS; cpu++)
 		prev_nmi_count[cpu] = irq_stat[cpu].__nmi_count;
 	local_irq_enable();
-	mdelay((10*1000)/nmi_hz); // wait 10 ticks
+	mdelay((100*1000)/nmi_hz); // wait 100 ticks
 
 	/* FIXME: Only boot CPU is online at this stage.  Check CPUs
            as they come up. */
@@ -141,7 +141,7 @@
 	/* now that we know it works we can reduce NMI frequency to
 	   something more reasonable; makes a difference in some configs */
 	if (nmi_watchdog == NMI_LOCAL_APIC)
-		nmi_hz = 1;
+		nmi_hz = 10000;
 
 	return 0;
 }
@@ -342,8 +342,8 @@
 		| K7_NMI_EVENT;
 
 	wrmsr(MSR_K7_EVNTSEL0, evntsel, 0);
-	Dprintk("setting K7_PERFCTR0 to %08lx\n", -(cpu_khz/nmi_hz*1000));
-	wrmsr(MSR_K7_PERFCTR0, -(cpu_khz/nmi_hz*1000), -1);
+	Dprintk("setting K7_PERFCTR0 to %08lx\n", -(cpu_khz*1000/nmi_hz));
+	wrmsr(MSR_K7_PERFCTR0, -(cpu_khz*1000/nmi_hz), -1);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 	evntsel |= K7_EVNTSEL_ENABLE;
 	wrmsr(MSR_K7_EVNTSEL0, evntsel, 0);
@@ -364,8 +364,8 @@
 		| P6_NMI_EVENT;
 
 	wrmsr(MSR_P6_EVNTSEL0, evntsel, 0);
-	Dprintk("setting P6_PERFCTR0 to %08lx\n", -(cpu_khz/nmi_hz*1000));
-	wrmsr(MSR_P6_PERFCTR0, -(cpu_khz/nmi_hz*1000), 0);
+	Dprintk("setting P6_PERFCTR0 to %08lx\n", -(cpu_khz*1000/nmi_hz));
+	wrmsr(MSR_P6_PERFCTR0, -(cpu_khz*1000/nmi_hz), 0);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 	evntsel |= P6_EVNTSEL0_ENABLE;
 	wrmsr(MSR_P6_EVNTSEL0, evntsel, 0);
@@ -405,8 +405,8 @@
 
 	wrmsr(MSR_P4_CRU_ESCR0, P4_NMI_CRU_ESCR0, 0);
 	wrmsr(MSR_P4_IQ_CCCR0, P4_NMI_IQ_CCCR0 & ~P4_CCCR_ENABLE, 0);
-	Dprintk("setting P4_IQ_COUNTER0 to 0x%08lx\n", -(cpu_khz/nmi_hz*1000));
-	wrmsr(MSR_P4_IQ_COUNTER0, -(cpu_khz/nmi_hz*1000), -1);
+	Dprintk("setting P4_IQ_COUNTER0 to 0x%08lx\n", -(cpu_khz*1000/nmi_hz));
+	wrmsr(MSR_P4_IQ_COUNTER0, -(cpu_khz*1000/nmi_hz), -1);
 	apic_write(APIC_LVTPC, APIC_DM_NMI);
 	wrmsr(MSR_P4_IQ_CCCR0, nmi_p4_cccr_val, 0);
 	return 1;
@@ -482,7 +482,29 @@
 
 extern void die_nmi(struct pt_regs *, const char *msg);
 
-void nmi_watchdog_tick (struct pt_regs * regs)
+int nmi_show_regs[NR_CPUS];
+
+void nmi_show_all_regs(void)
+{
+	int i;
+
+	if (nmi_watchdog == NMI_NONE)
+		return;
+	if (system_state != SYSTEM_RUNNING) {
+		printk("nmi_show_all_regs(): system state %d, not doing.\n",
+			system_state);
+		return;
+	}
+		
+	for_each_online_cpu(i)
+		nmi_show_regs[i] = 1;
+	for_each_online_cpu(i)
+		while (nmi_show_regs[i] == 1)
+			barrier();
+}
+static DECLARE_RAW_SPINLOCK(nmi_print_lock);
+ 
+void notrace nmi_watchdog_tick (struct pt_regs * regs)
 {
 
 	/*
@@ -494,6 +516,13 @@
 
 	sum = irq_stat[cpu].apic_timer_irqs;
 
+	if (nmi_show_regs[cpu]) {
+		nmi_show_regs[cpu] = 0;
+		spin_lock(&nmi_print_lock);
+		show_regs(regs);
+		spin_unlock(&nmi_print_lock);
+	}
+
 #ifdef CONFIG_KGDB
 	if (!in_kgdb(regs) && last_irq_sums[cpu] == sum) {
 
@@ -512,6 +541,12 @@
 			alert_counter[cpu] = 0;
 		}
 #endif
+		if (alert_counter[cpu] == 5*nmi_hz) {
+			int i;
+
+			for (i = 0; i < NR_CPUS; i++)
+				nmi_show_regs[i] = 1;
+		}
 		if (alert_counter[cpu] == 5*nmi_hz)
 			die_nmi(regs, "NMI Watchdog detected LOCKUP");
 	} else {
@@ -536,7 +571,7 @@
 			 * other P6 variant */
 			apic_write(APIC_LVTPC, APIC_DM_NMI);
 		}
-		wrmsr(nmi_perfctr_msr, -(cpu_khz/nmi_hz*1000), -1);
+		wrmsr(nmi_perfctr_msr, -(cpu_khz*1000/nmi_hz), -1);
 	}
 }
 
Index: xx-sources/arch/i386/kernel/process.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/process.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/process.c	2004-10-16 20:56:50.000000000 -0400
@@ -150,6 +150,7 @@
 			 * for our completion.
 			 */
 			rcu_read_lock();
+			stop_preempt_timing();
 			idle = pm_idle;
 
 			if (!idle)
Index: xx-sources/arch/i386/kernel/ptrace.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/ptrace.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/ptrace.c	2004-10-16 20:56:50.000000000 -0400
@@ -390,7 +390,7 @@
 		long tmp;
 
 		ret = 0;
-		if (child->state == TASK_ZOMBIE)	/* already dead */
+		if (child->exit_state == __TASK_ZOMBIE)	/* already dead */
 			break;
 		child->exit_code = SIGKILL;
 		clear_tsk_thread_flag(child, TIF_SINGLESTEP);
Index: xx-sources/arch/i386/kernel/signal.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/signal.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/signal.c	2004-10-16 20:56:50.000000000 -0400
@@ -576,6 +576,12 @@
 	int signr;
 	struct k_sigaction ka;
 
+#ifdef CONFIG_PREEMPT_REALTIME
+	/*
+	 * Fully-preemptible kernel does not need interrupts disabled:
+	 */
+	local_irq_enable();
+#endif
 	/*
 	 * We want the common case to go fast, which
 	 * is why we may in certain cases get here from
Index: xx-sources/arch/i386/kernel/smp.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/smp.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/smp.c	2004-10-16 20:56:50.000000000 -0400
@@ -166,12 +166,12 @@
 	unsigned long flags;
 
 	local_irq_save(flags);
-		
+
 	/*
 	 * Wait for idle.
 	 */
 	apic_wait_icr_idle();
-		
+
 	/*
 	 * prepare target chip field
 	 */
@@ -250,7 +250,7 @@
 static cpumask_t flush_cpumask;
 static struct mm_struct * flush_mm;
 static unsigned long flush_va;
-static spinlock_t tlbstate_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(tlbstate_lock);
 #define FLUSH_ALL	0xffffffff
 
 /*
@@ -395,7 +395,7 @@
 
 	while (!cpus_empty(flush_cpumask))
 		/* nothing. lockup detection does not belong here */
-		mb();
+		cpu_relax();
 
 	flush_mm = NULL;
 	flush_va = 0;
@@ -502,7 +502,7 @@
  * Structure and data for smp_call_function(). This is designed to minimise
  * static memory requirements. It also looks cleaner.
  */
-static spinlock_t call_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(call_lock);
 
 struct call_data_struct {
 	void (*func) (void *info);
@@ -600,8 +600,9 @@
  * all the work is done automatically when
  * we return from the interrupt.
  */
-asmlinkage void smp_reschedule_interrupt(void)
+asmlinkage notrace void smp_reschedule_interrupt(struct pt_regs regs)
 {
+	__trace((unsigned long)smp_reschedule_interrupt, regs.eip);
 	ack_APIC_irq();
 }
 
Index: xx-sources/arch/i386/kernel/time.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/time.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/time.c	2004-10-16 20:56:50.000000000 -0400
@@ -67,7 +67,8 @@
 
 #include "io_ports.h"
 
-extern spinlock_t i8259A_lock;
+#include <asm/i8259.h>
+
 int pit_latch_buggy;              /* extern */
 
 #include "do_timer.h"
@@ -80,9 +81,11 @@
 
 extern unsigned long wall_jiffies;
 
-spinlock_t rtc_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_RAW_SPINLOCK(rtc_lock);
+
+#include <asm/i8253.h>
 
-spinlock_t i8253_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_RAW_SPINLOCK(i8253_lock);
 EXPORT_SYMBOL(i8253_lock);
 
 struct timer_opts *cur_timer = &timer_none;
Index: xx-sources/arch/i386/kernel/timers/timer_cyclone.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/timers/timer_cyclone.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/timers/timer_cyclone.c	2004-10-16 20:56:50.000000000 -0400
@@ -17,9 +17,9 @@
 #include <asm/io.h>
 #include <asm/pgtable.h>
 #include <asm/fixmap.h>
-#include "io_ports.h"
+#include <asm/i8253.h>
 
-extern spinlock_t i8253_lock;
+#include "io_ports.h"
 
 /* Number of usecs that the last interrupt was delayed */
 static int delay_at_last_interrupt;
@@ -36,7 +36,7 @@
 static u32 last_cyclone_low;
 static u32 last_cyclone_high;
 static unsigned long long monotonic_base;
-static seqlock_t monotonic_lock = SEQLOCK_UNLOCKED;
+static DECLARE_RAW_SEQLOCK(monotonic_lock);
 
 /* helper macro to atomically read both cyclone counter registers */
 #define read_cyclone_counter(low,high) \
Index: xx-sources/arch/i386/kernel/timers/timer_hpet.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/timers/timer_hpet.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/timers/timer_hpet.c	2004-10-16 20:56:50.000000000 -0400
@@ -24,7 +24,7 @@
 static unsigned long last_tsc_low;	/* lsb 32 bits of Time Stamp Counter */
 static unsigned long last_tsc_high; 	/* msb 32 bits of Time Stamp Counter */
 static unsigned long long monotonic_base;
-static seqlock_t monotonic_lock = SEQLOCK_UNLOCKED;
+static DECLARE_RAW_SEQLOCK(monotonic_lock);
 
 /* convert from cycles(64bits) => nanoseconds (64bits)
  *  basic equation:
Index: xx-sources/arch/i386/kernel/timers/timer_pit.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/timers/timer_pit.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/timers/timer_pit.c	2004-10-16 20:56:50.000000000 -0400
@@ -15,9 +15,8 @@
 #include <asm/smp.h>
 #include <asm/io.h>
 #include <asm/arch_hooks.h>
+#include <asm/i8253.h>
 
-extern spinlock_t i8259A_lock;
-extern spinlock_t i8253_lock;
 #include "do_timer.h"
 #include "io_ports.h"
 
@@ -162,7 +161,6 @@
 
 void setup_pit_timer(void)
 {
-	extern spinlock_t i8253_lock;
 	unsigned long flags;
 
 	spin_lock_irqsave(&i8253_lock, flags);
Index: xx-sources/arch/i386/kernel/timers/timer_pm.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/timers/timer_pm.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/timers/timer_pm.c	2004-10-16 20:56:50.000000000 -0400
@@ -41,7 +41,7 @@
 static u32 offset_delay;
 
 static unsigned long long monotonic_base;
-static seqlock_t monotonic_lock = SEQLOCK_UNLOCKED;
+static DECLARE_RAW_SEQLOCK(monotonic_lock);
 
 #define ACPI_PM_MASK 0xFFFFFF /* limit it to 24 bits */
 
Index: xx-sources/arch/i386/kernel/timers/timer_tsc.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/timers/timer_tsc.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/timers/timer_tsc.c	2004-10-16 20:56:50.000000000 -0400
@@ -24,6 +24,7 @@
 #include "mach_timer.h"
 
 #include <asm/hpet.h>
+#include <asm/i8253.h>
 
 #ifdef CONFIG_HPET_TIMER
 static unsigned long hpet_usec_quotient;
@@ -35,8 +36,6 @@
 
 int tsc_disable __initdata = 0;
 
-extern spinlock_t i8253_lock;
-
 static int use_tsc;
 /* Number of usecs that the last interrupt was delayed */
 static int delay_at_last_interrupt;
@@ -44,7 +43,7 @@
 static unsigned long last_tsc_low; /* lsb 32 bits of Time Stamp Counter */
 static unsigned long last_tsc_high; /* msb 32 bits of Time Stamp Counter */
 static unsigned long long monotonic_base;
-static seqlock_t monotonic_lock = SEQLOCK_UNLOCKED;
+static DECLARE_RAW_SEQLOCK(monotonic_lock);
 
 /* convert from cycles(64bits) => nanoseconds (64bits)
  *  basic equation:
Index: xx-sources/arch/i386/kernel/traps.c
===================================================================
--- xx-sources.orig/arch/i386/kernel/traps.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/kernel/traps.c	2004-10-16 20:56:50.000000000 -0400
@@ -93,7 +93,7 @@
 
 static int kstack_depth_to_print = 24;
 struct notifier_block *i386die_chain;
-static spinlock_t die_notifier_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(die_notifier_lock);
 
 int register_die_notifier(struct notifier_block *nb)
 {
@@ -196,6 +196,7 @@
 			break;
 		printk(" =======================\n");
 	}
+	print_traces();
 }
 
 void show_stack(struct task_struct *task, unsigned long *esp)
@@ -261,8 +262,8 @@
 		regs->eax, regs->ebx, regs->ecx, regs->edx);
 	printk("esi: %08lx   edi: %08lx   ebp: %08lx   esp: %08lx\n",
 		regs->esi, regs->edi, regs->ebp, esp);
-	printk("ds: %04x   es: %04x   ss: %04x\n",
-		regs->xds & 0xffff, regs->xes & 0xffff, ss);
+	printk("ds: %04x   es: %04x   ss: %04x   preempt: %08x\n",
+		regs->xds & 0xffff, regs->xes & 0xffff, ss, preempt_count());
 	printk("Process %s (pid: %d, threadinfo=%p task=%p)",
 		current->comm, current->pid, current_thread_info(), current);
 	/*
@@ -333,11 +334,11 @@
 void die(const char * str, struct pt_regs * regs, long err)
 {
 	static struct {
-		spinlock_t lock;
+		raw_spinlock_t lock;
 		u32 lock_owner;
 		int lock_owner_depth;
 	} die = {
-		.lock =			SPIN_LOCK_UNLOCKED,
+		.lock =			RAW_SPIN_LOCK_UNLOCKED,
 		.lock_owner =		-1,
 		.lock_owner_depth =	0
 	};
@@ -429,6 +430,10 @@
 	if (!(regs->xcs & 3))
 		goto kernel_trap;
 
+#ifdef CONFIG_PREEMPT_REALTIME
+	local_irq_enable();
+#endif
+
 	trap_signal: {
 		struct task_struct *tsk = current;
 		tsk->thread.error_code = error_code;
@@ -613,7 +618,7 @@
 	printk("Do you have a strange power saving mode enabled?\n");
 }
 
-static spinlock_t nmi_print_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(nmi_print_lock);
 
 void die_nmi (struct pt_regs *regs, const char *msg)
 {
@@ -631,10 +636,11 @@
 	console_silent();
 	spin_unlock(&nmi_print_lock);
 	bust_spinlocks(0);
+	nmi_exit();
 	do_exit(SIGSEGV);
 }
 
-static void default_do_nmi(struct pt_regs * regs)
+static void notrace default_do_nmi(struct pt_regs * regs)
 {
 	unsigned char reason = get_nmi_reason();
  
@@ -668,18 +674,19 @@
 	reassert_nmi();
 }
 
-static int dummy_nmi_callback(struct pt_regs * regs, int cpu)
+static int notrace dummy_nmi_callback(struct pt_regs * regs, int cpu)
 {
 	return 0;
 }
  
 static nmi_callback_t nmi_callback = dummy_nmi_callback;
- 
-asmlinkage void do_nmi(struct pt_regs * regs, long error_code)
+
+asmlinkage notrace void do_nmi(struct pt_regs * regs, long error_code)
 {
 	int cpu;
 
 	nmi_enter();
+	nmi_trace((unsigned long)do_nmi, regs->eip, regs->eflags);
 
 	cpu = smp_processor_id();
 	++nmi_count(cpu);
Index: xx-sources/arch/i386/lib/dec_and_lock.c
===================================================================
--- xx-sources.orig/arch/i386/lib/dec_and_lock.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/lib/dec_and_lock.c	2004-10-16 20:56:50.000000000 -0400
@@ -10,7 +10,7 @@
 #include <linux/spinlock.h>
 #include <asm/atomic.h>
 
-int atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock)
+int _atomic_dec_and_spin_lock(atomic_t *atomic, raw_spinlock_t *lock)
 {
 	int counter;
 	int newcount;
@@ -32,9 +32,9 @@
 	return 0;
 
 slow_path:
-	spin_lock(lock);
+	_spin_lock(lock);
 	if (atomic_dec_and_test(atomic))
 		return 1;
-	spin_unlock(lock);
+	_spin_unlock(lock);
 	return 0;
 }
Index: xx-sources/arch/i386/mach-default/setup.c
===================================================================
--- xx-sources.orig/arch/i386/mach-default/setup.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mach-default/setup.c	2004-10-16 20:56:50.000000000 -0400
@@ -27,7 +27,7 @@
 /*
  * IRQ2 is cascade interrupt to second interrupt controller
  */
-static struct irqaction irq2 = { no_action, 0, CPU_MASK_NONE, "cascade", NULL, NULL};
+static struct irqaction irq2 = { no_action, SA_NODELAY, CPU_MASK_NONE, "cascade", NULL, NULL};
 
 /**
  * intr_init_hook - post gate setup interrupt initialisation
@@ -71,7 +71,7 @@
 {
 }
 
-static struct irqaction irq0  = { timer_interrupt, SA_INTERRUPT, CPU_MASK_NONE, "timer", NULL, NULL};
+static struct irqaction irq0  = { timer_interrupt, SA_INTERRUPT | SA_NODELAY, CPU_MASK_NONE, "timer", NULL, NULL};
 
 /**
  * time_init_hook - do any specific initialisations for the system timer.
Index: xx-sources/arch/i386/mach-visws/setup.c
===================================================================
--- xx-sources.orig/arch/i386/mach-visws/setup.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mach-visws/setup.c	2004-10-16 20:56:50.000000000 -0400
@@ -112,7 +112,7 @@
 
 static struct irqaction irq0 = {
 	.handler =	timer_interrupt,
-	.flags =	SA_INTERRUPT,
+	.flags =	SA_INTERRUPT | SA_NODELAY,
 	.name =		"timer",
 };
 
Index: xx-sources/arch/i386/mach-visws/visws_apic.c
===================================================================
--- xx-sources.orig/arch/i386/mach-visws/visws_apic.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mach-visws/visws_apic.c	2004-10-16 20:56:50.000000000 -0400
@@ -261,11 +261,13 @@
 static struct irqaction master_action = {
 	.handler =	piix4_master_intr,
 	.name =		"PIIX4-8259",
+	.flags =	SA_NODELAY,
 };
 
 static struct irqaction cascade_action = {
 	.handler = 	no_action,
 	.name =		"cascade",
+	.flags =	SA_NODELAY,
 };
 
 
Index: xx-sources/arch/i386/mach-voyager/setup.c
===================================================================
--- xx-sources.orig/arch/i386/mach-voyager/setup.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mach-voyager/setup.c	2004-10-16 20:56:50.000000000 -0400
@@ -17,7 +17,7 @@
 /*
  * IRQ2 is cascade interrupt to second interrupt controller
  */
-static struct irqaction irq2 = { no_action, 0, CPU_MASK_NONE, "cascade", NULL, NULL};
+static struct irqaction irq2 = { no_action, SA_NODELAY, CPU_MASK_NONE, "cascade", NULL, NULL};
 
 void __init intr_init_hook(void)
 {
@@ -40,7 +40,7 @@
 {
 }
 
-static struct irqaction irq0  = { timer_interrupt, SA_INTERRUPT, CPU_MASK_NONE, "timer", NULL, NULL};
+static struct irqaction irq0  = { timer_interrupt, SA_INTERRUPT | SA_NODELAY, CPU_MASK_NONE, "timer", NULL, NULL};
 
 void __init time_init_hook(void)
 {
Index: xx-sources/arch/i386/mach-voyager/voyager_basic.c
===================================================================
--- xx-sources.orig/arch/i386/mach-voyager/voyager_basic.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mach-voyager/voyager_basic.c	2004-10-16 20:56:50.000000000 -0400
@@ -30,6 +30,7 @@
 #include <linux/irq.h>
 #include <asm/tlbflush.h>
 #include <asm/arch_hooks.h>
+#include <asm/i8253.h>
 
 /*
  * Power off function, if any
@@ -184,7 +185,6 @@
 		 * and swiftly introduce it to something sharp and
 		 * pointy.  */
 		__u16 val;
-		extern spinlock_t i8253_lock;
 
 		spin_lock(&i8253_lock);
 		
Index: xx-sources/arch/i386/mm/fault.c
===================================================================
--- xx-sources.orig/arch/i386/mm/fault.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mm/fault.c	2004-10-16 20:56:50.000000000 -0400
@@ -38,6 +38,7 @@
 	int loglevel_save = console_loglevel;
 
 	if (yes) {
+		stop_trace();
 		oops_in_progress = 1;
 		return;
 	}
@@ -213,7 +214,7 @@
  *	bit 1 == 0 means read, 1 means write
  *	bit 2 == 0 means kernel, 1 means user-mode
  */
-asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code)
+asmlinkage void notrace do_page_fault(struct pt_regs *regs, unsigned long error_code)
 {
 	struct task_struct *tsk;
 	struct mm_struct *mm;
@@ -223,6 +224,8 @@
 	int write;
 	siginfo_t info;
 
+	__trace((unsigned long)do_page_fault, regs->eip);
+
 	/* get the address */
 	__asm__("movl %%cr2,%0":"=r" (address));
 
Index: xx-sources/arch/i386/mm/highmem.c
===================================================================
--- xx-sources.orig/arch/i386/mm/highmem.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mm/highmem.c	2004-10-16 20:56:50.000000000 -0400
@@ -17,6 +17,27 @@
 	kunmap_high(page);
 }
 
+void kunmap_virt(void *ptr)
+{
+	struct page *page;
+
+	if ((unsigned long)ptr < PKMAP_ADDR(0))
+		return;
+	page = pte_page(pkmap_page_table[PKMAP_NR((unsigned long)ptr)]);
+	kunmap(page);
+}
+
+struct page *kmap_to_page(void *ptr)
+{
+	struct page *page;
+
+	if ((unsigned long)ptr < PKMAP_ADDR(0))
+		return virt_to_page(ptr);
+	page = pte_page(pkmap_page_table[PKMAP_NR((unsigned long)ptr)]);
+	return page;
+}
+
+
 /*
  * kmap_atomic/kunmap_atomic is significantly faster than kmap/kunmap because
  * no global lock is needed and because the kmap code must perform a global TLB
Index: xx-sources/arch/i386/mm/pageattr.c
===================================================================
--- xx-sources.orig/arch/i386/mm/pageattr.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mm/pageattr.c	2004-10-16 20:56:50.000000000 -0400
@@ -13,7 +13,7 @@
 #include <asm/processor.h>
 #include <asm/tlbflush.h>
 
-static spinlock_t cpa_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(cpa_lock);
 static struct list_head df_list = LIST_HEAD_INIT(df_list);
 
 
Index: xx-sources/arch/i386/mm/pgtable.c
===================================================================
--- xx-sources.orig/arch/i386/mm/pgtable.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/i386/mm/pgtable.c	2004-10-16 20:56:50.000000000 -0400
@@ -176,7 +176,7 @@
  * recommendations and having no core impact whatsoever.
  * -- wli
  */
-spinlock_t pgd_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_RAW_SPINLOCK(pgd_lock);
 struct page *pgd_list;
 
 static inline void pgd_list_add(pgd_t *pgd)
Index: xx-sources/arch/x86_64/Kconfig
===================================================================
--- xx-sources.orig/arch/x86_64/Kconfig	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/x86_64/Kconfig	2004-10-16 20:56:50.000000000 -0400
@@ -256,6 +256,54 @@
 	  Say Y here if you are building a kernel for a desktop system.
 	  Say N if you are unsure.
 
+config PREEMPT_VOLUNTARY
+	bool "Voluntary Kernel Preemption"
+	default y
+	help
+	  This option reduces the latency of the kernel by adding more
+	  "explicit preemption points" to the kernel code. These new
+	  preemption points have been selected to minimize the maximum
+	  latency of rescheduling, providing faster application reactions.
+
+	  Say Y here if you are building a kernel for a desktop system.
+	  Say N if you are unsure.
+
+config PREEMPT_SOFTIRQS
+	bool "Preempt Softirqs"
+	default y
+	help
+	  This option reduces the latency of the kernel by 'threading'
+          soft interrupts. This means that all softirqs will execute
+          in softirqd's context. While this helps latency, it can also
+          reduce performance.
+
+          The threading of softirqs can also be controlled via
+          /proc/sys/kernel/softirq_preemption runtime flag and the
+          sofirq-preempt=0/1 boot-time option.
+
+	  Say N if you are unsure.
+
+#
+# Not Yet
+#
+# config PREEMPT_HARDIRQS
+# 	bool "Preempt Hardirqs"
+# 	default y
+# 	help
+#           This option reduces the latency of the kernel by 'threading'
+#           hardirqs. This means that all (or selected) hardirqs will run
+#           in their own kernel thread context. While this helps latency,
+#           this feature can also reduce performance.
+# 
+#           The threading of hardirqs can also be controlled via the
+#           /proc/sys/kernel/hardirq_preemption runtime flag and the
+#           hardirq-preempt=0/1 boot-time option. Per-irq threading can
+#           be enabled/disable via the /proc/irq/<IRQ>/<handler>/threaded
+#           runtime flags.
+#
+#           Say N if you are unsure.
+
+
 config SCHED_SMT
 	bool "SMT (Hyperthreading) scheduler support"
 	depends on SMP
Index: xx-sources/arch/x86_64/kernel/entry.S
===================================================================
--- xx-sources.orig/arch/x86_64/kernel/entry.S	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/x86_64/kernel/entry.S	2004-10-16 20:56:50.000000000 -0400
@@ -924,3 +924,40 @@
 ENTRY(call_debug)
        zeroentry do_call_debug
 
+#ifdef CONFIG_LATENCY_TRACE
+
+ENTRY(mcount)
+	cmpq $0, trace_enabled
+	jz out
+
+	push %rbp
+	mov %rsp,%rbp
+
+	push %r9
+	push %r8
+	push %rdi
+	push %rsi
+	push %rdx
+	push %rcx
+	push %rax
+
+	mov 0x0(%rbp),%rax
+	mov 0x8(%rbp),%rdi
+	mov 0x8(%rax),%rsi
+	
+	call   ___trace
+
+	pop %rax
+	pop %rcx
+	pop %rdx
+	pop %rsi
+	pop %rdi
+	pop %r8
+	pop %r9
+
+	leaveq
+out:
+	ret
+
+#endif
+
Index: xx-sources/arch/x86_64/mm/fault.c
===================================================================
--- xx-sources.orig/arch/x86_64/mm/fault.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/arch/x86_64/mm/fault.c	2004-10-16 20:56:50.000000000 -0400
@@ -37,6 +37,7 @@
 {
 	int loglevel_save = console_loglevel;
 	if (yes) {
+		stop_trace();
 		oops_in_progress = 1;
 	} else {
 #ifdef CONFIG_VT
Index: xx-sources/drivers/block/floppy.c
===================================================================
--- xx-sources.orig/drivers/block/floppy.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/block/floppy.c	2004-10-16 20:56:51.000000000 -0400
@@ -224,7 +224,7 @@
  * record each buffers capabilities
  */
 
-static spinlock_t floppy_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(floppy_lock);
 static struct completion device_release;
 
 static unsigned short virtual_dma_port = 0x3f0;
@@ -1080,7 +1080,7 @@
 	return 0;
 }
 
-static spinlock_t floppy_hlt_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(floppy_hlt_lock);
 static int hlt_disabled;
 static void floppy_disable_hlt(void)
 {
@@ -4588,7 +4588,7 @@
 	return err;
 }
 
-static spinlock_t floppy_usage_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(floppy_usage_lock);
 
 static int floppy_grab_irq_and_dma(void)
 {
Index: xx-sources/drivers/block/ll_rw_blk.c
===================================================================
--- xx-sources.orig/drivers/block/ll_rw_blk.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/block/ll_rw_blk.c	2004-10-16 20:56:51.000000000 -0400
@@ -1212,7 +1212,9 @@
  */
 void blk_plug_device(request_queue_t *q)
 {
+#ifndef CONFIG_PREEMPT_REALTIME
 	WARN_ON(!irqs_disabled());
+#endif
 
 	/*
 	 * don't plug a stopped queue, it must be paired with blk_start_queue()
@@ -1233,7 +1235,9 @@
  */
 int blk_remove_plug(request_queue_t *q)
 {
+#ifndef CONFIG_PREEMPT_REALTIME
 	WARN_ON(!irqs_disabled());
+#endif
 
 	if (!test_and_clear_bit(QUEUE_FLAG_PLUGGED, &q->queue_flags))
 		return 0;
Index: xx-sources/drivers/char/random.c
===================================================================
--- xx-sources.orig/drivers/char/random.c	2004-10-16 20:56:44.000000000 -0400
+++ xx-sources/drivers/char/random.c	2004-10-16 20:56:51.000000000 -0400
@@ -497,7 +497,7 @@
 	const char	*name;
 
 	/* read-write data: */
-	spinlock_t lock ____cacheline_aligned_in_smp;
+	raw_spinlock_t lock ____cacheline_aligned_in_smp;
 	unsigned	add_ptr;
 	int		entropy_count;
 	int		input_rotate;
@@ -540,7 +540,7 @@
 		return -ENOMEM;
 	}
 	memset(r->pool, 0, POOLBYTES);
-	r->lock = SPIN_LOCK_UNLOCKED;
+	r->lock = RAW_SPIN_LOCK_UNLOCKED;
 	r->name = name;
 	*ret_bucket = r;
 	return 0;
@@ -669,7 +669,7 @@
 
 static struct sample *batch_entropy_pool, *batch_entropy_copy;
 static int	batch_head, batch_tail;
-static spinlock_t batch_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(batch_lock);
 
 static int	batch_max;
 static void batch_entropy_process(void *private_);
Index: xx-sources/drivers/char/rtc.c
===================================================================
--- xx-sources.orig/drivers/char/rtc.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/char/rtc.c	2004-10-16 20:56:51.000000000 -0400
@@ -177,7 +177,7 @@
 /*
  * rtc_task_lock nests inside rtc_lock.
  */
-static spinlock_t rtc_task_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(rtc_task_lock);
 static rtc_task_t *rtc_callback = NULL;
 #endif
 
Index: xx-sources/drivers/char/sysrq.c
===================================================================
--- xx-sources.orig/drivers/char/sysrq.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/char/sysrq.c	2004-10-16 20:56:51.000000000 -0400
@@ -167,6 +167,21 @@
 	.action_msg	= "Show Regs",
 };
 
+#if defined(__i386__) && defined(CONFIG_SMP)
+ 
+ static void sysrq_handle_showallregs(int key, struct pt_regs *pt_regs,
+ 				  struct tty_struct *tty) 
+{
+	nmi_show_all_regs();
+}
+
+static struct sysrq_key_op sysrq_showallregs_op = {
+	.handler	= sysrq_handle_showallregs,
+	.help_msg	= "showalLcpupc",
+	.action_msg	= "Show Regs On All CPUs",
+};
+
+#endif
 
 static void sysrq_handle_showstate(int key, struct pt_regs *pt_regs,
 				   struct tty_struct *tty) 
@@ -237,7 +252,7 @@
 
 
 /* Key Operations table and lock */
-static spinlock_t sysrq_key_table_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(sysrq_key_table_lock);
 #define SYSRQ_KEY_TABLE_LENGTH 36
 static struct sysrq_key_op *sysrq_key_table[SYSRQ_KEY_TABLE_LENGTH] = {
 /* 0 */	&sysrq_loglevel_op,
@@ -267,7 +282,11 @@
 #else
 /* k */	NULL,
 #endif
+#if defined(__i386__) && defined(CONFIG_SMP)
+/* l */	&sysrq_showallregs_op,
+#else
 /* l */	NULL,
+#endif
 /* m */	&sysrq_showmem_op,
 /* n */	NULL,
 /* o */	NULL, /* This will often be registered
Index: xx-sources/drivers/char/tty_io.c
===================================================================
--- xx-sources.orig/drivers/char/tty_io.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/char/tty_io.c	2004-10-16 20:56:52.000000000 -0400
@@ -826,8 +826,8 @@
 				p->signal->tty = NULL;
 			if (!p->signal->leader)
 				continue;
-			send_group_sig_info(SIGHUP, SEND_SIG_PRIV, p);
-			send_group_sig_info(SIGCONT, SEND_SIG_PRIV, p);
+			group_send_sig_info(SIGHUP, SEND_SIG_PRIV, p);
+			group_send_sig_info(SIGCONT, SEND_SIG_PRIV, p);
 			if (tty->pgrp > 0)
 				p->signal->tty_old_pgrp = tty->pgrp;
 		} while_each_task_pid(tty->session, PIDTYPE_SID, p);
Index: xx-sources/drivers/char/vt.c
===================================================================
--- xx-sources.orig/drivers/char/vt.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/char/vt.c	2004-10-16 20:56:56.000000000 -0400
@@ -2926,16 +2926,16 @@
 	/* This isn't perfectly race free, but a race here would be mostly harmless,
 	 * at worse, we'll do a spurrious blank and it's unlikely
 	 */
-	del_timer(&console_timer);
-	blank_timer_expired = 0;
+//	del_timer(&console_timer);
+//	blank_timer_expired = 0;
 
 	if (ignore_poke || !vt_cons[fg_console] || vt_cons[fg_console]->vc_mode == KD_GRAPHICS)
 		return;
 	if (console_blanked)
 		unblank_screen();
 	else if (blankinterval) {
-		mod_timer(&console_timer, jiffies + blankinterval);
-		blank_state = blank_normal_wait;
+//		mod_timer(&console_timer, jiffies + blankinterval);
+//		blank_state = blank_normal_wait;
 	}
 }
 
Index: xx-sources/drivers/ide/ide-io.c
===================================================================
--- xx-sources.orig/drivers/ide/ide-io.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/ide/ide-io.c	2004-10-16 20:56:57.000000000 -0400
@@ -115,6 +115,9 @@
 	int ret = 1;
 
 	BUG_ON(!(rq->flags & REQ_STARTED));
+	spin_unlock(&ide_lock);
+	if (drive->unmask)
+		local_irq_enable();
 
 	/*
 	 * if failfast is set on a request, override number of sectors and
@@ -136,6 +139,7 @@
 	}
 
 	if (!end_that_request_first(rq, uptodate, nr_sectors)) {
+		spin_lock_irq(&ide_lock);
 		add_disk_randomness(rq->rq_disk);
 
 		if (blk_rq_tagged(rq))
@@ -145,7 +149,8 @@
 		HWGROUP(drive)->rq = NULL;
 		end_that_request_last(rq);
 		ret = 0;
-	}
+	} else
+		spin_lock_irq(&ide_lock);
 	return ret;
 }
 
@@ -1087,7 +1092,9 @@
 	ide_get_lock(ide_intr, hwgroup);
 
 	/* caller must own ide_lock */
+#ifndef CONFIG_PREEMPT_REALTIME
 	BUG_ON(!irqs_disabled());
+#endif
 
 	while (!hwgroup->busy) {
 		hwgroup->busy = 1;
Index: xx-sources/drivers/ide/ide.c
===================================================================
--- xx-sources.orig/drivers/ide/ide.c	2004-10-16 20:56:46.000000000 -0400
+++ xx-sources/drivers/ide/ide.c	2004-10-16 20:56:57.000000000 -0400
@@ -175,7 +175,7 @@
 static int initializing;	/* set while initializing built-in drivers */
 
 DECLARE_MUTEX(ide_cfg_sem);
-spinlock_t ide_lock __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(ide_lock);
 
 #ifdef CONFIG_BLK_DEV_IDEPCI
 static int ide_scan_direction; /* THIS was formerly 2.2.x pci=reverse */
Index: xx-sources/drivers/input/serio/i8042.c
===================================================================
--- xx-sources.orig/drivers/input/serio/i8042.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/input/serio/i8042.c	2004-10-16 20:56:57.000000000 -0400
@@ -74,7 +74,7 @@
 #undef DEBUG
 #include "i8042.h"
 
-spinlock_t i8042_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(i8042_lock);
 
 struct i8042_values {
 	int irq;
Index: xx-sources/drivers/input/serio/serio.c
===================================================================
--- xx-sources.orig/drivers/input/serio/serio.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/input/serio/serio.c	2004-10-16 20:56:57.000000000 -0400
@@ -114,7 +114,7 @@
 	SERIO_UNREGISTER_PORT,
 };
 
-static spinlock_t serio_event_lock = SPIN_LOCK_UNLOCKED;	/* protects serio_event_list */
+static DECLARE_SPINLOCK(serio_event_lock);	/* protects serio_event_list */
 static LIST_HEAD(serio_event_list);
 static DECLARE_WAIT_QUEUE_HEAD(serio_wait);
 static DECLARE_COMPLETION(serio_exited);
Index: xx-sources/drivers/net/8139too.c
===================================================================
--- xx-sources.orig/drivers/net/8139too.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/net/8139too.c	2004-10-16 20:56:58.000000000 -0400
@@ -2062,6 +2062,8 @@
 		RTL_W16 (RxBufPtr, (u16) (cur_rx - 16));
 
 		rtl8139_isr_ack(tp);
+		if (softirq_need_resched())
+			break;
 	}
 
 	if (unlikely(!received || rx_size == 0xfff0))
Index: xx-sources/drivers/net/e100.c
===================================================================
--- xx-sources.orig/drivers/net/e100.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/net/e100.c	2004-10-16 20:56:58.000000000 -0400
@@ -1518,6 +1518,8 @@
 
 	rx->skb = NULL;
 
+	if (softirq_need_resched())
+		return -EAGAIN;
 	return 0;
 }
 
Index: xx-sources/drivers/usb/core/hcd.c
===================================================================
--- xx-sources.orig/drivers/usb/core/hcd.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/usb/core/hcd.c	2004-10-16 20:56:59.000000000 -0400
@@ -333,7 +333,7 @@
 	u8		*ubuf = urb->transfer_buffer;
 	int		len = 0;
 	int		patch_wakeup = 0;
-	unsigned long	flags;
+//	unsigned long	flags;
 
 	cmd = (struct usb_ctrlrequest *) urb->setup_packet;
 	typeReq  = (cmd->bRequestType << 8) | cmd->bRequest;
@@ -467,9 +467,9 @@
 	}
 
 	/* any errors get returned through the urb completion */
-	local_irq_save (flags);
+//	local_irq_save (flags);
 	usb_hcd_giveback_urb (hcd, urb, NULL);
-	local_irq_restore (flags);
+//	local_irq_restore (flags);
 	return 0;
 }
 
@@ -517,15 +517,13 @@
 	unsigned long	flags;
 
 	urb = (struct urb *) ptr;
-	local_irq_save (flags);
-	spin_lock (&urb->lock);
+	spin_lock_irqsave (&urb->lock, flags);
 
 	/* do nothing if the urb's been unlinked */
 	if (!urb->dev
 			|| urb->status != -EINPROGRESS
 			|| (hcd = urb->dev->bus->hcpriv) == 0) {
-		spin_unlock (&urb->lock);
-		local_irq_restore (flags);
+		spin_unlock_irqrestore (&urb->lock, flags);
 		return;
 	}
 
@@ -543,12 +541,12 @@
 	} else
 		mod_timer (&hcd->rh_timer, jiffies + HZ/4);
 	spin_unlock (&hcd_data_lock);
-	spin_unlock (&urb->lock);
 
 	/* local irqs are always blocked in completions */
 	if (length > 0)
 		usb_hcd_giveback_urb (hcd, urb, NULL);
-	local_irq_restore (flags);
+
+	spin_unlock_irqrestore (&urb->lock, flags);
 }
 
 /*-------------------------------------------------------------------------*/
@@ -574,16 +572,16 @@
 
 int usb_rh_status_dequeue (struct usb_hcd *hcd, struct urb *urb)
 {
-	unsigned long	flags;
+//	unsigned long	flags;
 
 	/* note:  always a synchronous unlink */
 	del_timer_sync (&hcd->rh_timer);
 	hcd->rh_timer.data = 0;
 
-	local_irq_save (flags);
+//	local_irq_save (flags);
 	urb->hcpriv = NULL;
 	usb_hcd_giveback_urb (hcd, urb, NULL);
-	local_irq_restore (flags);
+//	local_irq_restore (flags);
 	return 0;
 }
 
@@ -1310,9 +1308,10 @@
 
 	WARN_ON (!HCD_IS_RUNNING (hcd->state) && hcd->state != USB_STATE_HALT);
 
-	local_irq_disable ();
-
 rescan:
+#ifndef CONFIG_PREEMPT_REALTIME
+	local_irq_disable();
+#endif
 	/* (re)block new requests, as best we can */
 	if (endpoint & USB_DIR_IN)
 		udev->epmaxpacketin [epnum] = 0;
@@ -1320,7 +1319,7 @@
 		udev->epmaxpacketout [epnum] = 0;
 
 	/* then kill any current requests */
-	spin_lock (&hcd_data_lock);
+	spin_lock_irq (&hcd_data_lock);
 	list_for_each_entry (urb, &dev->urb_list, urb_list) {
 		int	tmp = urb->pipe;
 
@@ -1339,13 +1338,13 @@
 		if (urb->status != -EINPROGRESS)
 			continue;
 		usb_get_urb (urb);
-		spin_unlock (&hcd_data_lock);
+		spin_unlock_irq (&hcd_data_lock);
 
-		spin_lock (&urb->lock);
+		spin_lock_irq (&urb->lock);
 		tmp = urb->status;
 		if (tmp == -EINPROGRESS)
 			urb->status = -ESHUTDOWN;
-		spin_unlock (&urb->lock);
+		spin_unlock_irq (&urb->lock);
 
 		/* kick hcd unless it's already returning this */
 		if (tmp == -EINPROGRESS) {
@@ -1368,8 +1367,7 @@
 		/* list contents may have changed */
 		goto rescan;
 	}
-	spin_unlock (&hcd_data_lock);
-	local_irq_enable ();
+	spin_unlock_irq (&hcd_data_lock);
 
 	/* synchronize with the hardware, so old configuration state
 	 * clears out immediately (and will be freed).
Index: xx-sources/drivers/video/console/vgacon.c
===================================================================
--- xx-sources.orig/drivers/video/console/vgacon.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/drivers/video/console/vgacon.c	2004-10-16 22:00:51.836379336 -0400
@@ -53,7 +53,7 @@
 #include <video/vga.h>
 #include <asm/io.h>
 
-static spinlock_t vga_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(vga_lock);
 static struct vgastate state;
 
 #define BLANK 0x0020
Index: xx-sources/fs/buffer.c
===================================================================
--- xx-sources.orig/fs/buffer.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/buffer.c	2004-10-16 20:56:59.000000000 -0400
@@ -529,7 +529,7 @@
  */
 static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
 {
-	static spinlock_t page_uptodate_lock = SPIN_LOCK_UNLOCKED;
+	static DECLARE_SPINLOCK(page_uptodate_lock);
 	unsigned long flags;
 	struct buffer_head *tmp;
 	struct page *page;
@@ -587,7 +587,7 @@
 void end_buffer_async_write(struct buffer_head *bh, int uptodate)
 {
 	char b[BDEVNAME_SIZE];
-	static spinlock_t page_uptodate_lock = SPIN_LOCK_UNLOCKED;
+	static DECLARE_SPINLOCK(page_uptodate_lock);
 	unsigned long flags;
 	struct buffer_head *tmp;
 	struct page *page;
Index: xx-sources/fs/dcache.c
===================================================================
--- xx-sources.orig/fs/dcache.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/dcache.c	2004-10-16 20:56:59.000000000 -0400
@@ -37,8 +37,8 @@
 
 int sysctl_vfs_cache_pressure = 100;
 
-spinlock_t dcache_lock __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED;
-seqlock_t rename_lock __cacheline_aligned_in_smp = SEQLOCK_UNLOCKED;
+DECLARE_SPINLOCK(dcache_lock);
+DECLARE_SEQLOCK(rename_lock);
 
 EXPORT_SYMBOL(dcache_lock);
 
@@ -983,7 +983,7 @@
 	struct dentry *found = NULL;
 	struct hlist_node *node;
 
-	rcu_read_lock();
+	rcu_read_lock_spin(&dcache_lock);
 	
 	hlist_for_each_rcu(node, head) {
 		struct dentry *dentry; 
@@ -1030,7 +1030,7 @@
 next:
 		spin_unlock(&dentry->d_lock);
  	}
- 	rcu_read_unlock();
+ 	rcu_read_unlock_spin(&dcache_lock);
 
  	return found;
 }
@@ -1471,7 +1471,7 @@
 	/* need rcu_readlock to protect against the d_parent trashing due to
 	 * d_move
 	 */
-	rcu_read_lock();
+	rcu_read_lock_spin(&dcache_lock);
         do {
 		/* for restarting inner loop in case of seq retry */
 		new_dentry = saved;
@@ -1488,7 +1488,7 @@
 			break;
 		}
 	} while (read_seqretry(&rename_lock, seq));
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&dcache_lock);
 
 	return result;
 }
Index: xx-sources/fs/exec.c
===================================================================
--- xx-sources.orig/fs/exec.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/exec.c	2004-10-16 20:56:59.000000000 -0400
@@ -628,14 +628,14 @@
 	if (current->pid != current->tgid) {
 		struct task_struct *leader = current->group_leader, *parent;
 		struct dentry *proc_dentry1, *proc_dentry2;
-		unsigned long state, ptrace;
+		unsigned long exit_state, ptrace;
 
 		/*
 		 * Wait for the thread group leader to be a zombie.
 		 * It should already be zombie at this point, most
 		 * of the time.
 		 */
-		while (leader->state != TASK_ZOMBIE)
+		while (leader->exit_state != __TASK_ZOMBIE)
 			yield();
 
 		spin_lock(&leader->proc_lock);
@@ -679,7 +679,7 @@
 		list_del(&current->tasks);
 		list_add_tail(&current->tasks, &init_task.tasks);
 		current->exit_signal = SIGCHLD;
-		state = leader->state;
+		exit_state = leader->exit_state;
 
 		write_unlock_irq(&tasklist_lock);
 		spin_unlock(&leader->proc_lock);
@@ -687,7 +687,7 @@
 		proc_pid_flush(proc_dentry1);
 		proc_pid_flush(proc_dentry2);
 
-		if (state != TASK_ZOMBIE)
+		if (exit_state != __TASK_ZOMBIE)
 			BUG();
 		release_task(leader);
         }
Index: xx-sources/fs/fcntl.c
===================================================================
--- xx-sources.orig/fs/fcntl.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/fcntl.c	2004-10-16 20:56:59.000000000 -0400
@@ -475,7 +475,8 @@
 				break;
 		/* fall-through: fall back on the old plain SIGIO signal */
 		case 0:
-			send_group_sig_info(SIGIO, SEND_SIG_PRIV, p);
+			// we hold the tasklist lock already:
+			group_send_sig_info(SIGIO, SEND_SIG_PRIV, p);
 	}
 }
 
@@ -509,7 +510,7 @@
                                 struct fown_struct *fown)
 {
 	if (sigio_perm(p, fown))
-		send_group_sig_info(SIGURG, SEND_SIG_PRIV, p);
+		group_send_sig_info(SIGURG, SEND_SIG_PRIV, p);
 }
 
 int send_sigurg(struct fown_struct *fown)
Index: xx-sources/fs/file_table.c
===================================================================
--- xx-sources.orig/fs/file_table.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/file_table.c	2004-10-16 20:56:59.000000000 -0400
@@ -27,7 +27,7 @@
 /* public. Not pretty! */
 spinlock_t __cacheline_aligned_in_smp files_lock = SPIN_LOCK_UNLOCKED;
 
-static spinlock_t filp_count_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(filp_count_lock);
 
 /* slab constructors and destructors are called from arbitrary
  * context and must be fully threaded - use a local spinlock
Index: xx-sources/fs/inode.c
===================================================================
--- xx-sources.orig/fs/inode.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/inode.c	2004-10-16 20:56:59.000000000 -0400
@@ -80,7 +80,7 @@
  * NOTE! You also have to own the lock if you change
  * the i_state of an inode while it is in use..
  */
-spinlock_t inode_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(inode_lock);
 EXPORT_SYMBOL(inode_lock);
 
 /*
Index: xx-sources/fs/namespace.c
===================================================================
--- xx-sources.orig/fs/namespace.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/namespace.c	2004-10-16 20:56:59.000000000 -0400
@@ -37,7 +37,7 @@
 #endif
 
 /* spinlock for vfsmount related operations, inplace of dcache_lock */
-spinlock_t vfsmount_lock __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(vfsmount_lock);
 
 static struct list_head *mount_hashtable;
 static int hash_mask, hash_bits;
Index: xx-sources/fs/proc/array.c
===================================================================
--- xx-sources.orig/fs/proc/array.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/proc/array.c	2004-10-16 20:56:59.000000000 -0400
@@ -138,13 +138,13 @@
 
 static inline const char * get_task_state(struct task_struct *tsk)
 {
-	unsigned int state = tsk->state & (TASK_RUNNING |
-					   TASK_INTERRUPTIBLE |
-					   TASK_UNINTERRUPTIBLE |
-					   TASK_ZOMBIE |
-					   TASK_DEAD |
-					   TASK_STOPPED |
-					   TASK_TRACED);
+	unsigned int state = (tsk->state & (TASK_RUNNING |
+					    TASK_INTERRUPTIBLE |
+					    TASK_UNINTERRUPTIBLE |
+					    TASK_STOPPED |
+					    TASK_TRACED)) |
+			(tsk->exit_state & (__TASK_ZOMBIE |
+					    __TASK_DEAD));
 	const char **p = &task_state_array[0];
 
 	while (state) {
Index: xx-sources/fs/proc/base.c
===================================================================
--- xx-sources.orig/fs/proc/base.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/proc/base.c	2004-10-16 22:00:56.057737592 -0400
@@ -368,6 +368,7 @@
 	int res = 0;
 	unsigned int len;
 	struct mm_struct *mm = get_task_mm(task);
+
 	if (!mm)
 		goto out;
 	if (!mm->arg_end)
Index: xx-sources/fs/proc/proc_misc.c
===================================================================
--- xx-sources.orig/fs/proc/proc_misc.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/proc/proc_misc.c	2004-10-16 22:00:54.929909048 -0400
@@ -639,6 +639,20 @@
 	return proc_calc_metrics(page, start, off, count, eof, len);
 }
 
+#ifdef CONFIG_LATENCY_TRACE
+extern struct seq_operations latency_trace_op;
+static int latency_trace_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &latency_trace_op);
+}
+static struct file_operations proc_latency_trace_operations = {
+	.open		= latency_trace_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+#endif
+
 #ifdef CONFIG_MAGIC_SYSRQ
 /*
  * writing 'C' to /proc/sysrq-trigger is like sysrq-C
@@ -722,6 +736,9 @@
 #ifdef CONFIG_SCHEDSTATS
 	create_seq_entry("schedstat", 0, &proc_schedstat_operations);
 #endif
+#ifdef CONFIG_LATENCY_TRACE
+	create_seq_entry("latency_trace", 0, &proc_latency_trace_operations);
+#endif
 #ifdef CONFIG_PROC_KCORE
 	proc_root_kcore = create_proc_entry("kcore", S_IRUSR, NULL);
 	if (proc_root_kcore) {
Index: xx-sources/fs/proc/task_mmu.c
===================================================================
--- xx-sources.orig/fs/proc/task_mmu.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/fs/proc/task_mmu.c	2004-10-16 20:56:59.000000000 -0400
@@ -93,8 +93,10 @@
 
 	down_read(&mm->mmap_sem);
 	map = mm->mmap;
-	while (l-- && map)
+	while (l-- && map) {
 		map = map->vm_next;
+		cond_resched();
+	}
 	if (!map) {
 		up_read(&mm->mmap_sem);
 		mmput(mm);
Index: xx-sources/include/asm-generic/percpu.h
===================================================================
--- xx-sources.orig/include/asm-generic/percpu.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-generic/percpu.h	2004-10-16 20:56:59.000000000 -0400
@@ -10,11 +10,23 @@
 /* Separate out the type, so (int[3], foo) works. */
 #define DEFINE_PER_CPU(type, name) \
     __attribute__((__section__(".data.percpu"))) __typeof__(type) per_cpu__##name
+#define DEFINE_PER_CPU_LOCKED(type, name) \
+    __attribute__((__section__(".data.percpu"))) spinlock_t per_cpu_lock__##name##_locked = SPIN_LOCK_UNLOCKED; \
+    __attribute__((__section__(".data.percpu"))) __typeof__(type) per_cpu__##name##_locked
 
 /* var is in discarded region: offset to particular copy we want */
 #define per_cpu(var, cpu) (*RELOC_HIDE(&per_cpu__##var, __per_cpu_offset[cpu]))
 #define __get_cpu_var(var) per_cpu(var, smp_processor_id())
 
+#define per_cpu_lock(var, cpu) \
+	(*RELOC_HIDE(&per_cpu_lock__##var##_locked, __per_cpu_offset[cpu]))
+#define per_cpu_var_locked(var, cpu) \
+		(*RELOC_HIDE(&per_cpu__##var##_locked, __per_cpu_offset[cpu]))
+#define __get_cpu_lock(var, cpu) \
+		per_cpu_lock(var, cpu)
+#define __get_cpu_var_locked(var, cpu) \
+		per_cpu_var_locked(var, cpu)
+
 /* A macro to avoid #include hell... */
 #define percpu_modcopy(pcpudst, src, size)			\
 do {								\
@@ -29,8 +41,14 @@
 #define DEFINE_PER_CPU(type, name) \
     __typeof__(type) per_cpu__##name
 
+#define DEFINE_PER_CPU_LOCKED(type, name) \
+    spinlock_t per_cpu_lock__##name##_locked = SPIN_LOCK_UNLOCKED; \
+    __typeof__(type) per_cpu__##name##_locked
+
 #define per_cpu(var, cpu)			(*((void)cpu, &per_cpu__##var))
 #define __get_cpu_var(var)			per_cpu__##var
+#define __get_cpu_lock(var, cpu)		per_cpu_lock__##var##_locked
+#define __get_cpu_var_locked(var, cpu)		per_cpu__##var##_locked
 
 #endif	/* SMP */
 
Index: xx-sources/include/asm-generic/semaphore.h
===================================================================
--- xx-sources.orig/include/asm-generic/semaphore.h	2004-10-14 14:49:41.073057592 -0400
+++ xx-sources/include/asm-generic/semaphore.h	2004-10-16 20:56:59.000000000 -0400
@@ -0,0 +1,22 @@
+#ifndef _ASM_GENERIC_SEMAPHORE_H
+#define _ASM_GENERIC_SEMAPHORE_H
+
+#include <linux/config.h>
+#include <linux/linkage.h>
+#include <linux/rwsem.h>
+
+/*
+ * SMP- and interrupt-safe semaphores..
+ *
+ * (C) Copyright 1996 Linus Torvalds
+ */
+
+struct semaphore {
+	struct rw_semaphore sem;
+};
+
+#define DECLARE_MUTEX(name) DECLARE_RWSEM(name)
+
+//#define DECLARE_MUTEX_LOCKED(name) DECLARE_RWSEM_WRITE_LOCKED(name)
+
+#endif
Index: xx-sources/include/asm-generic/tlb.h
===================================================================
--- xx-sources.orig/include/asm-generic/tlb.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-generic/tlb.h	2004-10-16 20:56:59.000000000 -0400
@@ -44,6 +44,11 @@
 	struct page *		pages[FREE_PTE_NR];
 };
 
+/*
+ * Some architectures might want to store mm in the tlb pointer itself.
+ */
+#define tlb_mm(tlb) ((tlb)->mm)
+
 /* Users of the generic TLB shootdown code must declare this storage space. */
 DECLARE_PER_CPU(struct mmu_gather, mmu_gathers);
 
@@ -53,7 +58,7 @@
 static inline struct mmu_gather *
 tlb_gather_mmu(struct mm_struct *mm, unsigned int full_mm_flush)
 {
-	struct mmu_gather *tlb = &per_cpu(mmu_gathers, smp_processor_id());
+	struct mmu_gather *tlb = &get_cpu_var(mmu_gathers);
 
 	tlb->mm = mm;
 
@@ -94,6 +99,7 @@
 		freed = rss;
 	mm->rss = rss - freed;
 	tlb_flush_mmu(tlb, start, end);
+	put_cpu_var(mmu_gathers);
 
 	/* keep the page table cache within bounds */
 	check_pgt_cache();
@@ -105,6 +111,15 @@
 	return tlb->fullmm;
 }
 
+/* tlb_free
+ *	this counts the number of pages we have to take off the RSS
+ *	at flush time.
+ */
+static inline void tlb_free(struct mmu_gather *tlb)
+{
+	tlb->freed++;
+}
+
 /* tlb_remove_page
  *	Must perform the equivalent to __free_pte(pte_get_and_clear(ptep)), while
  *	handling the additional races in SMP caused by other CPUs caching valid
Index: xx-sources/include/asm-i386/dma.h
===================================================================
--- xx-sources.orig/include/asm-i386/dma.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/dma.h	2004-10-16 20:56:59.000000000 -0400
@@ -135,7 +135,7 @@
 #define DMA_AUTOINIT	0x10
 
 
-extern spinlock_t  dma_spin_lock;
+extern spinlock_t dma_spin_lock;
 
 static __inline__ unsigned long claim_dma_lock(void)
 {
Index: xx-sources/include/asm-i386/highmem.h
===================================================================
--- xx-sources.orig/include/asm-i386/highmem.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/highmem.h	2004-10-16 20:56:59.000000000 -0400
@@ -58,6 +58,8 @@
 extern void FASTCALL(kunmap_high(struct page *page));
 
 void *kmap(struct page *page);
+extern void kunmap_virt(void *ptr);
+extern struct page *kmap_to_page(void *ptr);
 void kunmap(struct page *page);
 void *kmap_atomic(struct page *page, enum km_type type);
 void kunmap_atomic(void *kvaddr, enum km_type type);
@@ -66,6 +68,22 @@
 
 #define flush_cache_kmaps()	do { } while (0)
 
+/*
+ * kmap_atomic_rt() is an atomic-kmap wrapper that uses kmap(),
+ * this is useful for code that is safe to preempt if PREEMPT_REALTIME:
+ */
+#ifdef CONFIG_PREEMPT_REALTIME
+# define kmap_atomic_rt(page, type) kmap(page)
+# define kmap_atomic_pfn_rt(pfn, type) kmap(pfn_to_page(pfn))
+# define kunmap_atomic_rt(kvaddr, type) kunmap_virt(kvaddr)
+# define kmap_atomic_to_page_rt(kvaddr) kmap_to_page(kvaddr)
+#else
+# define kmap_atomic_rt kmap_atomic
+# define kmap_atomic_pfn_rt kmap_atomic_pfn
+# define kunmap_atomic_rt kunmap_atomic
+# define kmap_atomic_to_page_rt(kvaddr) kmap_atomic_to_page(kvaddr)
+#endif
+
 #endif /* __KERNEL__ */
 
 #endif /* _ASM_HIGHMEM_H */
Index: xx-sources/include/asm-i386/i8253.h
===================================================================
--- xx-sources.orig/include/asm-i386/i8253.h	2004-10-14 14:49:41.073057592 -0400
+++ xx-sources/include/asm-i386/i8253.h	2004-10-16 20:56:59.000000000 -0400
@@ -0,0 +1,6 @@
+#ifndef __ASM_I8253_H__
+#define __ASM_I8253_H__
+
+extern raw_spinlock_t i8253_lock;
+
+#endif	/* __ASM_I8253_H__ */
Index: xx-sources/include/asm-i386/i8259.h
===================================================================
--- xx-sources.orig/include/asm-i386/i8259.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/i8259.h	2004-10-16 20:56:59.000000000 -0400
@@ -7,7 +7,7 @@
 #define cached_master_mask	(__byte(0, cached_irq_mask))
 #define cached_slave_mask	(__byte(1, cached_irq_mask))
 
-extern spinlock_t i8259A_lock;
+extern raw_spinlock_t i8259A_lock;
 
 extern void init_8259A(int auto_eoi);
 extern void enable_8259A_irq(unsigned int irq);
Index: xx-sources/include/asm-i386/io_apic.h
===================================================================
--- xx-sources.orig/include/asm-i386/io_apic.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/io_apic.h	2004-10-16 20:56:59.000000000 -0400
@@ -16,11 +16,10 @@
 #ifdef CONFIG_PCI_MSI
 static inline int use_pci_vector(void)	{return 1;}
 static inline void disable_edge_ioapic_vector(unsigned int vector) { }
-static inline void mask_and_ack_level_ioapic_vector(unsigned int vector) { }
 static inline void end_edge_ioapic_vector (unsigned int vector) { }
 #define startup_level_ioapic	startup_level_ioapic_vector
 #define shutdown_level_ioapic	mask_IO_APIC_vector
-#define enable_level_ioapic	unmask_IO_APIC_vector
+#define enable_level_ioapic	enable_level_ioapic_vector
 #define disable_level_ioapic	mask_IO_APIC_vector
 #define mask_and_ack_level_ioapic mask_and_ack_level_ioapic_vector
 #define end_level_ioapic	end_level_ioapic_vector
@@ -35,11 +34,10 @@
 #else
 static inline int use_pci_vector(void)	{return 0;}
 static inline void disable_edge_ioapic_irq(unsigned int irq) { }
-static inline void mask_and_ack_level_ioapic_irq(unsigned int irq) { }
 static inline void end_edge_ioapic_irq (unsigned int irq) { }
 #define startup_level_ioapic	startup_level_ioapic_irq
 #define shutdown_level_ioapic	mask_IO_APIC_irq
-#define enable_level_ioapic	unmask_IO_APIC_irq
+#define enable_level_ioapic	enable_level_ioapic_irq
 #define disable_level_ioapic	mask_IO_APIC_irq
 #define mask_and_ack_level_ioapic mask_and_ack_level_ioapic_irq
 #define end_level_ioapic	end_level_ioapic_irq
Index: xx-sources/include/asm-i386/mach-default/do_timer.h
===================================================================
--- xx-sources.orig/include/asm-i386/mach-default/do_timer.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/mach-default/do_timer.h	2004-10-16 20:56:59.000000000 -0400
@@ -1,6 +1,7 @@
 /* defines for inline arch setup functions */
 
 #include <asm/apic.h>
+#include <asm/i8259.h>
 
 /**
  * do_timer_interrupt_hook - hook into timer tick
Index: xx-sources/include/asm-i386/pgtable.h
===================================================================
--- xx-sources.orig/include/asm-i386/pgtable.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/pgtable.h	2004-10-16 20:56:59.000000000 -0400
@@ -34,7 +34,7 @@
 extern pgd_t swapper_pg_dir[1024];
 extern kmem_cache_t *pgd_cache;
 extern kmem_cache_t *pmd_cache;
-extern spinlock_t pgd_lock;
+extern raw_spinlock_t pgd_lock;
 extern struct page *pgd_list;
 
 void pmd_ctor(void *, kmem_cache_t *, unsigned long);
@@ -365,11 +365,11 @@
 
 #if defined(CONFIG_HIGHPTE)
 #define pte_offset_map(dir, address) \
-	((pte_t *)kmap_atomic(pmd_page(*(dir)),KM_PTE0) + pte_index(address))
+	((pte_t *)kmap_atomic_rt(pmd_page(*(dir)),KM_PTE0) + pte_index(address))
 #define pte_offset_map_nested(dir, address) \
-	((pte_t *)kmap_atomic(pmd_page(*(dir)),KM_PTE1) + pte_index(address))
-#define pte_unmap(pte) kunmap_atomic(pte, KM_PTE0)
-#define pte_unmap_nested(pte) kunmap_atomic(pte, KM_PTE1)
+	((pte_t *)kmap_atomic_rt(pmd_page(*(dir)),KM_PTE1) + pte_index(address))
+#define pte_unmap(pte) kunmap_atomic_rt(pte, KM_PTE0)
+#define pte_unmap_nested(pte) kunmap_atomic_rt(pte, KM_PTE1)
 #else
 #define pte_offset_map(dir, address) \
 	((pte_t *)page_address(pmd_page(*(dir))) + pte_index(address))
Index: xx-sources/include/asm-i386/rwsem.h
===================================================================
--- xx-sources.orig/include/asm-i386/rwsem.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/rwsem.h	2004-10-16 20:56:59.000000000 -0400
@@ -76,7 +76,7 @@
 #endif
 
 #define __RWSEM_INITIALIZER(name) \
-{ RWSEM_UNLOCKED_VALUE, SPIN_LOCK_UNLOCKED, LIST_HEAD_INIT((name).wait_list) \
+{ RWSEM_UNLOCKED_VALUE, RAW_SPIN_LOCK_UNLOCKED, LIST_HEAD_INIT((name).wait_list) \
 	__RWSEM_DEBUG_INIT }
 
 #define DECLARE_RWSEM(name) \
Index: xx-sources/include/asm-i386/semaphore.h
===================================================================
--- xx-sources.orig/include/asm-i386/semaphore.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/semaphore.h	2004-10-16 20:56:59.000000000 -0400
@@ -1,10 +1,13 @@
 #ifndef _I386_SEMAPHORE_H
 #define _I386_SEMAPHORE_H
 
+#include <linux/config.h>
 #include <linux/linkage.h>
+#include <linux/realtime_lock.h>
 
-#ifdef __KERNEL__
-
+#if defined(CONFIG_PREEMPT_REALTIME) && 0
+# include <asm-generic/semaphore.h>
+#else
 /*
  * SMP- and interrupt-safe semaphores..
  *
@@ -41,11 +44,13 @@
 #include <linux/wait.h>
 #include <linux/rwsem.h>
 
+#if 0
 struct semaphore {
 	atomic_t count;
 	int sleepers;
 	wait_queue_head_t wait;
 };
+#endif
 
 
 #define __SEMAPHORE_INITIALIZER(name, n)				\
@@ -190,6 +195,5 @@
 		:"c" (sem)
 		:"memory");
 }
-
 #endif
 #endif
Index: xx-sources/include/asm-i386/spinlock.h
===================================================================
--- xx-sources.orig/include/asm-i386/spinlock.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/spinlock.h	2004-10-16 20:56:59.000000000 -0400
@@ -5,25 +5,9 @@
 #include <asm/rwlock.h>
 #include <asm/page.h>
 #include <linux/config.h>
+#include <linux/list.h>
 #include <linux/compiler.h>
 
-asmlinkage int printk(const char * fmt, ...)
-	__attribute__ ((format (printf, 1, 2)));
-
-/*
- * Your basic SMP spinlocks, allowing only a single CPU anywhere
- */
-
-typedef struct {
-	volatile unsigned int lock;
-#ifdef CONFIG_DEBUG_SPINLOCK
-	unsigned magic;
-#endif
-#ifdef CONFIG_PREEMPT
-	unsigned int break_lock;
-#endif
-} spinlock_t;
-
 #define SPINLOCK_MAGIC	0xdead4ead
 
 #ifdef CONFIG_DEBUG_SPINLOCK
@@ -32,9 +16,10 @@
 #define SPINLOCK_MAGIC_INIT	/* */
 #endif
 
-#define SPIN_LOCK_UNLOCKED (spinlock_t) { 1 SPINLOCK_MAGIC_INIT }
+#define __RAW_SPIN_LOCK_UNLOCKED { 1 SPINLOCK_MAGIC_INIT }
+#define RAW_SPIN_LOCK_UNLOCKED (raw_spinlock_t) __RAW_SPIN_LOCK_UNLOCKED
 
-#define spin_lock_init(x)	do { *(x) = SPIN_LOCK_UNLOCKED; } while(0)
+#define _raw_spin_lock_init(x)	do { *(x) = RAW_SPIN_LOCK_UNLOCKED; } while(0)
 
 /*
  * Simple spin lock operations.  There are two variants, one clears IRQ's
@@ -43,8 +28,9 @@
  * We make no fairness assumptions. They have a cost.
  */
 
-#define spin_is_locked(x)	(*(volatile signed char *)(&(x)->lock) <= 0)
-#define spin_unlock_wait(x)	do { barrier(); } while(spin_is_locked(x))
+#define _raw_spin_is_locked(x)	(*(volatile signed char *)(&(x)->lock) <= 0)
+#define _raw_spin_unlock_wait(x) \
+		do { barrier(); } while (_raw_spin_is_locked(x))
 
 #define spin_lock_string \
 	"\n1:\t" \
@@ -86,11 +72,11 @@
 		:"=m" (lock->lock) : : "memory"
 
 
-static inline void _raw_spin_unlock(spinlock_t *lock)
+static inline void _raw_spin_unlock(raw_spinlock_t *lock)
 {
 #ifdef CONFIG_DEBUG_SPINLOCK
 	BUG_ON(lock->magic != SPINLOCK_MAGIC);
-	BUG_ON(!spin_is_locked(lock));
+	BUG_ON(!_raw_spin_is_locked(lock));
 #endif
 	__asm__ __volatile__(
 		spin_unlock_string
@@ -104,12 +90,12 @@
 		:"=q" (oldval), "=m" (lock->lock) \
 		:"0" (oldval) : "memory"
 
-static inline void _raw_spin_unlock(spinlock_t *lock)
+static inline void _raw_spin_unlock(raw_spinlock_t *lock)
 {
 	char oldval = 1;
 #ifdef CONFIG_DEBUG_SPINLOCK
 	BUG_ON(lock->magic != SPINLOCK_MAGIC);
-	BUG_ON(!spin_is_locked(lock));
+	BUG_ON(!_raw_spin_is_locked(lock));
 #endif
 	__asm__ __volatile__(
 		spin_unlock_string
@@ -118,7 +104,7 @@
 
 #endif
 
-static inline int _raw_spin_trylock(spinlock_t *lock)
+static inline int _raw_spin_trylock(raw_spinlock_t *lock)
 {
 	char oldval;
 	__asm__ __volatile__(
@@ -128,7 +114,7 @@
 	return oldval > 0;
 }
 
-static inline void _raw_spin_lock(spinlock_t *lock)
+static inline void _raw_spin_lock(raw_spinlock_t *lock)
 {
 #ifdef CONFIG_DEBUG_SPINLOCK
 	if (unlikely(lock->magic != SPINLOCK_MAGIC)) {
@@ -141,7 +127,7 @@
 		:"=m" (lock->lock) : : "memory");
 }
 
-static inline void _raw_spin_lock_flags (spinlock_t *lock, unsigned long flags)
+static inline void _raw_spin_lock_flags (raw_spinlock_t *lock, unsigned long flags)
 {
 #ifdef CONFIG_DEBUG_SPINLOCK
 	if (unlikely(lock->magic != SPINLOCK_MAGIC)) {
@@ -154,26 +140,6 @@
 		:"=m" (lock->lock) : "r" (flags) : "memory");
 }
 
-/*
- * Read-write spinlocks, allowing multiple readers
- * but only one writer.
- *
- * NOTE! it is quite common to have readers in interrupts
- * but no interrupt writers. For those circumstances we
- * can "mix" irq-safe locks - any writer needs to get a
- * irq-safe write-lock, but readers can get non-irqsafe
- * read-locks.
- */
-typedef struct {
-	volatile unsigned int lock;
-#ifdef CONFIG_DEBUG_SPINLOCK
-	unsigned magic;
-#endif
-#ifdef CONFIG_PREEMPT
-	unsigned int break_lock;
-#endif
-} rwlock_t;
-
 #define RWLOCK_MAGIC	0xdeaf1eed
 
 #ifdef CONFIG_DEBUG_SPINLOCK
@@ -182,11 +148,12 @@
 #define RWLOCK_MAGIC_INIT	/* */
 #endif
 
-#define RW_LOCK_UNLOCKED (rwlock_t) { RW_LOCK_BIAS RWLOCK_MAGIC_INIT }
+#define __RAW_RW_LOCK_UNLOCKED { RW_LOCK_BIAS RWLOCK_MAGIC_INIT }
+#define RAW_RW_LOCK_UNLOCKED (raw_rwlock_t) { RW_LOCK_BIAS RWLOCK_MAGIC_INIT }
 
-#define rwlock_init(x)	do { *(x) = RW_LOCK_UNLOCKED; } while(0)
+#define _raw_rwlock_init(x) do { *(x) = RAW_RW_LOCK_UNLOCKED; } while(0)
 
-#define rwlock_is_locked(x) ((x)->lock != RW_LOCK_BIAS)
+#define _raw_rwlock_is_locked(x) 	((x)->lock != RW_LOCK_BIAS)
 
 /*
  * On x86, we implement read-write locks as a 32-bit counter
@@ -199,7 +166,7 @@
  */
 /* the spinlock helpers are in arch/i386/kernel/semaphore.c */
 
-static inline void _raw_read_lock(rwlock_t *rw)
+static inline void _raw_read_lock(raw_rwlock_t *rw)
 {
 #ifdef CONFIG_DEBUG_SPINLOCK
 	BUG_ON(rw->magic != RWLOCK_MAGIC);
@@ -207,7 +174,7 @@
 	__build_read_lock(rw, "__read_lock_failed");
 }
 
-static inline void _raw_write_lock(rwlock_t *rw)
+static inline void _raw_write_lock(raw_rwlock_t *rw)
 {
 #ifdef CONFIG_DEBUG_SPINLOCK
 	BUG_ON(rw->magic != RWLOCK_MAGIC);
@@ -218,7 +185,7 @@
 #define _raw_read_unlock(rw)		asm volatile("lock ; incl %0" :"=m" ((rw)->lock) : : "memory")
 #define _raw_write_unlock(rw)	asm volatile("lock ; addl $" RW_LOCK_BIAS_STR ",%0":"=m" ((rw)->lock) : : "memory")
 
-static inline int _raw_read_trylock(rwlock_t *lock)
+static inline int _raw_read_trylock(raw_rwlock_t *lock)
 {
 	atomic_t *count = (atomic_t *)lock;
 	atomic_dec(count);
@@ -228,7 +195,7 @@
 	return 0;
 }
 
-static inline int _raw_write_trylock(rwlock_t *lock)
+static inline int _raw_write_trylock(raw_rwlock_t *lock)
 {
 	atomic_t *count = (atomic_t *)lock;
 	if (atomic_sub_and_test(RW_LOCK_BIAS, count))
Index: xx-sources/include/asm-i386/tlb.h
===================================================================
--- xx-sources.orig/include/asm-i386/tlb.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/asm-i386/tlb.h	2004-10-16 20:56:59.000000000 -0400
@@ -13,8 +13,60 @@
  * .. because we flush the whole mm when it
  * fills up.
  */
-#define tlb_flush(tlb) flush_tlb_mm((tlb)->mm)
+#define tlb_flush(tlb) flush_tlb_mm(tlb_mm(tlb))
 
-#include <asm-generic/tlb.h>
+/*
+ * The mutex based kernel can preempt anytime so the per-CPU
+ * gather structures dont really fit. Fortunately TLB flushing
+ * is really simple on x86 ...
+ */
+#ifndef CONFIG_PREEMPT_REALTIME
+# include <asm-generic/tlb.h>
+#else
+
+# include <linux/config.h>
+# include <linux/swap.h>
+# include <asm/pgalloc.h>
+# include <asm/tlbflush.h>
+
+struct mmu_gather {
+	void *self;
+};
+
+/*
+ * We store the mm in the tlb pointer itself, so we dont
+ * have to allocate anything on tlb-gather:
+ */
+#define tlb_mm(tlb) ((struct mm_struct *)(tlb))
+
+static inline struct mmu_gather *
+tlb_gather_mmu(struct mm_struct *mm, unsigned int full_mm_flush)
+{
+	return (struct mmu_gather *)mm;
+}
+static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
+{
+	free_page_and_swap_cache(page);
+}
+static inline void tlb_free(struct mmu_gather *tlb)
+{
+	if (tlb_mm(tlb)->rss)
+		tlb_mm(tlb)->rss--;
+}
+
+# define tlb_remove_tlb_entry __tlb_remove_tlb_entry
+# define pmd_free_tlb __pmd_free_tlb
+# define pte_free_tlb __pte_free_tlb
+# define tlb_migrate_finish(mm) do { } while (0)
+# define tlb_is_full_mm(tlb) 1
+
+static inline void
+tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
+{
+	flush_tlb_mm(tlb_mm(tlb));
+	check_pgt_cache();
+}
+
+#endif
 
 #endif
Index: xx-sources/include/linux/genhd.h
===================================================================
--- xx-sources.orig/include/linux/genhd.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/genhd.h	2004-10-16 20:57:00.000000000 -0400
@@ -137,18 +137,26 @@
  * variants disable/enable preemption.
  */
 #ifdef	CONFIG_SMP
-#define __disk_stat_add(gendiskp, field, addnd) 	\
-	(per_cpu_ptr(gendiskp->dkstats, smp_processor_id())->field += addnd)
+#define __disk_stat_add(gendiskp, field, addnd)			\
+do {								\
+	preempt_disable();					\
+	(per_cpu_ptr(gendiskp->dkstats,				\
+			smp_processor_id())->field += addnd);	\
+	preempt_enable();					\
+} while (0)
+
 
 #define disk_stat_read(gendiskp, field)					\
 ({									\
 	typeof(gendiskp->dkstats->field) res = 0;			\
 	int i;								\
+	preempt_disable();						\
 	for (i=0; i < NR_CPUS; i++) {					\
 		if (!cpu_possible(i))					\
 			continue;					\
 		res += per_cpu_ptr(gendiskp->dkstats, i)->field;	\
 	}								\
+	preempt_enable();						\
 	res;								\
 })
 
Index: xx-sources/include/linux/hardirq.h
===================================================================
--- xx-sources.orig/include/linux/hardirq.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/hardirq.h	2004-10-16 20:57:00.000000000 -0400
@@ -57,9 +57,9 @@
  * Are we doing bottom half or hardware interrupt processing?
  * Are we in a softirq context? Interrupt context?
  */
-#define in_irq()		(hardirq_count())
-#define in_softirq()		(softirq_count())
-#define in_interrupt()		(irq_count())
+#define in_irq()	(hardirq_count() || (current->flags & PF_HARDIRQ))
+#define in_softirq()	(softirq_count() || (current->flags & PF_SOFTIRQ))
+#define in_interrupt()	(irq_count())
 
 #if defined(CONFIG_PREEMPT) && !defined(CONFIG_PREEMPT_BKL)
 # define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != kernel_locked())
Index: xx-sources/include/linux/highmem.h
===================================================================
--- xx-sources.orig/include/linux/highmem.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/highmem.h	2004-10-16 20:57:00.000000000 -0400
@@ -33,6 +33,11 @@
 #define kmap_atomic_pfn(pfn, idx)	page_address(pfn_to_page(pfn))
 #define kmap_atomic_to_page(ptr)	virt_to_page(ptr)
 
+#define kmap_atomic_rt kmap_atomic
+#define kmap_atomic_pfn_rt kmap_atomic_pfn
+#define kunmap_atomic_rt kunmap_atomic
+#define kmap_atomic_to_page_rt(kvaddr) kmap_atomic_to_page(kvaddr)
+
 #endif /* CONFIG_HIGHMEM */
 
 /* when CONFIG_HIGHMEM is not set these will be plain clear/copy_page */
Index: xx-sources/include/linux/init_task.h
===================================================================
--- xx-sources.orig/include/linux/init_task.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/init_task.h	2004-10-16 20:57:00.000000000 -0400
@@ -38,7 +38,7 @@
 	.mm_users	= ATOMIC_INIT(2), 			\
 	.mm_count	= ATOMIC_INIT(1), 			\
 	.mmap_sem	= __RWSEM_INITIALIZER(name.mmap_sem),	\
-	.page_table_lock =  SPIN_LOCK_UNLOCKED, 		\
+	.page_table_lock = SPIN_LOCK_UNLOCKED, 			\
 	.mmlist		= LIST_HEAD_INIT(name.mmlist),		\
 	.cpu_vm_mask	= CPU_MASK_ALL,				\
 	.default_kioctx = INIT_KIOCTX(name.default_kioctx, name),	\
@@ -124,9 +124,9 @@
 		.list = LIST_HEAD_INIT(tsk.pending.list),		\
 		.signal = {{0}}},					\
 	.blocked	= {{0}},					\
-	.alloc_lock	= SPIN_LOCK_UNLOCKED,				\
+	.alloc_lock	= RAW_SPIN_LOCK_UNLOCKED,			\
 	.proc_lock	= SPIN_LOCK_UNLOCKED,				\
-	.switch_lock	= SPIN_LOCK_UNLOCKED,				\
+	.switch_lock	= RAW_SPIN_LOCK_UNLOCKED,			\
 	.journal_info	= NULL,						\
 	.private_pages	= LIST_HEAD_INIT(tsk.private_pages),		\
 	.private_pages_count = 0,					\
Index: xx-sources/include/linux/interrupt.h
===================================================================
--- xx-sources.orig/include/linux/interrupt.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/interrupt.h	2004-10-16 20:57:00.000000000 -0400
@@ -41,7 +41,7 @@
 	void *dev_id;
 	struct irqaction *next;
 	int irq;
-	struct proc_dir_entry *dir;
+	struct proc_dir_entry *dir, *threaded;
 };
 
 extern irqreturn_t no_action(int cpl, void *dev_id, struct pt_regs *regs);
@@ -68,13 +68,18 @@
 # define save_and_cli(x)	local_irq_save(x)
 #endif
 
+#ifdef CONFIG_PREEMPT_REALTIME
+# define local_bh_disable() do { } while (0)
+# define local_bh_enable() do { } while (0)
+# define __local_bh_enable() do { } while (0)
+#else
 /* SoftIRQ primitives.  */
-#define local_bh_disable() \
+# define local_bh_disable() \
 		do { add_preempt_count(SOFTIRQ_OFFSET); barrier(); } while (0)
-#define __local_bh_enable() \
+# define __local_bh_enable() \
 		do { barrier(); sub_preempt_count(SOFTIRQ_OFFSET); } while (0)
-
-extern void local_bh_enable(void);
+  extern void local_bh_enable(void);
+#endif
 
 /* PLEASE, avoid to allocate new softirqs, if you need not _really_ high
    frequency threaded job scheduling. For almost all the purposes
@@ -108,6 +113,7 @@
 #define __raise_softirq_irqoff(nr) do { local_softirq_pending() |= 1UL << (nr); } while (0)
 extern void FASTCALL(raise_softirq_irqoff(unsigned int nr));
 extern void FASTCALL(raise_softirq(unsigned int nr));
+extern void wakeup_irqd(void);
 
 #ifndef invoke_softirq
 #define invoke_softirq() do_softirq()
Index: xx-sources/include/linux/ipc.h
===================================================================
--- xx-sources.orig/include/linux/ipc.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/ipc.h	2004-10-16 20:57:00.000000000 -0400
@@ -56,7 +56,7 @@
 /* used by in-kernel data structures */
 struct kern_ipc_perm
 {
-	spinlock_t	lock;
+	raw_spinlock_t	lock;
 	int		deleted;
 	key_t		key;
 	uid_t		uid;
Index: xx-sources/include/linux/irq.h
===================================================================
--- xx-sources.orig/include/linux/irq.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/irq.h	2004-10-16 20:57:00.000000000 -0400
@@ -17,6 +17,7 @@
 #include <linux/cache.h>
 #include <linux/spinlock.h>
 #include <linux/cpumask.h>
+#include <linux/wait.h>
 
 #include <asm/irq.h>
 #include <asm/ptrace.h>
@@ -33,6 +34,15 @@
 #define IRQ_LEVEL	64	/* IRQ level triggered */
 #define IRQ_MASKED	128	/* IRQ masked - shouldn't be seen again */
 #define IRQ_PER_CPU	256	/* IRQ is per CPU */
+#define IRQ_NODELAY	512	/* IRQ must run immediately */
+
+/*
+ * Not used on any of the architectures, but feel free to provide
+ * your own per-arch one:
+ */
+#ifndef SA_NODELAY
+# define SA_NODELAY 0x01000000
+#endif
 
 /*
  * Interrupt controller descriptor. This is all we need
@@ -65,7 +75,9 @@
 	unsigned int depth;		/* nested irq disables */
 	unsigned int irq_count;		/* For detecting broken interrupts */
 	unsigned int irqs_unhandled;
-	spinlock_t lock;
+	struct task_struct *thread;
+	wait_queue_head_t wait_for_handler;
+	raw_spinlock_t lock;
 } ____cacheline_aligned irq_desc_t;
 
 extern irq_desc_t irq_desc [NR_IRQS];
@@ -86,7 +98,12 @@
 extern void report_bad_irq(unsigned int irq, irq_desc_t *desc, int action_ret);
 extern int can_request_irq(unsigned int irq, unsigned long irqflags);
 
+extern void early_init_hardirqs(void);
+extern void init_hardirqs(void);
 extern void init_irq_proc(void);
+#else
+static inline void early_init_hardirqs(void) { }
+static inline void init_hardirqs(void) { }
 #endif
 
 extern hw_irq_controller no_irq_type;  /* needed in every arch ? */
Index: xx-sources/include/linux/kernel.h
===================================================================
--- xx-sources.orig/include/linux/kernel.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/linux/kernel.h	2004-10-16 20:57:00.000000000 -0400
@@ -46,15 +46,23 @@
 
 struct completion;
 
-#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
-void __might_sleep(char *file, int line);
-#define might_sleep() __might_sleep(__FILE__, __LINE__)
-#define might_sleep_if(cond) do { if (unlikely(cond)) might_sleep(); } while (0)
+#ifdef CONFIG_PREEMPT_VOLUNTARY
+extern int cond_resched(void);
+# define might_resched() cond_resched()
+#else
+# define might_resched() do { } while (0)
+#endif
+
+#if defined(CONFIG_DEBUG_SPINLOCK_SLEEP) || defined(CONFIG_DEBUG_PREEMPT)
+   void __might_sleep(char *file, int line);
+# define might_sleep() \
+	do { __might_sleep(__FILE__, __LINE__); might_resched(); } while (0)
 #else
-#define might_sleep() do {} while(0)
-#define might_sleep_if(cond) do {} while (0)
+# define might_sleep() do { might_resched(); } while (0)
 #endif
 
+#define might_sleep_if(cond) do { if (unlikely(cond)) might_sleep(); } while (0)
+
 #define abs(x) ({				\
 		int __x = (x);			\
 		(__x < 0) ? -__x : __x;		\
@@ -141,6 +149,7 @@
 /* Values used for system_state */
 extern enum system_states {
 	SYSTEM_BOOTING,
+	SYSTEM_BOOTING_SCHEDULER_OK,
 	SYSTEM_RUNNING,
 	SYSTEM_HALT,
 	SYSTEM_POWER_OFF,
Index: xx-sources/include/linux/linkage.h
===================================================================
--- xx-sources.orig/include/linux/linkage.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/linux/linkage.h	2004-10-16 20:57:00.000000000 -0400
@@ -4,6 +4,8 @@
 #include <linux/config.h>
 #include <asm/linkage.h>
 
+#define notrace __attribute ((no_instrument_function))
+
 #ifdef __cplusplus
 #define CPP_ASMLINKAGE extern "C"
 #else
@@ -31,7 +33,7 @@
 
 #endif
 
-#define NORET_TYPE    /**/
+#define NORET_TYPE    /* */
 #define ATTRIB_NORET  __attribute__((noreturn))
 #define NORET_AND     noreturn,
 
Index: xx-sources/include/linux/mc146818rtc.h
===================================================================
--- xx-sources.orig/include/linux/mc146818rtc.h	2004-10-16 20:52:32.000000000 -0400
+++ xx-sources/include/linux/mc146818rtc.h	2004-10-16 20:57:00.000000000 -0400
@@ -16,7 +16,7 @@
 #include <linux/spinlock.h>		/* spinlock_t */
 #include <asm/mc146818rtc.h>		/* register access macros */
 
-extern spinlock_t rtc_lock;		/* serialize CMOS RAM access */
+extern raw_spinlock_t rtc_lock;		/* serialize CMOS RAM access */
 
 /**********************************************************************
  * register summary
Index: xx-sources/include/linux/mmzone.h
===================================================================
--- xx-sources.orig/include/linux/mmzone.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/mmzone.h	2004-10-16 20:57:00.000000000 -0400
@@ -110,7 +110,7 @@
 	/*
 	 * Commonly accessed fields:
 	 */
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	unsigned long		free_pages;
 	unsigned long		pages_min, pages_low, pages_high;
 	/*
Index: xx-sources/include/linux/pagemap.h
===================================================================
--- xx-sources.orig/include/linux/pagemap.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/pagemap.h	2004-10-16 20:57:00.000000000 -0400
@@ -111,20 +111,19 @@
  * an offset in their per-cpu arena and will spill that into the
  * global count whenever the absolute value of the local count
  * exceeds the counter's threshold.
- *
- * MUST be protected from preemption.
- * current protection is mapping->page_lock.
  */
 static inline void pagecache_acct(int count)
 {
 	long *local;
 
+	preempt_disable();
 	local = &__get_cpu_var(nr_pagecache_local);
 	*local += count;
 	if (*local > PAGECACHE_ACCT_THRESHOLD || *local < -PAGECACHE_ACCT_THRESHOLD) {
 		atomic_add(*local, &nr_pagecache);
 		*local = 0;
 	}
+	preempt_enable();
 }
 
 #else
Index: xx-sources/include/linux/pagevec.h
===================================================================
--- xx-sources.orig/include/linux/pagevec.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/pagevec.h	2004-10-16 20:57:00.000000000 -0400
@@ -5,7 +5,7 @@
  * pages.  A pagevec is a multipage container which is used for that.
  */
 
-#define PAGEVEC_SIZE	15
+#define PAGEVEC_SIZE	8
 
 struct page;
 struct address_space;
Index: xx-sources/include/linux/percpu.h
===================================================================
--- xx-sources.orig/include/linux/percpu.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/linux/percpu.h	2004-10-16 20:57:00.000000000 -0400
@@ -8,13 +8,28 @@
 
 /* Enough to cover all DEFINE_PER_CPUs in kernel, including modules. */
 #ifndef PERCPU_ENOUGH_ROOM
-#define PERCPU_ENOUGH_ROOM 32768
+#define PERCPU_ENOUGH_ROOM (192*1024)
 #endif
 
 /* Must be an lvalue. */
 #define get_cpu_var(var) (*({ preempt_disable(); &__get_cpu_var(var); }))
 #define put_cpu_var(var) preempt_enable()
 
+/*
+ * Per-CPU data structures with an additional lock - useful for
+ * PREEMPT_REALTIME code that wants to reschedule but also wants
+ * per-CPU data structures. 
+ *
+ * NOTE: on normal !PREEMPT_REALTIME kernels these per-CPU variables
+ * are the same as the normal per-CPU variables.
+ */
+#define get_cpu_var_locked(var, cpu) \
+		(*({ spin_lock(&__get_cpu_lock(var, cpu)); \
+		&__get_cpu_var_locked(var, cpu); }))
+
+#define put_cpu_var_locked(var, cpu) \
+		 do { (void)cpu; spin_unlock(&__get_cpu_lock(var, cpu)); } while (0)
+
 #ifdef CONFIG_SMP
 
 struct percpu_data {
Index: xx-sources/include/linux/percpu_counter.h
===================================================================
--- xx-sources.orig/include/linux/percpu_counter.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/percpu_counter.h	2004-10-16 20:57:00.000000000 -0400
@@ -15,7 +15,7 @@
 #ifdef CONFIG_SMP
 
 struct percpu_counter {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	long count;
 	long *counters;
 };
Index: xx-sources/include/linux/preempt.h
===================================================================
--- xx-sources.orig/include/linux/preempt.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/preempt.h	2004-10-16 20:57:00.000000000 -0400
@@ -9,14 +9,22 @@
 #include <linux/config.h>
 #include <linux/linkage.h>
 
-#ifdef CONFIG_DEBUG_PREEMPT
-  extern void fastcall add_preempt_count(int val);
-  extern void fastcall sub_preempt_count(int val);
+#if defined(CONFIG_DEBUG_PREEMPT) || defined(CONFIG_PREEMPT_TIMING)
+  extern void notrace add_preempt_count(int val);
+  extern void notrace sub_preempt_count(int val);
 #else
 # define add_preempt_count(val)	do { preempt_count() += (val); } while (0)
 # define sub_preempt_count(val)	do { preempt_count() -= (val); } while (0)
 #endif
 
+#if defined(CONFIG_PREEMPT_TIMING)
+  extern void touch_preempt_timing(void);
+  extern void stop_preempt_timing(void);
+#else
+# define touch_preempt_timing()	do { } while (0)
+# define stop_preempt_timing()	do { } while (0)
+#endif
+
 #define inc_preempt_count() add_preempt_count(1)
 #define dec_preempt_count() sub_preempt_count(1)
 
Index: xx-sources/include/linux/radix-tree.h
===================================================================
--- xx-sources.orig/include/linux/radix-tree.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/linux/radix-tree.h	2004-10-16 20:57:00.000000000 -0400
@@ -19,6 +19,7 @@
 #ifndef _LINUX_RADIX_TREE_H
 #define _LINUX_RADIX_TREE_H
 
+#include <linux/config.h>
 #include <linux/preempt.h>
 #include <linux/types.h>
 
@@ -51,7 +52,18 @@
 unsigned int
 radix_tree_gang_lookup(struct radix_tree_root *root, void **results,
 			unsigned long first_index, unsigned int max_items);
+/*
+ * On a mutex based kernel we can freely schedule within the radix code:
+ */
+#ifdef CONFIG_PREEMPT_REALTIME
+static inline int radix_tree_preload(int gfp_mask)
+{
+	return 0;
+}
+#else
 int radix_tree_preload(int gfp_mask);
+#endif
+
 void radix_tree_init(void);
 void *radix_tree_tag_set(struct radix_tree_root *root,
 			unsigned long index, int tag);
@@ -66,7 +78,9 @@
 
 static inline void radix_tree_preload_end(void)
 {
+#ifndef CONFIG_PREEMPT_REALTIME
 	preempt_enable();
+#endif
 }
 
 #endif /* _LINUX_RADIX_TREE_H */
Index: xx-sources/include/linux/rcupdate.h
===================================================================
--- xx-sources.orig/include/linux/rcupdate.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/rcupdate.h	2004-10-16 20:57:00.000000000 -0400
@@ -193,6 +193,21 @@
  */
 #define rcu_read_unlock()	preempt_enable()
 
+#ifdef CONFIG_PREEMPT_REALTIME
+# define rcu_read_lock_spin(lock)	spin_lock(lock)
+# define rcu_read_unlock_spin(lock)	spin_unlock(lock)
+#else
+# define rcu_read_lock_spin(lock)	rcu_read_lock()
+# define rcu_read_unlock_spin(lock)	rcu_read_unlock()
+#endif
+
+#ifdef CONFIG_PREEMPT_REALTIME
+# define rcu_read_lock_sem(lock)	down(lock)
+# define rcu_read_unlock_sem(lock)	up(lock)
+#else
+# define rcu_read_lock_sem(lock)	rcu_read_lock()
+# define rcu_read_unlock_sem(lock)	rcu_read_unlock()
+#endif
 /*
  * So where is rcu_write_lock()?  It does not exist, as there is no
  * way for writers to lock out RCU readers.  This is a feature, not
@@ -223,6 +238,15 @@
  */
 #define rcu_read_unlock_bh()	local_bh_enable()
 
+#ifdef CONFIG_PREEMPT_REALTIME
+# define rcu_read_lock_bh_spin(lock)	spin_lock(lock)
+# define rcu_read_unlock_bh_spin(lock)	spin_unlock(lock)
+#else
+# define rcu_read_lock_bh_spin(lock) \
+		do { (void)lock; rcu_read_lock(); } while (0)
+# define rcu_read_unlock_bh_spin(lock) \
+		do { (void)lock; rcu_read_unlock(); } while (0)
+#endif
 /**
  * rcu_dereference - fetch an RCU-protected pointer in an
  * RCU read-side critical section.  This pointer may later
Index: xx-sources/include/linux/realtime_lock.h
===================================================================
--- xx-sources.orig/include/linux/realtime_lock.h	2004-10-14 14:49:41.073057592 -0400
+++ xx-sources/include/linux/realtime_lock.h	2004-10-16 22:01:18.687297376 -0400
@@ -0,0 +1,103 @@
+#ifndef __LINUX_REALTIME_LOCK_H
+#define __LINUX_REALTIME_LOCK_H
+
+#include <linux/config.h>
+#include <linux/list.h>
+#include <asm/atomic.h>
+#define RWSEM_DEBUG 0
+/*
+ * These are the basic SMP spinlocks, allowing only a single CPU anywhere.
+ * We use them 
+ */
+
+#ifdef CONFIG_SMP
+typedef struct {
+	volatile unsigned long lock;
+# ifdef CONFIG_DEBUG_SPINLOCK
+	unsigned int magic;
+# endif
+# ifdef CONFIG_PREEMPT
+	unsigned int break_lock;
+# endif
+} raw_spinlock_t;
+#else
+  typedef struct { } raw_spinlock_t;
+# define __RAW_SPIN_LOCK_UNLOCKED { }
+# define RAW_SPIN_LOCK_UNLOCKED (raw_spinlock_t) __RAW_SPIN_LOCK_UNLOCKED
+#endif
+
+/*
+ * Read-write spinlocks, allowing multiple readers
+ * but only one writer.
+ *
+ * NOTE! it is quite common to have readers in interrupts
+ * but no interrupt writers. For those circumstances we
+ * can "mix" irq-safe locks - any writer needs to get a
+ * irq-safe write-lock, but readers can get non-irqsafe
+ * read-locks.
+ */
+#ifdef CONFIG_SMP
+typedef struct {
+	volatile unsigned long lock;
+# ifdef CONFIG_DEBUG_SPINLOCK
+	unsigned magic;
+# endif
+# ifdef CONFIG_PREEMPT
+	unsigned int break_lock;
+# endif
+} raw_rwlock_t;
+#else
+  typedef struct { } raw_rwlock_t;
+# define __RAW_RW_LOCK_UNLOCKED { }
+# define RAW_RW_LOCK_UNLOCKED (raw_rwlock_t) __RAW_RW_LOCK_UNLOCKED
+#endif
+
+typedef struct __wait_queue_head {
+	raw_spinlock_t lock;
+	struct list_head task_list;
+} wait_queue_head_t;
+
+struct semaphore {
+	atomic_t count;
+	int sleepers;
+	wait_queue_head_t wait;
+};
+
+#ifdef CONFIG_PREEMPT_REALTIME
+
+  typedef struct {
+	unsigned int initialized;
+	struct semaphore lock;
+	unsigned int break_lock;
+  } _mutex_t;
+# define SPIN_LOCK_UNLOCKED (_mutex_t) { 0, }
+#else
+  typedef raw_spinlock_t _mutex_t;
+# define SPIN_LOCK_UNLOCKED RAW_SPIN_LOCK_UNLOCKED
+#endif
+
+typedef _mutex_t spinlock_t;
+
+#ifdef CONFIG_PREEMPT_REALTIME
+struct rw_semaphore {
+        __s32                   activity;
+        raw_spinlock_t          wait_lock;
+        struct list_head        wait_list;
+#if RWSEM_DEBUG
+        int                     debug;
+#endif
+};
+ typedef struct {
+	unsigned int initialized;
+	struct rw_semaphore lock;
+	unsigned int break_lock;
+  } rw_mutex_t;
+# define RW_LOCK_UNLOCKED (rw_mutex_t) { 0, }
+#else
+  typedef raw_rwlock_t rw_mutex_t;
+# define RW_LOCK_UNLOCKED RAW_RW_LOCK_UNLOCKED
+#endif
+
+typedef rw_mutex_t rwlock_t;
+
+#endif
Index: xx-sources/include/linux/rwsem-spinlock.h
===================================================================
--- xx-sources.orig/include/linux/rwsem-spinlock.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/rwsem-spinlock.h	2004-10-16 20:57:00.000000000 -0400
@@ -28,14 +28,16 @@
  * - if activity is -1 then there is one active writer
  * - if wait_list is not empty, then there are processes waiting for the semaphore
  */
+#if 0
 struct rw_semaphore {
 	__s32			activity;
-	spinlock_t		wait_lock;
+	raw_spinlock_t		wait_lock;
 	struct list_head	wait_list;
 #if RWSEM_DEBUG
 	int			debug;
 #endif
 };
+#endif
 
 /*
  * initialisation
@@ -47,7 +49,7 @@
 #endif
 
 #define __RWSEM_INITIALIZER(name) \
-{ 0, SPIN_LOCK_UNLOCKED, LIST_HEAD_INIT((name).wait_list) __RWSEM_DEBUG_INIT }
+{ 0, RAW_SPIN_LOCK_UNLOCKED, LIST_HEAD_INIT((name).wait_list) __RWSEM_DEBUG_INIT }
 
 #define DECLARE_RWSEM(name) \
 	struct rw_semaphore name = __RWSEM_INITIALIZER(name)
Index: xx-sources/include/linux/rwsem.h
===================================================================
--- xx-sources.orig/include/linux/rwsem.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/rwsem.h	2004-10-16 20:57:00.000000000 -0400
@@ -21,6 +21,8 @@
 
 struct rw_semaphore;
 
+extern void print_rwsem(struct rw_semaphore *sem);
+
 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
 #include <linux/rwsem-spinlock.h> /* use a generic implementation */
 #else
Index: xx-sources/include/linux/sched.h
===================================================================
--- xx-sources.orig/include/linux/sched.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/sched.h	2004-10-16 22:00:56.060737136 -0400
@@ -21,6 +21,53 @@
 #include <asm/ptrace.h>
 #include <asm/mmu.h>
 
+#ifdef CONFIG_PREEMPT
+extern int kernel_preemption;
+#else
+# define kernel_preemption 0
+#endif
+#ifdef CONFIG_PREEMPT_VOLUNTARY
+extern int voluntary_preemption;
+#else
+# define voluntary_preemption 0
+#endif
+#ifdef CONFIG_PREEMPT_SOFTIRQS
+extern int softirq_preemption;
+#else
+# define softirq_preemption 0
+#endif
+#ifdef CONFIG_PREEMPT_HARDIRQS
+extern int hardirq_preemption;
+#else
+# define hardirq_preemption 0
+#endif
+
+#if defined(CONFIG_PREEMPT_TRACE) || defined(CONFIG_LATENCY_TRACE)
+  extern void print_traces(void);
+#else
+# define print_traces() do { } while (0)
+#endif
+
+#ifdef CONFIG_LATENCY_TRACE
+  extern void notrace __trace(unsigned long eip, unsigned long parent_eip);
+  extern void nmi_trace(unsigned long eip, unsigned long parent_eip, unsigned long flags);
+  extern void stop_trace(void);
+#else
+# define __trace(eip, parent_eip) do { } while (0)
+# define nmi_trace(eip, parent_eip, flags) do { } while (0)
+# define stop_trace() do { } while (0)
+#endif
+
+
+#ifdef CONFIG_X86_LOCAL_APIC
+extern void nmi_show_all_regs(void);
+#else
+# define nmi_show_all_regs() do { } while (0)
+#endif
+extern unsigned long preempt_thresh;
+extern unsigned long preempt_max_latency;
+extern int trace_enabled;
+
 #include <linux/smp.h>
 #include <linux/sem.h>
 #include <linux/signal.h>
@@ -122,8 +169,8 @@
 #define TASK_UNINTERRUPTIBLE	2
 #define TASK_STOPPED		4
 #define TASK_TRACED		8
-#define TASK_ZOMBIE		16
-#define TASK_DEAD		32
+#define __TASK_ZOMBIE		16
+#define __TASK_DEAD		32
 
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)->state = (state_value); } while (0)
@@ -631,6 +678,7 @@
 
 /* task state */
 	struct linux_binfmt *binfmt;
+	long exit_state;
 	int exit_code, exit_signal;
 	int pdeath_signal;  /*  The signal sent when the parent dies  */
 	/* ??? */
@@ -716,11 +764,11 @@
    	u32 parent_exec_id;
    	u32 self_exec_id;
 /* Protection of (de-)allocation: mm, files, fs, tty, keyrings */
-	spinlock_t alloc_lock;
+	raw_spinlock_t alloc_lock;
 /* Protection of proc_dentry: nesting proc_lock, dcache_lock, write_lock_irq(&tasklist_lock); */
 	spinlock_t proc_lock;
 /* context-switch lock */
-	spinlock_t switch_lock;
+	raw_spinlock_t switch_lock;
 
 /* journalling filesystem info */
 	void *journal_info;
@@ -752,6 +800,13 @@
 	int cpuset_mems_generation;
 #endif
 
+#define MAX_PREEMPT_TRACE 16
+
+#ifdef CONFIG_PREEMPT_TRACE
+	unsigned long preempt_trace_eip[MAX_PREEMPT_TRACE];
+	unsigned long preempt_trace_parent_eip[MAX_PREEMPT_TRACE];
+#endif
+
 	struct list_head private_pages;	/* per-process private pages */
 	int private_pages_count;
 };
@@ -797,6 +852,8 @@
 #define PF_UISLEEP	0x02000000	/* Uninterruptible sleep */
 #define PF_SINBINNED	0x04000000	/* I am sinbinned */
 #define PF_UNPRIV_RT	0x08000000	/* I wanted to be RT but had insufficient privilege*/
+#define PF_SOFTIRQ	0x10000000      /* softirq context */
+#define PF_HARDIRQ	0x20000000      /* hardirq context */
 
 /*
  * Scheduling statistics for a task/thread
@@ -1162,24 +1219,14 @@
 {
 	return unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));
 }
-  
+
 static inline int need_resched(void)
 {
+	touch_preempt_timing();
 	return unlikely(test_thread_flag(TIF_NEED_RESCHED));
 }
 
 /*
- * cond_resched() and cond_resched_lock(): latency reduction via
- * explicit rescheduling in places that are safe. The return
- * value indicates whether a reschedule was done in fact.
- * cond_resched_lock() will drop the spinlock before scheduling,
- * cond_resched_softirq() will enable bhs before scheduling.
- */
-extern int cond_resched(void);
-extern int cond_resched_lock(spinlock_t * lock);
-extern int cond_resched_softirq(void);
-
-/*
  * Does a critical section need to be broken due to another
  * task waiting?:
  */
@@ -1200,6 +1247,48 @@
 	return 0;
 }
 
+static inline int softirq_need_resched(void)
+{
+	if (softirq_preemption)
+		return need_resched();
+	return 0;
+}
+
+static inline int hardirq_need_resched(void)
+{
+	if (current->flags & PF_HARDIRQ)
+		return need_resched();
+	return 0;
+}
+
+/*
+ * cond_resched() and cond_resched_lock(): latency reduction via
+ * explicit rescheduling in places that are safe. The return
+ * value indicates whether a reschedule was done in fact.
+ * cond_resched_lock() will drop the spinlock before scheduling,
+ * cond_resched_softirq() will enable bhs before scheduling.
+ */
+extern int cond_resched(void);
+extern int __cond_resched_raw_spinlock(raw_spinlock_t *lock);
+extern int __cond_resched_mutex(_mutex_t *mutex);
+
+#define cond_resched_lock(lock) \
+({								\
+	int __ret;						\
+								\
+	if (TYPE_EQUAL((lock), raw_spinlock_t))	 		\
+		__ret = __cond_resched_raw_spinlock((raw_spinlock_t *)lock);\
+	else if (TYPE_EQUAL(lock, _mutex_t))			\
+		__ret = __cond_resched_mutex((_mutex_t *)lock);	\
+	else __ret = __bad_spinlock_type();			\
+								\
+	__ret;							\
+})
+
+extern int cond_resched_softirq(void);
+extern int cond_resched_hardirq(void);
+extern int cond_resched_all(void);
+
 /* Reevaluate whether the task has signals pending delivery.
    This is required every time the blocked sigset_t changes.
    callers must hold sighand->siglock.  */
Index: xx-sources/include/linux/sem.h
===================================================================
--- xx-sources.orig/include/linux/sem.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/sem.h	2004-10-16 20:57:00.000000000 -0400
@@ -126,7 +126,7 @@
  */ 
 struct sem_undo_list {
 	atomic_t	refcnt;
-	spinlock_t	lock;
+	raw_spinlock_t	lock;
 	struct sem_undo	*proc_list;
 };
 
Index: xx-sources/include/linux/seqlock.h
===================================================================
--- xx-sources.orig/include/linux/seqlock.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/linux/seqlock.h	2004-10-16 20:57:00.000000000 -0400
@@ -33,35 +33,57 @@
 typedef struct {
 	unsigned sequence;
 	spinlock_t lock;
-} seqlock_t;
+} __seqlock_t;
+
+typedef struct {
+	unsigned sequence;
+	raw_spinlock_t lock;
+} __raw_seqlock_t;
+
+#define USE_SEQLOCK_MUTEX
+
+#ifdef USE_SEQLOCK_MUTEX
+typedef __seqlock_t seqlock_t;
+#else
+typedef __raw_seqlock_t seqlock_t;
+#endif
+typedef __raw_seqlock_t raw_seqlock_t;
 
 /*
  * These macros triggered gcc-3.x compile-time problems.  We think these are
  * OK now.  Be cautious.
  */
+#ifdef USE_SEQLOCK_MUTEX
 #define SEQLOCK_UNLOCKED { 0, SPIN_LOCK_UNLOCKED }
 #define seqlock_init(x)	do { *(x) = (seqlock_t) SEQLOCK_UNLOCKED; } while (0)
-
+#else
+#define SEQLOCK_UNLOCKED { 0, RAW_SPIN_LOCK_UNLOCKED }
+#define seqlock_init(x)	do { *(x) = (seqlock_t) RAW_SEQLOCK_UNLOCKED; } while (0)
+#endif
+
+#define RAW_SEQLOCK_UNLOCKED { 0, RAW_SPIN_LOCK_UNLOCKED }
+#define raw_seqlock_init(x) \
+		do { *(x) = (raw_seqlock_t) RAW_SEQLOCK_UNLOCKED; } while (0)
 
 /* Lock out other writers and update the count.
  * Acts like a normal spin_lock/unlock.
  * Don't need preempt_disable() because that is in the spin_lock already.
  */
-static inline void write_seqlock(seqlock_t *sl)
+static inline void __write_seqlock(seqlock_t *sl)
 {
 	spin_lock(&sl->lock);
 	++sl->sequence;
 	smp_wmb();			
 }	
 
-static inline void write_sequnlock(seqlock_t *sl) 
+static inline void __write_sequnlock(seqlock_t *sl) 
 {
 	smp_wmb();
 	sl->sequence++;
 	spin_unlock(&sl->lock);
 }
 
-static inline int write_tryseqlock(seqlock_t *sl)
+static inline int __write_tryseqlock(seqlock_t *sl)
 {
 	int ret = spin_trylock(&sl->lock);
 
@@ -73,7 +95,7 @@
 }
 
 /* Start of read calculation -- fetch last complete writer token */
-static inline unsigned read_seqbegin(const seqlock_t *sl)
+static inline unsigned __read_seqbegin(const seqlock_t *sl)
 {
 	unsigned ret = sl->sequence;
 	smp_rmb();
@@ -88,13 +110,114 @@
  *    
  * Using xor saves one conditional branch.
  */
-static inline int read_seqretry(const seqlock_t *sl, unsigned iv)
+static inline int __read_seqretry(const seqlock_t *sl, unsigned iv)
+{
+	smp_rmb();
+	return (iv & 1) | (sl->sequence ^ iv);
+}
+
+static inline void __write_seqlock_raw(raw_seqlock_t *sl)
+{
+	spin_lock(&sl->lock);
+	++sl->sequence;
+	smp_wmb();			
+}	
+
+static inline void __write_sequnlock_raw(raw_seqlock_t *sl) 
+{
+	smp_wmb();
+	sl->sequence++;
+	spin_unlock(&sl->lock);
+}
+
+static inline int __write_tryseqlock_raw(raw_seqlock_t *sl)
+{
+	int ret = spin_trylock(&sl->lock);
+
+	if (ret) {
+		++sl->sequence;
+		smp_wmb();			
+	}
+	return ret;
+}
+
+static inline unsigned __read_seqbegin_raw(const raw_seqlock_t *sl)
+{
+	unsigned ret = sl->sequence;
+	smp_rmb();
+	return ret;
+}
+
+static inline int __read_seqretry_raw(const raw_seqlock_t *sl, unsigned iv)
 {
 	smp_rmb();
 	return (iv & 1) | (sl->sequence ^ iv);
 }
 
 
+extern int __bad_seqlock_type(void);
+
+#define PICK_SEQOP(op, lock)					\
+do {								\
+	if (TYPE_EQUAL((lock), seqlock_t))			\
+		op((seqlock_t *)(lock));			\
+	else if (TYPE_EQUAL(lock, raw_seqlock_t))		\
+		op##_raw((raw_seqlock_t *)(lock));		\
+	else __bad_seqlock_type();				\
+} while (0)
+
+#define PICK_SEQOP_RET(op, lock)				\
+({								\
+	unsigned int __ret;					\
+								\
+	if (TYPE_EQUAL((lock), seqlock_t))			\
+		__ret = op((seqlock_t *)(lock));		\
+	else if (TYPE_EQUAL(lock, raw_seqlock_t))		\
+		__ret = op##_raw((raw_seqlock_t *)(lock));	\
+	else __ret = __bad_seqlock_type();			\
+								\
+	__ret;							\
+})
+
+#define PICK_SEQOP_CONST_RET(op, lock)				\
+({								\
+	unsigned int __ret;					\
+								\
+	if (TYPE_EQUAL((lock), seqlock_t))			\
+		__ret = op((const seqlock_t *)(lock));		\
+	else if (TYPE_EQUAL(lock, raw_seqlock_t))		\
+		__ret = op##_raw((const raw_seqlock_t *)(lock));\
+	else __ret = __bad_seqlock_type();			\
+								\
+	__ret;							\
+})
+
+#define PICK_SEQOP2_CONST_RET(op, lock, arg)				\
+({									\
+	unsigned int __ret;						\
+									\
+	if (TYPE_EQUAL((lock), seqlock_t))				\
+		__ret = op((const seqlock_t *)(lock), (arg));		\
+	else if (TYPE_EQUAL(lock, raw_seqlock_t))			\
+		__ret = op##_raw((const raw_seqlock_t *)(lock), (arg));	\
+	else __ret = __bad_seqlock_type();				\
+									\
+	__ret;								\
+})
+
+
+#define write_seqlock(sl)	PICK_SEQOP(__write_seqlock, sl)
+#define write_sequnlock(sl)	PICK_SEQOP(__write_sequnlock, sl)
+#define write_tryseqlock(sl)	PICK_SEQOP_RET(__write_tryseqlock, sl)
+#define read_seqbegin(sl)	PICK_SEQOP_CONST_RET(__read_seqbegin, sl)
+#define read_seqretry(sl, iv)	PICK_SEQOP2_CONST_RET(__read_seqretry, sl, iv)
+
+#define DECLARE_SEQLOCK(name) \
+	seqlock_t name __cacheline_aligned_in_smp = SEQLOCK_UNLOCKED
+
+#define DECLARE_RAW_SEQLOCK(name) \
+	raw_seqlock_t name __cacheline_aligned_in_smp = RAW_SEQLOCK_UNLOCKED
+
 /*
  * Version using sequence counter only.
  * This can be used when code has its own mutex protecting the
@@ -145,30 +268,50 @@
 	s->sequence++;
 }
 
+#define PICK_IRQOP(op, lock)					\
+do {								\
+	if (TYPE_EQUAL((lock), raw_seqlock_t))			\
+		op();						\
+	else if (TYPE_EQUAL((lock), seqlock_t))			\
+		{ /* nothing */ }				\
+	else __bad_seqlock_type();				\
+} while (0)
+
+#define PICK_IRQOP2(op, arg, lock)				\
+do {								\
+	if (TYPE_EQUAL((lock), raw_seqlock_t))			\
+		op(arg);					\
+	else if (TYPE_EQUAL(lock, seqlock_t))			\
+		{ /* nothing */ }				\
+	else __bad_seqlock_type();				\
+} while (0)
+
+
+
 /*
  * Possible sw/hw IRQ protected versions of the interfaces.
  */
 #define write_seqlock_irqsave(lock, flags)				\
-	do { local_irq_save(flags); write_seqlock(lock); } while (0)
+	do { PICK_IRQOP2(local_irq_save, flags, lock); write_seqlock(lock); } while (0)
 #define write_seqlock_irq(lock)						\
-	do { local_irq_disable();   write_seqlock(lock); } while (0)
+	do { PICK_IRQOP(local_irq_disable, lock); write_seqlock(lock); } while (0)
 #define write_seqlock_bh(lock)						\
-        do { local_bh_disable();    write_seqlock(lock); } while (0)
+        do { PICK_IRQOP(local_bh_disable, lock); write_seqlock(lock); } while (0)
 
 #define write_sequnlock_irqrestore(lock, flags)				\
-	do { write_sequnlock(lock); local_irq_restore(flags); } while(0)
+	do { write_sequnlock(lock); PICK_IRQOP2(local_irq_restore, flags, lock); } while(0)
 #define write_sequnlock_irq(lock)					\
-	do { write_sequnlock(lock); local_irq_enable(); } while(0)
+	do { write_sequnlock(lock); PICK_IRQOP(local_irq_enable, lock); } while(0)
 #define write_sequnlock_bh(lock)					\
-	do { write_sequnlock(lock); local_bh_enable(); } while(0)
+	do { write_sequnlock(lock); PICK_IRQOP(local_bh_enable, lock); } while(0)
 
 #define read_seqbegin_irqsave(lock, flags)				\
-	({ local_irq_save(flags);   read_seqbegin(lock); })
+	({ PICK_IRQOP2(local_irq_save, flags, lock); read_seqbegin(lock); })
 
 #define read_seqretry_irqrestore(lock, iv, flags)			\
 	({								\
 		int ret = read_seqretry(lock, iv);			\
-		local_irq_restore(flags);				\
+		PICK_IRQOP2(local_irq_restore, flags, lock);		\
 		ret;							\
 	})
 
Index: xx-sources/include/linux/spin_undefs.h
===================================================================
--- xx-sources.orig/include/linux/spin_undefs.h	2004-10-14 14:49:41.073057592 -0400
+++ xx-sources/include/linux/spin_undefs.h	2004-10-16 20:57:00.000000000 -0400
@@ -0,0 +1,67 @@
+
+/*
+ * include/linux/spin_undefs.h - spinlock un-substitution
+ *
+ *  2004-08-12  RT Prototype 2004 (c) MontaVista Software, Inc.
+ *              This file is licensed under the terms of the GNU
+ *              General Public License version 2. This program
+ *              is licensed "as is" without any warranty of any kind,
+ *              whether express or implied.
+ *
+ * This file unmaps the kernel mutex operations associated with
+ * spinlocks. This file is used when all the spinlocks in a file
+ * are of the old type. It redefines existing definitions for
+ * spinlocks back to the old type.
+ * The file should be included using the following construct:
+ * #ifdef KMUTEX
+ * # include <linux/spin_undefs.h>
+ * #endif
+ *
+ *
+ * This is a dirty dirty hack, but it helps to compress the
+ * patch for mutexes down to 1 line in some files, which
+ * is essential for making a portable prototype.
+ * 
+ */
+
+/* use this include only in C files where all spinlock calls
+   are replaced with _spin... */
+
+#include <linux/config.h>
+
+#ifdef CONFIG_KMUTEX
+# undef  spin_lock
+# define spin_lock              _spin_lock
+
+# undef  spin_unlock
+# define spin_unlock            _spin_unlock
+
+# undef  spin_lock_bh
+# define spin_lock_bh   	_spin_lock_bh
+
+# undef  spin_unlock_bh
+# define spin_unlock_bh 	_spin_unlock_bh
+
+# undef  spin_lock_irq
+# define spin_lock_irq   	_spin_lock_irq
+
+# undef  spin_unlock_irq
+# define spin_unlock_irq 	_spin_unlock_irq
+
+# undef  spin_lock_irqsave
+# define spin_lock_irqsave      _spin_lock_irqsave
+
+# undef  spin_unlock_irqrestore
+# define spin_unlock_irqrestore _spin_unlock_irqrestore
+
+# undef  spin_is_locked
+# define spin_is_locked		_spin_is_locked
+
+
+# undef  spinlock_t
+# define spinlock_t             _spinlock_t
+
+# undef  SPIN_LOCK_UNLOCKED
+# define SPIN_LOCK_UNLOCKED _SPIN_LOCK_UNLOCKED
+#endif
+
Index: xx-sources/include/linux/spinlock.h
===================================================================
--- xx-sources.orig/include/linux/spinlock.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/spinlock.h	2004-10-16 20:57:00.000000000 -0400
@@ -13,6 +13,8 @@
 #include <linux/kernel.h>
 #include <linux/stringify.h>
 
+#include <linux/realtime_lock.h>
+
 #include <asm/processor.h>	/* for cpu relax */
 #include <asm/system.h>
 #ifdef CONFIG_KGDB
@@ -46,36 +48,36 @@
 
 #define __lockfunc fastcall __attribute__((section(".spinlock.text")))
 
-int __lockfunc _spin_trylock(spinlock_t *lock);
-int __lockfunc _read_trylock(rwlock_t *lock);
-int __lockfunc _write_trylock(rwlock_t *lock);
-void __lockfunc _spin_lock(spinlock_t *lock);
-void __lockfunc _write_lock(rwlock_t *lock);
-void __lockfunc _spin_lock(spinlock_t *lock);
-void __lockfunc _read_lock(rwlock_t *lock);
-void __lockfunc _spin_unlock(spinlock_t *lock);
-void __lockfunc _write_unlock(rwlock_t *lock);
-void __lockfunc _read_unlock(rwlock_t *lock);
-unsigned long __lockfunc _spin_lock_irqsave(spinlock_t *lock);
-unsigned long __lockfunc _read_lock_irqsave(rwlock_t *lock);
-unsigned long __lockfunc _write_lock_irqsave(rwlock_t *lock);
-void __lockfunc _spin_lock_irq(spinlock_t *lock);
-void __lockfunc _spin_lock_bh(spinlock_t *lock);
-void __lockfunc _read_lock_irq(rwlock_t *lock);
-void __lockfunc _read_lock_bh(rwlock_t *lock);
-void __lockfunc _write_lock_irq(rwlock_t *lock);
-void __lockfunc _write_lock_bh(rwlock_t *lock);
-void __lockfunc _spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags);
-void __lockfunc _spin_unlock_irq(spinlock_t *lock);
-void __lockfunc _spin_unlock_bh(spinlock_t *lock);
-void __lockfunc _read_unlock_irqrestore(rwlock_t *lock, unsigned long flags);
-void __lockfunc _read_unlock_irq(rwlock_t *lock);
-void __lockfunc _read_unlock_bh(rwlock_t *lock);
-void __lockfunc _write_unlock_irqrestore(rwlock_t *lock, unsigned long flags);
-void __lockfunc _write_unlock_irq(rwlock_t *lock);
-void __lockfunc _write_unlock_bh(rwlock_t *lock);
-int __lockfunc _spin_trylock_bh(spinlock_t *lock);
-int __lockfunc generic_raw_read_trylock(rwlock_t *lock);
+int __lockfunc _spin_trylock(raw_spinlock_t *lock);
+int __lockfunc _read_trylock(raw_rwlock_t *lock);
+int __lockfunc _write_trylock(raw_rwlock_t *lock);
+void __lockfunc _spin_lock(raw_spinlock_t *lock);
+void __lockfunc _write_lock(raw_rwlock_t *lock);
+void __lockfunc _spin_lock(raw_spinlock_t *lock);
+void __lockfunc _read_lock(raw_rwlock_t *lock);
+void __lockfunc _spin_unlock(raw_spinlock_t *lock);
+void __lockfunc _write_unlock(raw_rwlock_t *lock);
+void __lockfunc _read_unlock(raw_rwlock_t *lock);
+unsigned long __lockfunc _spin_lock_irqsave(raw_spinlock_t *lock);
+unsigned long __lockfunc _read_lock_irqsave(raw_rwlock_t *lock);
+unsigned long __lockfunc _write_lock_irqsave(raw_rwlock_t *lock);
+void __lockfunc _spin_lock_irq(raw_spinlock_t *lock);
+void __lockfunc _spin_lock_bh(raw_spinlock_t *lock);
+void __lockfunc _read_lock_irq(raw_rwlock_t *lock);
+void __lockfunc _read_lock_bh(raw_rwlock_t *lock);
+void __lockfunc _write_lock_irq(raw_rwlock_t *lock);
+void __lockfunc _write_lock_bh(raw_rwlock_t *lock);
+void __lockfunc _spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags);
+void __lockfunc _spin_unlock_irq(raw_spinlock_t *lock);
+void __lockfunc _spin_unlock_bh(raw_spinlock_t *lock);
+void __lockfunc _read_unlock_irqrestore(raw_rwlock_t *lock, unsigned long flags);
+void __lockfunc _read_unlock_irq(raw_rwlock_t *lock);
+void __lockfunc _read_unlock_bh(raw_rwlock_t *lock);
+void __lockfunc _write_unlock_irqrestore(raw_rwlock_t *lock, unsigned long flags);
+void __lockfunc _write_unlock_irq(raw_rwlock_t *lock);
+void __lockfunc _write_unlock_bh(raw_rwlock_t *lock);
+int __lockfunc _spin_trylock_bh(raw_spinlock_t *lock);
+int __lockfunc generic_raw_read_trylock(raw_rwlock_t *lock);
 int in_lock_functions(unsigned long addr);
 
 #else
@@ -83,7 +85,7 @@
 #define in_lock_functions(ADDR) 0
 
 #if !defined(CONFIG_PREEMPT) && !defined(CONFIG_DEBUG_SPINLOCK)
-# define atomic_dec_and_lock(atomic,lock) atomic_dec_and_test(atomic)
+# define _atomic_dec_and_spin_lock(atomic,lock) atomic_dec_and_test(atomic)
 # define ATOMIC_DEC_AND_LOCK
 #endif
 
@@ -191,39 +193,29 @@
 		(x)->lock = 0; \
 	} while (0)
 #else
-/*
- * gcc versions before ~2.95 have a nasty bug with empty initializers.
- */
-#if (__GNUC__ > 2)
-  typedef struct { } spinlock_t;
-  #define SPIN_LOCK_UNLOCKED (spinlock_t) { }
-#else
-  typedef struct { int gcc_is_buggy; } spinlock_t;
-  #define SPIN_LOCK_UNLOCKED (spinlock_t) { 0 }
-#endif
+
+// typedef struct { } raw_spinlock_t;
+// #define __RAW_SPIN_LOCK_UNLOCKED { }
+// #define RAW_SPIN_LOCK_UNLOCKED (raw_spinlock_t) __RAW_SPIN_LOCK_UNLOCKED
 
 /*
  * If CONFIG_SMP is unset, declare the _raw_* definitions as nops
  */
-#define spin_lock_init(lock)	do { (void)(lock); } while(0)
-#define _raw_spin_lock(lock)	do { (void)(lock); } while(0)
-#define spin_is_locked(lock)	((void)(lock), 0)
-#define _raw_spin_trylock(lock)	(((void)(lock), 1))
-#define spin_unlock_wait(lock)	(void)(lock);
-#define _raw_spin_unlock(lock) do { (void)(lock); } while(0)
+#define _raw_spin_lock_init(lock)	do { (void)(lock); } while(0)
+#define _raw_spin_lock(lock)		do { (void)(lock); } while(0)
+#define _raw_spin_is_locked(lock)	((void)(lock), 0)
+#define _raw_spin_trylock(lock)		(((void)(lock), 1))
+#define _raw_spin_unlock_wait(lock)	(void)(lock)
+#define _raw_spin_unlock(lock) 		do { (void)(lock); } while(0)
 #endif /* CONFIG_DEBUG_SPINLOCK */
 
 /* RW spinlocks: No debug version */
 
-#if (__GNUC__ > 2)
-  typedef struct { } rwlock_t;
-  #define RW_LOCK_UNLOCKED (rwlock_t) { }
-#else
-  typedef struct { int gcc_is_buggy; } rwlock_t;
-  #define RW_LOCK_UNLOCKED (rwlock_t) { 0 }
-#endif
+// typedef struct { } raw_rwlock_t;
+// #define __RAW_RW_LOCK_UNLOCKED { }
+// #define RAW_RW_LOCK_UNLOCKED (raw_rwlock_t) __RAW_RW_LOCK_UNLOCKED
 
-#define rwlock_init(lock)	do { (void)(lock); } while(0)
+#define _raw_rwlock_init(lock)	do { (void)(lock); } while(0)
 #define _raw_read_lock(lock)	do { (void)(lock); } while(0)
 #define _raw_read_unlock(lock)	do { (void)(lock); } while(0)
 #define _raw_write_lock(lock)	do { (void)(lock); } while(0)
@@ -280,12 +272,13 @@
 	preempt_enable(); \
 } while(0)
 
-#define _spin_lock_irqsave(lock, flags) \
-do {	\
-	local_irq_save(flags); \
+#define _spin_lock_irqsave(lock) \
+({	unsigned long __flags; \
+	local_irq_save(__flags); \
 	preempt_disable(); \
 	_raw_spin_lock(lock); \
-} while (0)
+	__flags; \
+})
 
 #define _spin_lock_irq(lock) \
 do { \
@@ -301,12 +294,13 @@
 	_raw_spin_lock(lock); \
 } while (0)
 
-#define _read_lock_irqsave(lock, flags) \
-do {	\
-	local_irq_save(flags); \
+#define _read_lock_irqsave(lock) \
+({	unsigned long __flags; \
+	local_irq_save(__flags); \
 	preempt_disable(); \
 	_raw_read_lock(lock); \
-} while (0)
+	__flags; \
+})
 
 #define _read_lock_irq(lock) \
 do { \
@@ -322,12 +316,13 @@
 	_raw_read_lock(lock); \
 } while (0)
 
-#define _write_lock_irqsave(lock, flags) \
-do {	\
-	local_irq_save(flags); \
+#define _write_lock_irqsave(lock) \
+({	unsigned long __flags; \
+	local_irq_save(__flags); \
 	preempt_disable(); \
 	_raw_write_lock(lock); \
-} while (0)
+	__flags; \
+})
 
 #define _write_lock_irq(lock) \
 do { \
@@ -414,74 +409,269 @@
 
 #endif /* !SMP */
 
+extern int __bad_spinlock_type(void);
+
+extern void _mutex_lock(_mutex_t *mutex);
+extern void _mutex_lock_bh(_mutex_t *mutex);
+extern void _mutex_lock_irq(_mutex_t *mutex);
+extern unsigned long _mutex_lock_irqsave(_mutex_t *mutex);
+extern void _mutex_unlock(_mutex_t *mutex);
+extern void _mutex_unlock_wait(_mutex_t *mutex);
+extern void _mutex_unlock_bh(_mutex_t *mutex);
+extern void _mutex_unlock_irq(_mutex_t *mutex);
+extern void _mutex_unlock_irqrestore(_mutex_t *mutex, unsigned long flags);
+extern int _mutex_trylock(_mutex_t *mutex);
+extern int _mutex_trylock_bh(_mutex_t *mutex);
+extern int _mutex_is_locked(_mutex_t *mutex);
+extern int atomic_dec_and_mutex_lock(atomic_t *atomic, _mutex_t *mutex);
+extern void _mutex_lock_init(_mutex_t *mutex);
+
+#define TYPE_EQUAL(lock, type) \
+		__builtin_types_compatible_p(typeof(lock), type *)
+
+#define PICK_OP(type, optype, op, lock)				\
+do {								\
+	if (TYPE_EQUAL((lock), type))				\
+		_##optype##op((type *)(lock));			\
+	else if (TYPE_EQUAL(lock, _mutex_t))			\
+		_mutex##op((_mutex_t *)(lock));			\
+	else __bad_spinlock_type();				\
+} while (0)
+
+#define PICK_OP_RET(type, optype, op, lock...)			\
+({								\
+	int __ret;						\
+								\
+	if (TYPE_EQUAL((lock), type))	  			\
+		__ret = _##optype##op((type *)(lock));		\
+	else if (TYPE_EQUAL(lock, _mutex_t))			\
+		__ret = _mutex##op((_mutex_t *)(lock));		\
+	else __ret = __bad_spinlock_type();			\
+								\
+	__ret;							\
+})
+
+#define PICK_OP2(type, optype, op, lock, flags)			\
+do {								\
+	if (TYPE_EQUAL((lock), type))				\
+		_##optype##op((type *)(lock), flags);		\
+	else if (TYPE_EQUAL(lock, _mutex_t))			\
+		_mutex##op((_mutex_t *)(lock), flags);		\
+	else __bad_spinlock_type();				\
+} while (0)
+
+extern int _rw_mutex_read_trylock(rw_mutex_t *rw_mutex);
+extern int _rw_mutex_write_trylock(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_write_lock(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_read_lock(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_write_unlock(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_read_unlock(rw_mutex_t *rw_mutex);
+extern unsigned long _rw_mutex_write_lock_irqsave(rw_mutex_t *rw_mutex);
+extern unsigned long _rw_mutex_read_lock_irqsave(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_write_lock_irq(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_read_lock_irq(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_write_lock_bh(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_read_lock_bh(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_write_unlock_irq(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_read_unlock_irq(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_write_unlock_bh(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_read_unlock_bh(rw_mutex_t *rw_mutex);
+extern void _rw_mutex_write_unlock_irqrestore(rw_mutex_t *rw_mutex, unsigned long flags);
+extern void _rw_mutex_read_unlock_irqrestore(rw_mutex_t *rw_mutex, unsigned long flags);
+extern void _rw_mutex_init(rw_mutex_t *rw_mutex);
+extern int _rw_mutex_is_locked(rw_mutex_t *rw_mutex);
+
+#define __PICK_RW_OP(type, optype, op, lock)				\
+do {									\
+	if (TYPE_EQUAL((lock), type))					\
+		_##optype##op((type *)(lock));				\
+	else if (TYPE_EQUAL(lock, rw_mutex_t))				\
+		_rw_mutex##op((rw_mutex_t *)(lock));			\
+	else __bad_spinlock_type();					\
+} while (0)
+
+#define PICK_RW_OP(type, optype, op, lock)				\
+do {									\
+	if (TYPE_EQUAL((lock), type))					\
+		_##optype##op((type *)(lock));				\
+	else if (TYPE_EQUAL(lock, rw_mutex_t))				\
+		_rw_mutex_##optype##op((rw_mutex_t *)(lock));		\
+	else __bad_spinlock_type();					\
+} while (0)
+
+#define __PICK_RW_OP_RET(type, optype, op, lock...)			\
+({									\
+	int __ret;							\
+									\
+	if (TYPE_EQUAL((lock), type))	  				\
+		__ret = _##optype##op((type *)(lock));			\
+	else if (TYPE_EQUAL(lock, rw_mutex_t))				\
+		__ret = _rw_mutex##op((rw_mutex_t *)(lock));		\
+	else __ret = __bad_spinlock_type();				\
+									\
+	__ret;								\
+})
+
+#define PICK_RW_OP_RET(type, optype, op, lock...)			\
+({									\
+	int __ret;							\
+									\
+	if (TYPE_EQUAL((lock), type))	  				\
+		__ret = _##optype##op((type *)(lock));			\
+	else if (TYPE_EQUAL(lock, rw_mutex_t))				\
+		__ret = _rw_mutex_##optype##op((rw_mutex_t *)(lock));	\
+	else __ret = __bad_spinlock_type();				\
+									\
+	__ret;								\
+})
+
+#define PICK_RW_OP2(type, optype, op, lock, flags)			\
+do {									\
+	if (TYPE_EQUAL((lock), type))					\
+		_##optype##op((type *)(lock), flags);			\
+	else if (TYPE_EQUAL(lock, rw_mutex_t))				\
+		_rw_mutex_##optype##op((rw_mutex_t *)(lock), flags);	\
+	else __bad_spinlock_type();					\
+} while (0)
+
+#define _spin_lock_init _raw_spin_lock_init
+
+#define spin_lock_init(lock) \
+		PICK_OP(raw_spinlock_t, spin, _lock_init, lock)
+
+#define _rwlock_init _raw_rwlock_init
+
+#define rwlock_init(lock) \
+		__PICK_RW_OP(raw_rwlock_t, rwlock, _init, lock)
+
+#define _spin_is_locked _raw_spin_is_locked
+
+#define spin_is_locked(lock) \
+		PICK_OP_RET(raw_spinlock_t, spin, _is_locked, lock)
+
+#define _rwlock_is_locked _raw_rwlock_is_locked
+
+#define rwlock_is_locked(lock) \
+		__PICK_RW_OP_RET(raw_rwlock_t, rwlock, _is_locked, lock)
+
+#define _spin_unlock_wait _raw_spin_unlock_wait
+
+#define spin_unlock_wait(lock) \
+		PICK_OP(raw_spinlock_t, spin, _unlock_wait, lock)
 /*
  * Define the various spin_lock and rw_lock methods.  Note we define these
  * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The various
  * methods are defined as nops in the case they are not required.
  */
-#define spin_trylock(lock)	_spin_trylock(lock)
-#define read_trylock(lock)	_read_trylock(lock)
-#define write_trylock(lock)	_write_trylock(lock)
-
-#define spin_lock(lock)		_spin_lock(lock)
-#define write_lock(lock)	_write_lock(lock)
-#define read_lock(lock)		_read_lock(lock)
+// #define spin_trylock(lock)	_spin_trylock(lock)
+#define spin_trylock(lock)	PICK_OP_RET(raw_spinlock_t, spin, _trylock, lock)
 
-#ifdef CONFIG_SMP
-#define spin_lock_irqsave(lock, flags)	flags = _spin_lock_irqsave(lock)
-#define read_lock_irqsave(lock, flags)	flags = _read_lock_irqsave(lock)
-#define write_lock_irqsave(lock, flags)	flags = _write_lock_irqsave(lock)
-#else
-#define spin_lock_irqsave(lock, flags)	_spin_lock_irqsave(lock, flags)
-#define read_lock_irqsave(lock, flags)	_read_lock_irqsave(lock, flags)
-#define write_lock_irqsave(lock, flags)	_write_lock_irqsave(lock, flags)
-#endif
+//#define read_trylock(lock)	_read_trylock(lock)
+#define read_trylock(lock)	PICK_RW_OP_RET(raw_rwlock_t, read, _trylock, lock)
 
-#define spin_lock_irq(lock)		_spin_lock_irq(lock)
-#define spin_lock_bh(lock)		_spin_lock_bh(lock)
+//#define write_trylock(lock)	_write_trylock(lock)
+#define write_trylock(lock)	PICK_RW_OP_RET(raw_rwlock_t, write, _trylock, lock)
 
-#define read_lock_irq(lock)		_read_lock_irq(lock)
-#define read_lock_bh(lock)		_read_lock_bh(lock)
+// #define spin_lock(lock)	_spin_lock(lock)
+#define spin_lock(lock)		PICK_OP(raw_spinlock_t, spin, _lock, lock)
 
-#define write_lock_irq(lock)		_write_lock_irq(lock)
-#define write_lock_bh(lock)		_write_lock_bh(lock)
+//#define write_lock(lock)	_write_lock(lock)
+#define write_lock(lock)	PICK_RW_OP(raw_rwlock_t, write, _lock, lock)
 
-#define spin_unlock(lock)	_spin_unlock(lock)
-#define write_unlock(lock)	_write_unlock(lock)
-#define read_unlock(lock)	_read_unlock(lock)
-
-#define spin_unlock_irqrestore(lock, flags)	_spin_unlock_irqrestore(lock, flags)
-#define spin_unlock_irq(lock)		_spin_unlock_irq(lock)
-#define spin_unlock_bh(lock)		_spin_unlock_bh(lock)
-
-#define read_unlock_irqrestore(lock, flags)	_read_unlock_irqrestore(lock, flags)
-#define read_unlock_irq(lock)			_read_unlock_irq(lock)
-#define read_unlock_bh(lock)			_read_unlock_bh(lock)
-
-#define write_unlock_irqrestore(lock, flags)	_write_unlock_irqrestore(lock, flags)
-#define write_unlock_irq(lock)			_write_unlock_irq(lock)
-#define write_unlock_bh(lock)			_write_unlock_bh(lock)
-
-#define spin_trylock_bh(lock)			_spin_trylock_bh(lock)
-
-#ifdef CONFIG_LOCKMETER
-extern void _metered_spin_lock   (spinlock_t *lock);
-extern void _metered_spin_unlock (spinlock_t *lock);
-extern int  _metered_spin_trylock(spinlock_t *lock);
-extern void _metered_read_lock    (rwlock_t *lock);
-extern void _metered_read_unlock  (rwlock_t *lock);
-extern void _metered_write_lock   (rwlock_t *lock);
-extern void _metered_write_unlock (rwlock_t *lock);
-extern int  _metered_read_trylock (rwlock_t *lock);
-extern int  _metered_write_trylock(rwlock_t *lock);
-#endif
+// #define read_lock(lock)		_read_lock(lock)
+#define read_lock(lock)		PICK_RW_OP(raw_rwlock_t, read, _lock, lock)
+
+#ifdef CONFIG_SMP
+// #define spin_lock_irqsave(lock, flags)	flags = _spin_lock_irqsave(lock)
+// #define read_lock_irqsave(lock, flags)	flags = _read_lock_irqsave(lock)
+// #define write_lock_irqsave(lock, flags)	flags = _write_lock_irqsave(lock)
+#else
+// #define spin_lock_irqsave(lock, flags)	_spin_lock_irqsave(lock, flags)
+// #define read_lock_irqsave(lock, flags)	_read_lock_irqsave(lock, flags)
+// #define write_lock_irqsave(lock, flags)	_write_lock_irqsave(lock, flags)
+#endif
+
+# define spin_lock_irqsave(lock, flags) \
+	flags = PICK_OP_RET(raw_spinlock_t, spin, _lock_irqsave, lock)
+# define read_lock_irqsave(lock, flags) \
+	flags = PICK_RW_OP_RET(raw_rwlock_t, read, _lock_irqsave, lock)
+# define write_lock_irqsave(lock, flags) \
+	flags = PICK_RW_OP_RET(raw_rwlock_t, write, _lock_irqsave, lock)
+
+// #define spin_lock_irq(lock)	_spin_lock_irq(lock)
+// #define spin_lock_bh(lock)	_spin_lock_bh(lock)
+#define spin_lock_irq(lock)	PICK_OP(raw_spinlock_t, spin, _lock_irq, lock)
+#define spin_lock_bh(lock)	PICK_OP(raw_spinlock_t, spin, _lock_bh, lock)
+
+// #define read_lock_irq(lock)	_read_lock_irq(lock)
+// #define read_lock_bh(lock)	_read_lock_bh(lock)
+#define read_lock_irq(lock)	PICK_RW_OP(raw_rwlock_t, read, _lock_irq, lock)
+#define read_lock_bh(lock)	PICK_RW_OP(raw_rwlock_t, read, _lock_bh, lock)
+
+// #define write_lock_irq(lock)		_write_lock_irq(lock)
+// #define write_lock_bh(lock)		_write_lock_bh(lock)
+#define write_lock_irq(lock)	PICK_RW_OP(raw_rwlock_t, write, _lock_irq, lock)
+#define write_lock_bh(lock)	PICK_RW_OP(raw_rwlock_t, write, _lock_bh, lock)
+
+// #define spin_unlock(lock)	_spin_unlock(lock)
+// #define write_unlock(lock)	_write_unlock(lock)
+// #define read_unlock(lock)	_read_unlock(lock)
+#define spin_unlock(lock)	PICK_OP(raw_spinlock_t, spin, _unlock, lock)
+#define read_unlock(lock)	PICK_RW_OP(raw_rwlock_t, read, _unlock, lock)
+#define write_unlock(lock)	PICK_RW_OP(raw_rwlock_t, write, _unlock, lock)
+
+//#define spin_unlock_irqrestore(lock, flags)
+//		_spin_unlock_irqrestore(lock, flags)
+//#define spin_unlock_irq(lock)	_spin_unlock_irq(lock)
+//#define spin_unlock_bh(lock)	_spin_unlock_bh(lock)
+#define spin_unlock_irqrestore(lock, flags) \
+	PICK_OP2(raw_spinlock_t, spin, _unlock_irqrestore, lock, flags)
+#define spin_unlock_irq(lock)	PICK_OP(raw_spinlock_t, spin, _unlock_irq, lock)
+#define spin_unlock_bh(lock)	PICK_OP(raw_spinlock_t, spin, _unlock_bh, lock)
+
+// #define read_unlock_irqrestore(lock, flags)
+// 		_read_unlock_irqrestore(lock, flags)
+// #define read_unlock_irq(lock)	_read_unlock_irq(lock)
+// #define read_unlock_bh(lock)	_read_unlock_bh(lock)
+#define read_unlock_irqrestore(lock, flags) \
+		PICK_RW_OP2(raw_rwlock_t, read, _unlock_irqrestore, lock, flags)
+#define read_unlock_irq(lock) PICK_RW_OP(raw_rwlock_t, read, _unlock_irq, lock)
+#define read_unlock_bh(lock) PICK_RW_OP(raw_rwlock_t, read, _unlock_bh, lock)
+
+// #define write_unlock_irqrestore(lock, flags)
+// 	_write_unlock_irqrestore(lock, flags)
+// #define write_unlock_irq(lock)			_write_unlock_irq(lock)
+// #define write_unlock_bh(lock)			_write_unlock_bh(lock)
+#define write_unlock_irqrestore(lock, flags) \
+	PICK_RW_OP2(raw_rwlock_t, write, _unlock_irqrestore, lock, flags)
+#define write_unlock_irq(lock) PICK_RW_OP(raw_rwlock_t, write, _unlock_irq, lock)
+#define write_unlock_bh(lock) PICK_RW_OP(raw_rwlock_t, write, _unlock_bh, lock)
+
+// #define spin_trylock_bh(lock)	_spin_trylock_bh(lock)
+#define spin_trylock_bh(lock)	PICK_OP_RET(raw_spinlock_t, spin, _trylock_bh, lock)
 
 /* "lock on reference count zero" */
 #ifndef ATOMIC_DEC_AND_LOCK
-#include <asm/atomic.h>
-extern int atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);
+# include <asm/atomic.h>
+  extern int _atomic_dec_and_spin_lock(atomic_t *atomic, raw_spinlock_t *lock);
 #endif
 
+#define atomic_dec_and_lock(atomic, lock)				\
+({									\
+	int __ret;							\
+									\
+	if (TYPE_EQUAL(lock, raw_spinlock_t))				\
+		__ret = _atomic_dec_and_spin_lock(atomic,		\
+					(raw_spinlock_t *)(lock));	\
+	else if (TYPE_EQUAL(lock, _mutex_t))				\
+		__ret = atomic_dec_and_mutex_lock(atomic,		\
+					(_mutex_t *)(lock));		\
+	else __ret = __bad_spinlock_type();				\
+									\
+	__ret;								\
+})
+
+
 /*
  *  bit-based spin_lock()
  *
@@ -497,15 +687,10 @@
 	 * busywait with less bus contention for a good time to
 	 * attempt to acquire the lock bit.
 	 */
-	preempt_disable();
-#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
-	while (test_and_set_bit(bitnum, addr)) {
-		while (test_bit(bitnum, addr)) {
-			preempt_enable();
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK) || defined(CONFIG_PREEMPT)
+	while (test_and_set_bit(bitnum, addr))
+		while (test_bit(bitnum, addr))
 			cpu_relax();
-			preempt_disable();
-		}
-	}
 #endif
 }
 
@@ -514,16 +699,12 @@
  */
 static inline int bit_spin_trylock(int bitnum, unsigned long *addr)
 {
-#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK) || defined(CONFIG_PREEMPT)
 	int ret;
 
-	preempt_disable();
 	ret = !test_and_set_bit(bitnum, addr);
-	if (!ret)
-		preempt_enable();
 	return ret;
 #else
-	preempt_disable();
 	return 1;
 #endif
 }
@@ -533,12 +714,11 @@
  */
 static inline void bit_spin_unlock(int bitnum, unsigned long *addr)
 {
-#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK) || defined(CONFIG_PREEMPT)
 	BUG_ON(!test_bit(bitnum, addr));
 	smp_mb__before_clear_bit();
 	clear_bit(bitnum, addr);
 #endif
-	preempt_enable();
 }
 
 /*
@@ -546,13 +726,36 @@
  */
 static inline int bit_spin_is_locked(int bitnum, unsigned long *addr)
 {
-#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK) || defined(CONFIG_PREEMPT)
 	return test_bit(bitnum, addr);
-#elif defined CONFIG_PREEMPT
-	return preempt_count();
 #else
 	return 1;
 #endif
 }
 
+#define DECLARE_SPINLOCK(name) \
+	spinlock_t name __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED
+
+#define DECLARE_RAW_SPINLOCK(name) \
+	raw_spinlock_t name __cacheline_aligned_in_smp = RAW_SPIN_LOCK_UNLOCKED
+
+//#define DECLARE_RWLOCK(name)
+//	rwlock_t name __cacheline_aligned_in_smp = RW_LOCK_UNLOCKED
+
+#define DECLARE_RAW_RWLOCK(name) \
+	raw_rwlock_t name __cacheline_aligned_in_smp = RAW_RW_LOCK_UNLOCKED
+
+#ifdef CONFIG_PREEMPT_REALTIME
+# define spin_lock_realirq(lock) \
+	do { spin_lock(lock); local_irq_disable(); } while (0)
+# define spin_lock_realirq_off(lock) \
+	do { local_irq_enable(); spin_lock(lock); \
+				local_irq_disable(); } while (0)
+# define spin_unlock_realirq(lock) \
+	do { spin_unlock(lock); local_irq_enable(); } while (0)
+#else
+# define spin_lock_realirq(lock) spin_lock_irq(lock)
+# define spin_lock_realirq_off(lock) spin_lock_irq(lock)
+# define spin_unlock_realirq(lock) spin_unlock_irq(lock)
+#endif
 #endif /* __LINUX_SPINLOCK_H */
Index: xx-sources/include/linux/swap.h
===================================================================
--- xx-sources.orig/include/linux/swap.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/linux/swap.h	2004-10-16 20:57:00.000000000 -0400
@@ -193,7 +193,7 @@
 extern struct address_space swapper_space;
 #define total_swapcache_pages  swapper_space.nrpages
 extern void show_swap_cache_info(void);
-extern int add_to_swap(struct page *);
+extern int add_to_swap(struct page *page, void *cookie, pgoff_t index);
 extern void __delete_from_swap_cache(struct page *);
 extern void delete_from_swap_cache(struct page *);
 extern int move_to_swap_cache(struct page *, swp_entry_t);
@@ -209,7 +209,7 @@
 extern unsigned int nr_swapfiles;
 extern struct swap_info_struct swap_info[];
 extern void si_swapinfo(struct sysinfo *);
-extern swp_entry_t get_swap_page(void);
+extern swp_entry_t get_swap_page(void *cookie, pgoff_t index);
 extern int swap_duplicate(swp_entry_t);
 extern int valid_swaphandles(swp_entry_t, unsigned long *);
 extern void swap_free(swp_entry_t);
@@ -275,7 +275,7 @@
 	return 0;
 }
 
-static inline swp_entry_t get_swap_page(void)
+static inline swp_entry_t get_swap_page(void *cookie, pgoff_t index)
 {
 	swp_entry_t entry;
 	entry.val = 0;
Index: xx-sources/include/linux/time.h
===================================================================
--- xx-sources.orig/include/linux/time.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/time.h	2004-10-16 20:57:00.000000000 -0400
@@ -80,7 +80,7 @@
 
 extern struct timespec xtime;
 extern struct timespec wall_to_monotonic;
-extern seqlock_t xtime_lock;
+extern raw_seqlock_t xtime_lock;
 
 static inline unsigned long get_seconds(void)
 { 
Index: xx-sources/include/linux/timer.h
===================================================================
--- xx-sources.orig/include/linux/timer.h	2004-10-16 20:52:33.000000000 -0400
+++ xx-sources/include/linux/timer.h	2004-10-16 20:57:00.000000000 -0400
@@ -12,7 +12,7 @@
 	struct list_head entry;
 	unsigned long expires;
 
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	unsigned long magic;
 
 	void (*function)(unsigned long);
@@ -29,7 +29,7 @@
 		.data = (_data),				\
 		.base = NULL,					\
 		.magic = TIMER_MAGIC,				\
-		.lock = SPIN_LOCK_UNLOCKED,			\
+		.lock = RAW_SPIN_LOCK_UNLOCKED,			\
 	}
 
 /***
Index: xx-sources/include/linux/wait.h
===================================================================
--- xx-sources.orig/include/linux/wait.h	2004-10-16 20:52:34.000000000 -0400
+++ xx-sources/include/linux/wait.h	2004-10-16 20:57:00.000000000 -0400
@@ -48,11 +48,13 @@
 	wait_queue_t wait;
 };
 
+#if 0
 struct __wait_queue_head {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct list_head task_list;
 };
 typedef struct __wait_queue_head wait_queue_head_t;
+#endif
 
 
 /*
@@ -68,7 +70,7 @@
 	wait_queue_t name = __WAITQUEUE_INITIALIZER(name, tsk)
 
 #define __WAIT_QUEUE_HEAD_INITIALIZER(name) {				\
-	.lock		= SPIN_LOCK_UNLOCKED,				\
+	.lock		= RAW_SPIN_LOCK_UNLOCKED,			\
 	.task_list	= { &(name).task_list, &(name).task_list } }
 
 #define DECLARE_WAIT_QUEUE_HEAD(name) \
@@ -79,7 +81,7 @@
 
 static inline void init_waitqueue_head(wait_queue_head_t *q)
 {
-	q->lock = SPIN_LOCK_UNLOCKED;
+	q->lock = RAW_SPIN_LOCK_UNLOCKED;
 	INIT_LIST_HEAD(&q->task_list);
 }
 
Index: xx-sources/include/net/protocol.h
===================================================================
--- xx-sources.orig/include/net/protocol.h	2004-10-16 20:52:28.000000000 -0400
+++ xx-sources/include/net/protocol.h	2004-10-16 20:57:00.000000000 -0400
@@ -79,6 +79,7 @@
 
 extern struct net_protocol *inet_protocol_base;
 extern struct net_protocol *inet_protos[MAX_INET_PROTOS];
+extern spinlock_t inet_proto_lock;
 
 #if defined(CONFIG_IPV6) || defined (CONFIG_IPV6_MODULE)
 extern struct inet6_protocol *inet6_protos[MAX_INET_PROTOS];
Index: xx-sources/include/net/sock.h
===================================================================
--- xx-sources.orig/include/net/sock.h	2004-10-16 20:52:31.000000000 -0400
+++ xx-sources/include/net/sock.h	2004-10-16 20:57:00.000000000 -0400
@@ -592,12 +592,12 @@
 /* Called with local bh disabled */
 static __inline__ void sock_prot_inc_use(struct proto *prot)
 {
-	prot->stats[smp_processor_id()].inuse++;
+	prot->stats[_smp_processor_id()].inuse++;
 }
 
 static __inline__ void sock_prot_dec_use(struct proto *prot)
 {
-	prot->stats[smp_processor_id()].inuse--;
+	prot->stats[_smp_processor_id()].inuse--;
 }
 
 /* About 10 seconds */
Index: xx-sources/init/main.c
===================================================================
--- xx-sources.orig/init/main.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/init/main.c	2004-10-16 20:57:00.000000000 -0400
@@ -45,6 +45,7 @@
 #include <linux/efi.h>
 #include <linux/unistd.h>
 #include <linux/rmap.h>
+#include <linux/irq.h>
 #include <linux/mempolicy.h>
 
 #include <asm/io.h>
@@ -442,6 +443,8 @@
 
 static void noinline rest_init(void)
 {
+	system_state = SYSTEM_BOOTING_SCHEDULER_OK;
+
 	kernel_thread(init, NULL, CLONE_FS | CLONE_SIGHAND);
 	numa_default_policy();
 	/*
@@ -519,6 +522,7 @@
 	preempt_disable();
 	build_all_zonelists();
 	page_alloc_init();
+	early_init_hardirqs();
 	trap_init();
 	printk("Kernel command line: %s\n", saved_command_line);
 	parse_early_param();
@@ -726,6 +730,8 @@
 
 	cpuset_init_smp();
 
+	init_hardirqs();
+
 	/*
 	 * Do this before initcalls, because some drivers want to access
 	 * firmware files.
@@ -750,6 +756,9 @@
 	 */
 	free_initmem();
 	unlock_kernel();
+#ifdef CONFIG_PREEMPT_TIMING
+	preempt_max_latency = 0;
+#endif
 	system_state = SYSTEM_RUNNING;
 	numa_default_policy();
 
Index: xx-sources/ipc/util.c
===================================================================
--- xx-sources.orig/ipc/util.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/ipc/util.c	2004-10-16 20:57:01.000000000 -0400
@@ -191,7 +191,7 @@
 	if(ids->seq > ids->seq_max)
 		ids->seq = 0;
 
-	new->lock = SPIN_LOCK_UNLOCKED;
+	new->lock = RAW_SPIN_LOCK_UNLOCKED;
 	new->deleted = 0;
 	rcu_read_lock();
 	spin_lock(&new->lock);
Index: xx-sources/kernel/Makefile
===================================================================
--- xx-sources.orig/kernel/Makefile	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/Makefile	2004-10-16 22:00:09.764775192 -0400
@@ -15,7 +15,11 @@
 	    sysctl.o capability.o ptrace.o timer.o user.o \
 	    signal.o sys.o kmod.o workqueue.o pid.o \
 	    rcupdate.o intermodule.o extable.o params.o posix-timers.o \
-	    kthread.o wait.o kfifo.o
+	    kthread.o wait.o kfifo.o mutex.o
+
+obj-$(CONFIG_DEBUG_PREEMPT) += latency.o
+obj-$(CONFIG_PREEMPT_TIMING) += latency.o
+obj-$(CONFIG_TRACE_CRITICAL) += latency.o
 
 obj-$(CONFIG_FUTEX) += futex.o
 obj-$(CONFIG_GENERIC_ISA_DMA) += dma.o
Index: xx-sources/kernel/default-sched.c
===================================================================
--- xx-sources.orig/kernel/default-sched.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/default-sched.c	2004-10-16 22:00:12.430369960 -0400
@@ -33,6 +33,7 @@
 #include <linux/seq_file.h>
 #include <linux/syscalls.h>
 #include <linux/times.h>
+#include <linux/kallsyms.h>
 
 #include <asm/tlb.h>
 
@@ -196,7 +197,7 @@
  * acquire operations must be ordered by ascending &runqueue.
  */
 struct runqueue {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
@@ -1315,6 +1316,7 @@
 	 */
 	prev_task_flags = prev->flags;
 	finish_arch_switch(rq, prev);
+	preempt_enable_no_resched();
 	if (mm)
 		mmdrop(mm);
 	if (unlikely(prev_task_flags & PF_DEAD))
@@ -2455,16 +2457,20 @@
 	unsigned long run_time;
 	int cpu, idx;
 
+	WARN_ON(system_state == SYSTEM_BOOTING);
 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
 	 * schedule() atomically, we ignore that path for now.
 	 * Otherwise, whine if we are scheduling when we should not be.
 	 */
-	if (likely(!(current->state & (TASK_DEAD | TASK_ZOMBIE)))) {
+	if (likely(!(current->exit_state & (__TASK_DEAD | __TASK_ZOMBIE)))) {
 		if (unlikely(in_atomic())) {
+			stop_trace();
 			printk(KERN_ERR "scheduling while atomic: "
 				"%s/0x%08x/%d\n",
 				current->comm, preempt_count(), current->pid);
+			print_symbol("caller is %s\n",
+				(long)__builtin_return_address(0));
 			dump_stack();
 		}
 	}
@@ -2501,6 +2507,8 @@
 
 	spin_lock_irq(&rq->lock);
 
+	if (unlikely(current->flags & PF_DEAD))
+		current->state = __TASK_DEAD;
 	/*
 	 * if entering off of a kernel preemption go straight
 	 * to picking the next task.
@@ -2600,12 +2608,14 @@
 		prev = context_switch(rq, prev, next);
 		barrier();
 
+		// re-enables preemption
 		finish_task_switch(prev);
-	} else
+	} else {
 		spin_unlock_irq(&rq->lock);
+		preempt_enable_no_resched();
+	}
 
 	reacquire_kernel_sem(current);
-	preempt_enable_no_resched();
 	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
 		goto need_resched;
 }
@@ -3498,7 +3508,7 @@
 	struct runqueue *rq = cpu_rq(dead_cpu);
 
 	/* Must be exiting, otherwise would be on tasklist. */
-	BUG_ON(tsk->state != TASK_ZOMBIE && tsk->state != TASK_DEAD);
+	BUG_ON(tsk->exit_state != __TASK_ZOMBIE && tsk->exit_state != __TASK_DEAD);
 
 	/* Cannot have done final schedule yet: would have vanished. */
 	BUG_ON(tsk->flags & PF_DEAD);
Index: xx-sources/kernel/dma.c
===================================================================
--- xx-sources.orig/kernel/dma.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/dma.c	2004-10-16 20:57:01.000000000 -0400
@@ -38,7 +38,7 @@
  */
 
 
-spinlock_t dma_spin_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(dma_spin_lock);
 
 /*
  *	If our port doesn't define this it has no PC like DMA
Index: xx-sources/kernel/exit.c
===================================================================
--- xx-sources.orig/kernel/exit.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/exit.c	2004-10-16 20:57:01.000000000 -0400
@@ -47,8 +47,11 @@
 	if (thread_group_leader(p)) {
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
-		if (p->pid)
+		if (p->pid) {
+			preempt_disable();
 			__get_cpu_var(process_counts)--;
+			preempt_enable();
+		}
 	}
 
 	REMOVE_LINKS(p);
@@ -79,7 +82,7 @@
 	 */
 	zap_leader = 0;
 	leader = p->group_leader;
-	if (leader != p && thread_group_empty(leader) && leader->state == TASK_ZOMBIE) {
+	if (leader != p && thread_group_empty(leader) && leader->exit_state == __TASK_ZOMBIE) {
 		BUG_ON(leader->exit_signal == -1);
 		do_notify_parent(leader, leader->exit_signal);
 		/*
@@ -164,7 +167,7 @@
 
 	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
 		if (p == ignored_task
-				|| p->state >= TASK_ZOMBIE 
+				|| p->exit_state >= __TASK_ZOMBIE 
 				|| p->real_parent->pid == 1)
 			continue;
 		if (process_group(p->real_parent) != pgrp
@@ -374,8 +377,10 @@
 		while (set) {
 			if (set & 1) {
 				struct file * file = xchg(&files->fd[i], NULL);
-				if (file)
+				if (file) {
 					filp_close(file, files);
+					cond_resched();
+				}
 			}
 			i++;
 			set >>= 1;
@@ -523,7 +528,7 @@
 	 * Make sure we're not reparenting to ourselves and that
 	 * the parent is not a zombie.
 	 */
-	BUG_ON(p == reaper || reaper->state >= TASK_ZOMBIE);
+	BUG_ON(p == reaper || reaper->state >= __TASK_ZOMBIE || reaper->exit_state >= __TASK_ZOMBIE);
 	p->real_parent = reaper;
 	if (p->parent == p->real_parent)
 		BUG();
@@ -558,7 +563,7 @@
 		/* If we'd notified the old parent about this child's death,
 		 * also notify the new parent.
 		 */
-		if (p->state == TASK_ZOMBIE && p->exit_signal != -1 &&
+		if (p->exit_state == __TASK_ZOMBIE && p->exit_signal != -1 &&
 		    thread_group_empty(p))
 			do_notify_parent(p, p->exit_signal);
 		else if (p->state == TASK_TRACED) {
@@ -606,7 +611,7 @@
 			reaper = child_reaper;
 			break;
 		}
-	} while (reaper->state >= TASK_ZOMBIE);
+	} while (reaper->exit_state >= __TASK_ZOMBIE);
 
 	/*
 	 * There are only two places where our children can be:
@@ -632,7 +637,7 @@
 		} else {
 			/* reparent ptraced task to its real parent */
 			__ptrace_unlink (p);
-			if (p->state == TASK_ZOMBIE && p->exit_signal != -1 &&
+			if (p->exit_state == __TASK_ZOMBIE && p->exit_signal != -1 &&
 			    thread_group_empty(p))
 				do_notify_parent(p, p->exit_signal);
 		}
@@ -643,7 +648,7 @@
 		 * zombie forever since we prevented it from self-reap itself
 		 * while it was being traced by us, to be able to see it in wait4.
 		 */
-		if (unlikely(ptrace && p->state == TASK_ZOMBIE && p->exit_signal == -1))
+		if (unlikely(ptrace && p->exit_state == __TASK_ZOMBIE && p->exit_signal == -1))
 			list_add(&p->ptrace_list, to_release);
 	}
 	list_for_each_safe(_p, _n, &father->ptrace_children) {
@@ -756,10 +761,10 @@
 		do_notify_parent(tsk, SIGCHLD);
 	}
 
-	state = TASK_ZOMBIE;
+	state = __TASK_ZOMBIE;
 	if (tsk->exit_signal == -1 && tsk->ptrace == 0)
-		state = TASK_DEAD;
-	tsk->state = state;
+		state = __TASK_DEAD;
+	tsk->exit_state = state;
 
 	/*
 	 * Clear these here so that update_process_times() won't try to deliver
@@ -777,7 +782,7 @@
 	}
 
 	/* If the process is dead, release it - nobody will wait for it */
-	if (state == TASK_DEAD)
+	if (state == __TASK_DEAD)
 		release_task(tsk);
 
 	/* PF_DEAD causes final put_task_struct after we schedule. */
@@ -836,6 +841,7 @@
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
+	BUG_ON(!(current->flags & PF_DEAD));
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */
@@ -863,8 +869,19 @@
 	if (!p->sighand)
 		BUG();
 	if (!spin_is_locked(&p->sighand->siglock) &&
-				!rwlock_is_locked(&tasklist_lock))
+				!rwlock_is_locked(&tasklist_lock)) {
+#ifdef CONFIG_PREEMPT_REALTIME
+		printk("hm #1, siglock: %d. tasklist_lock: %d.\n",
+			atomic_read(&p->sighand->siglock.lock.count),
+			tasklist_lock.lock.activity);
+		spin_lock(&tasklist_lock.lock.wait_lock);
+		spin_unlock(&tasklist_lock.lock.wait_lock);
+		printk("hm #2, siglock: %d. tasklist_lock: %d.\n",
+			atomic_read(&p->sighand->siglock.lock.count),
+			tasklist_lock.lock.activity);
+#endif
 		BUG();
+	}
 #endif
 	return pid_task(p->pids[PIDTYPE_TGID].pid_list.next, PIDTYPE_TGID);
 }
@@ -979,7 +996,7 @@
 }
 
 /*
- * Handle sys_wait4 work for one task in state TASK_ZOMBIE.  We hold
+ * Handle sys_wait4 work for one task in state __TASK_ZOMBIE.  We hold
  * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
@@ -998,7 +1015,7 @@
 		int exit_code = p->exit_code;
 		int why, status;
 
-		if (unlikely(p->state != TASK_ZOMBIE))
+		if (unlikely(p->exit_state != __TASK_ZOMBIE))
 			return 0;
 		if (unlikely(p->exit_signal == -1 && p->ptrace == 0))
 			return 0;
@@ -1019,9 +1036,9 @@
 	 * Try to move the task's state to DEAD
 	 * only one thread is allowed to do this:
 	 */
-	state = xchg(&p->state, TASK_DEAD);
-	if (state != TASK_ZOMBIE) {
-		BUG_ON(state != TASK_DEAD);
+	state = xchg(&p->exit_state, __TASK_DEAD);
+	if (state != __TASK_ZOMBIE) {
+		BUG_ON(state != __TASK_DEAD);
 		return 0;
 	}
 	if (unlikely(p->exit_signal == -1 && p->ptrace == 0)) {
@@ -1066,7 +1083,7 @@
 
 	/*
 	 * Now we are sure this task is interesting, and no other
-	 * thread can reap it because we set its state to TASK_DEAD.
+	 * thread can reap it because we set its state to __TASK_DEAD.
 	 */
 	read_unlock(&tasklist_lock);
 
@@ -1098,7 +1115,8 @@
 	if (!retval && infop)
 		retval = put_user(p->uid, &infop->si_uid);
 	if (retval) {
-		p->state = TASK_ZOMBIE;
+		// FIXME: is this safe?
+		p->exit_state = __TASK_ZOMBIE;
 		return retval;
 	}
 	retval = p->pid;
@@ -1107,7 +1125,8 @@
 		/* Double-check with lock held.  */
 		if (p->real_parent != p->parent) {
 			__ptrace_unlink(p);
-			p->state = TASK_ZOMBIE;
+			// FIXME: is this safe?
+			p->exit_state = __TASK_ZOMBIE;
 			/*
 			 * If this is not a detached task, notify the parent.
 			 * If it's still not detached after that, don't release
@@ -1178,13 +1197,13 @@
 	/*
 	 * This uses xchg to be atomic with the thread resuming and setting
 	 * it.  It must also be done with the write lock held to prevent a
-	 * race with the TASK_ZOMBIE case.
+	 * race with the __TASK_ZOMBIE case.
 	 */
 	exit_code = xchg(&p->exit_code, 0);
-	if (unlikely(p->state >= TASK_ZOMBIE)) {
+	if (unlikely(p->exit_state >= __TASK_ZOMBIE)) {
 		/*
 		 * The task resumed and then died.  Let the next iteration
-		 * catch it in TASK_ZOMBIE.  Note that exit_code might
+		 * catch it in __TASK_ZOMBIE.  Note that exit_code might
 		 * already be zero here if it resumed and did _exit(0).
 		 * The task itself is dead and won't touch exit_code again;
 		 * other processors in this function are locked out.
@@ -1345,23 +1364,28 @@
 				if (retval != 0) /* He released the lock.  */
 					goto end;
 				break;
-			case TASK_ZOMBIE:
-				/*
-				 * Eligible but we cannot release it yet:
-				 */
-				if (ret == 2)
-					goto check_continued;
-				if (!likely(options & WEXITED))
-					continue;
-				retval = wait_task_zombie(
-					p, (options & WNOWAIT),
-					infop, stat_addr, ru);
-				if (retval != 0) /* He released the lock.  */
-					goto end;
-				break;
-			case TASK_DEAD:
-				continue;
 			default:
+			// case __TASK_DEAD:
+				if (p->exit_state == __TASK_DEAD)
+					continue;
+			// case __TASK_ZOMBIE:
+				if (p->exit_state == __TASK_ZOMBIE) {
+					/*
+					 * Eligible but we cannot release
+					 * it yet:
+					 */
+					if (ret == 2)
+						goto check_continued;
+					if (!likely(options & WEXITED))
+						continue;
+					retval = wait_task_zombie(
+						p, (options & WNOWAIT),
+						infop, stat_addr, ru);
+					/* He released the lock.  */
+					if (retval != 0)
+						goto end;
+					break;
+				}
 check_continued:
 				if (!unlikely(options & WCONTINUED))
 					continue;
Index: xx-sources/kernel/fork.c
===================================================================
--- xx-sources.orig/kernel/fork.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/fork.c	2004-10-16 20:57:01.000000000 -0400
@@ -88,7 +88,7 @@
 
 void __put_task_struct(struct task_struct *tsk)
 {
-	WARN_ON(!(tsk->state & (TASK_DEAD | TASK_ZOMBIE)));
+	WARN_ON(!(tsk->exit_state & (__TASK_DEAD | __TASK_ZOMBIE)));
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
@@ -283,7 +283,7 @@
 #define mm_free_pgd(mm)
 #endif /* CONFIG_MMU */
 
-spinlock_t mmlist_lock __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(mmlist_lock);
 
 #define allocate_mm()	(kmem_cache_alloc(mm_cachep, SLAB_KERNEL))
 #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
@@ -927,6 +927,7 @@
 	/* ok, now we should be set up.. */
 	p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
 	p->pdeath_signal = 0;
+	p->exit_state = 0;
 
 	/* Perform scheduler related setup */
 	sched_fork(p);
@@ -949,8 +950,10 @@
 	 * another CPU - so we re-copy it here and set the child's CPU to
 	 * the parent's CPU. This avoids alot of nasty races.
 	 */
+	preempt_disable();
 	p->cpus_allowed = current->cpus_allowed;
 	set_task_cpu(p, smp_processor_id());
+	preempt_enable();
 
 	/*
 	 * Check for pending SIGKILL! The new thread should not be allowed
@@ -1008,8 +1011,11 @@
 	if (thread_group_leader(p)) {
 		attach_pid(p, PIDTYPE_PGID, process_group(p));
 		attach_pid(p, PIDTYPE_SID, p->signal->session);
-		if (p->pid)
+		if (p->pid) {
+			preempt_disable();
 			__get_cpu_var(process_counts)++;
+			preempt_enable();
+		}
 	}
 
 	nr_threads++;
Index: xx-sources/kernel/irq/handle.c
===================================================================
--- xx-sources.orig/kernel/irq/handle.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/irq/handle.c	2004-10-16 20:57:01.000000000 -0400
@@ -31,7 +31,7 @@
 irq_desc_t irq_desc[NR_IRQS] __cacheline_aligned = {
 	[0 ... NR_IRQS-1] = {
 		.handler = &no_irq_type,
-		.lock = SPIN_LOCK_UNLOCKED
+		.lock = RAW_SPIN_LOCK_UNLOCKED
 	}
 };
 
@@ -83,6 +83,21 @@
 	preempt_enable_no_resched();
 }
 
+int redirect_hardirq(struct irq_desc *desc)
+{
+	/*
+	 * Direct execution:
+	 */
+	if (!hardirq_preemption || (desc->status & IRQ_NODELAY))
+		return 0;
+
+	BUG_ON(!irqs_disabled());
+	if (desc->thread && desc->thread->state != TASK_RUNNING)
+		wake_up_process(desc->thread);
+
+	return 1;
+}
+
 /*
  * Have got an event to handle:
  */
@@ -91,7 +106,11 @@
 {
 	int ret, retval = 0, status = 0;
 
-	if (!(action->flags & SA_INTERRUPT))
+	/*
+	 * Unconditionally enable interrupts for threaded
+	 * IRQ handlers:
+	 */
+	if (!hardirq_count() || !(action->flags & SA_INTERRUPT))
 		local_irq_enable();
 
 	do {
@@ -114,7 +133,7 @@
  * SMP cross-CPU interrupts have their own specific
  * handlers).
  */
-asmlinkage unsigned int __do_IRQ(unsigned int irq, struct pt_regs *regs)
+asmlinkage notrace unsigned int __do_IRQ(unsigned int irq, struct pt_regs *regs)
 {
 	irq_desc_t *desc = irq_desc + irq;
 	struct irqaction * action;
@@ -166,6 +185,12 @@
 		goto out;
 
 	/*
+	 * hardirq redirection to the irqd process context:
+	 */
+	if (redirect_hardirq(desc))
+		goto out_no_end;
+
+	/*
 	 * Edge triggered interrupts need to remember
 	 * pending events.
 	 * This applies to any hw interrupts that allow a second
@@ -197,6 +222,7 @@
 	 * disabled while the handler was running.
 	 */
 	desc->handler->end(irq);
+out_no_end:
 	spin_unlock(&desc->lock);
 
 	return 1;
Index: xx-sources/kernel/irq/internals.h
===================================================================
--- xx-sources.orig/kernel/irq/internals.h	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/irq/internals.h	2004-10-16 20:57:01.000000000 -0400
@@ -4,6 +4,8 @@
 
 extern int noirqdebug;
 
+void recalculate_desc_flags(struct irq_desc *desc);
+
 #ifdef CONFIG_PROC_FS
 extern void register_irq_proc(unsigned int irq);
 extern void register_handler_proc(unsigned int irq, struct irqaction *action);
Index: xx-sources/kernel/irq/manage.c
===================================================================
--- xx-sources.orig/kernel/irq/manage.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/irq/manage.c	2004-10-16 20:57:01.000000000 -0400
@@ -7,8 +7,9 @@
  */
 
 #include <linux/irq.h>
-#include <linux/module.h>
 #include <linux/random.h>
+#include <linux/module.h>
+#include <linux/kthread.h>
 #include <linux/interrupt.h>
 
 #include "internals.h"
@@ -28,8 +29,12 @@
 {
 	struct irq_desc *desc = irq_desc + irq;
 
-	while (desc->status & IRQ_INPROGRESS)
-		cpu_relax();
+	if (hardirq_preemption && !(desc->status & IRQ_NODELAY))
+		wait_event(desc->wait_for_handler,
+			!(desc->status & IRQ_INPROGRESS));
+	else
+		while (desc->status & IRQ_INPROGRESS)
+			cpu_relax();
 }
 
 EXPORT_SYMBOL(synchronize_irq);
@@ -145,6 +150,21 @@
 }
 
 /*
+ * If any action has SA_NODELAY then turn IRQ_NODELAY on:
+ */
+void recalculate_desc_flags(struct irq_desc *desc)
+{
+	struct irqaction *action;
+
+	desc->status &= ~IRQ_NODELAY;
+	for (action = desc->action ; action; action = action->next)
+		if (action->flags & SA_NODELAY)
+			desc->status |= IRQ_NODELAY;
+}
+
+static int start_irq_thread(int irq, struct irq_desc *desc);
+
+/*
  * Internal function to register an irqaction - typically used to
  * allocate special interrupts that are part of the architecture.
  */
@@ -174,6 +194,9 @@
 		rand_initialize_irq(irq);
 	}
 
+	if (!(new->flags & SA_NODELAY))
+		if (start_irq_thread(irq, desc))
+			return -ENOMEM;
 	/*
 	 * The following block of code has to be executed atomically
 	 */
@@ -196,6 +219,11 @@
 
 	*p = new;
 
+	/*
+	 * Propagate any possible SA_NODELAY flag into IRQ_NODELAY:
+	 */
+	recalculate_desc_flags(desc);
+
 	if (!shared) {
 		desc->depth = 0;
 		desc->status &= ~(IRQ_DISABLED | IRQ_AUTODETECT |
@@ -209,7 +237,7 @@
 
 	new->irq = irq;
 	register_irq_proc(irq);
-	new->dir = NULL;
+	new->dir = new->threaded = NULL;
 	register_handler_proc(irq, new);
 
 	return 0;
@@ -260,6 +288,7 @@
 				else
 					desc->handler->disable(irq);
 			}
+			recalculate_desc_flags(desc);
 			spin_unlock_irqrestore(&desc->lock,flags);
 			unregister_handler_proc(irq, action);
 
@@ -345,3 +374,168 @@
 
 EXPORT_SYMBOL(request_irq);
 
+#ifdef CONFIG_PREEMPT_HARDIRQS
+
+int hardirq_preemption = 1;
+
+EXPORT_SYMBOL(hardirq_preemption);
+
+/*
+ * Real-Time Preemption depends on hardirq threading:
+ */
+#ifndef CONFIG_PREEMPT_REALTIME
+
+static int __init hardirq_preempt_setup (char *str)
+{
+	if (!strncmp(str, "off", 3))
+		hardirq_preemption = 0;
+	else
+		get_option(&str, &hardirq_preemption);
+	if (!hardirq_preemption)
+		printk("turning off hardirq preemption!\n");
+
+	return 1;
+}
+
+__setup("hardirq-preempt=", hardirq_preempt_setup);
+
+#endif
+
+static void do_hardirq(struct irq_desc *desc)
+{
+	struct irqaction * action;
+	unsigned int irq = desc - irq_desc;
+
+	local_irq_disable();
+
+	if (desc->status & IRQ_INPROGRESS) {
+		action = desc->action;
+		spin_lock(&desc->lock);
+		for (;;) {
+			irqreturn_t action_ret = 0;
+
+			if (action) {
+				spin_unlock(&desc->lock);
+				action_ret = handle_IRQ_event(irq, NULL,action);
+				cond_resched_all();
+				spin_lock_irq(&desc->lock);
+			}
+			if (!noirqdebug)
+				note_interrupt(irq, desc, action_ret);
+			if (likely(!(desc->status & IRQ_PENDING)))
+				break;
+			desc->status &= ~IRQ_PENDING;
+		}
+		desc->status &= ~IRQ_INPROGRESS;
+//		if (waitqueue_active(&desc->wait_for_handler))
+			wake_up(&desc->wait_for_handler);
+		/*
+		 * The ->end() handler has to deal with interrupts which got
+		 * disabled while the handler was running.
+		 */
+		desc->handler->end(irq);
+		spin_unlock(&desc->lock);
+	}
+
+	local_irq_enable();
+}
+
+extern asmlinkage void __do_softirq(void);
+
+static int do_irqd(void * __desc)
+{
+	struct irq_desc *desc = __desc;
+#ifdef CONFIG_SMP
+	int irq = desc - irq_desc;
+	cpumask_t mask;
+
+	mask = cpumask_of_cpu(any_online_cpu(irq_affinity[irq]));
+	set_cpus_allowed(current, mask);
+#endif
+	current->flags |= PF_NOFREEZE | PF_HARDIRQ;
+
+	set_user_nice(current, -10);
+
+//	printk("IRQ#%d thread started up.\n", irq);
+
+	while (!kthread_should_stop()) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		do_hardirq(desc);
+		cond_resched_all();
+		__do_softirq();
+#ifdef CONFIG_SMP
+		/*
+		 * Did IRQ affinities change?
+		 */
+		if (!cpu_isset(smp_processor_id(), irq_affinity[irq])) {
+			mask = cpumask_of_cpu(any_online_cpu(irq_affinity[irq]));
+			set_cpus_allowed(current, mask);
+		}
+#endif
+		schedule();
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+static int ok_to_create_irq_threads;
+
+static int start_irq_thread(int irq, struct irq_desc *desc)
+{
+	if (desc->thread || !ok_to_create_irq_threads)
+		return 0;
+
+	desc->thread = kthread_create(do_irqd, desc, "IRQ %d", irq);
+	if (!desc->thread) {
+		printk(KERN_ERR "irqd: could not create IRQ thread %d!\n", irq);
+		return -ENOMEM;
+	}
+
+	/*
+	 * An interrupt may have come in before the thread pointer was
+	 * stored in desc->thread; make sure the thread gets woken up in
+	 * such a case:
+	 */
+	smp_mb();
+	
+	if (desc->status & IRQ_INPROGRESS)
+		wake_up_process(desc->thread);
+	
+	return 0;
+}
+
+void __init init_hardirqs(void)
+{	
+	int i;
+	ok_to_create_irq_threads = 1;
+
+	for (i = 0; i < NR_IRQS; i++) {
+		irq_desc_t *desc = irq_desc + i;
+
+		if (desc->action && !(desc->status & IRQ_NODELAY))
+			start_irq_thread(i, desc);
+	}
+}
+
+#else
+
+void __init init_hardirqs(void)
+{
+}
+
+static int start_irq_thread(int irq, struct irq_desc *desc)
+{
+	return 0;
+}
+
+#endif
+
+void __init early_init_hardirqs(void)
+{	
+	int i;
+
+	for (i = 0; i < NR_IRQS; i++)
+		init_waitqueue_head(&irq_desc[i].wait_for_handler);
+}
+
+
Index: xx-sources/kernel/irq/proc.c
===================================================================
--- xx-sources.orig/kernel/irq/proc.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/irq/proc.c	2004-10-16 20:57:01.000000000 -0400
@@ -7,9 +7,12 @@
  */
 
 #include <linux/irq.h>
+#include <asm/uaccess.h>
 #include <linux/proc_fs.h>
 #include <linux/interrupt.h>
 
+#include "internals.h"
+
 static struct proc_dir_entry *root_irq_dir, *irq_dir[NR_IRQS];
 
 #ifdef CONFIG_SMP
@@ -64,24 +67,6 @@
 
 #endif
 
-#define MAX_NAMELEN 128
-
-void register_handler_proc(unsigned int irq, struct irqaction *action)
-{
-	char name [MAX_NAMELEN];
-
-	if (!irq_dir[irq] || action->dir || !action->name)
-		return;
-
-	memset(name, 0, MAX_NAMELEN);
-	snprintf(name, MAX_NAMELEN, "%s", action->name);
-
-	/* create /proc/irq/1234/handler/ */
-	action->dir = proc_mkdir(name, irq_dir[irq]);
-}
-
-#undef MAX_NAMELEN
-
 #define MAX_NAMELEN 10
 
 void register_irq_proc(unsigned int irq)
@@ -121,10 +106,101 @@
 
 void unregister_handler_proc(unsigned int irq, struct irqaction *action)
 {
+	if (action->threaded)
+		remove_proc_entry(action->threaded->name, action->dir);
 	if (action->dir)
 		remove_proc_entry(action->dir->name, irq_dir[irq]);
 }
 
+static int threaded_read_proc(char *page, char **start, off_t off,
+			      int count, int *eof, void *data)
+{
+	return sprintf(page, "%c\n",
+		((struct irqaction *)data)->flags & SA_NODELAY ? '0' : '1');
+}
+
+static int threaded_write_proc(struct file *file, const char __user *buffer,
+			       unsigned long count, void *data)
+{
+	int c;
+	struct irqaction *action = data;
+	irq_desc_t *desc = irq_desc + action->irq;
+
+	if (get_user(c, buffer))
+		return -EFAULT;
+	if (c != '0' && c != '1')
+		return -EINVAL;
+
+	spin_lock_irq(&desc->lock);
+
+	if (c == '0')
+		action->flags |= SA_NODELAY;
+	if (c == '1')
+		action->flags &= ~SA_NODELAY;
+	recalculate_desc_flags(desc);
+
+	spin_unlock_irq(&desc->lock);
+
+	return 1;
+}
+
+#define MAX_NAMELEN 128
+
+static int name_unique(unsigned int irq, struct irqaction *new_action)
+{
+	struct irq_desc *desc = irq_desc + irq;
+	struct irqaction *action;
+
+	for (action = desc->action ; action; action = action->next)
+		if ((action != new_action) && action->name &&
+				!strcmp(new_action->name, action->name))
+			return 0;
+	return 1;
+}
+
+void register_handler_proc(unsigned int irq, struct irqaction *action)
+{
+	char name [MAX_NAMELEN];
+	struct proc_dir_entry *entry;
+
+	if (!irq_dir[irq] || action->dir || !action->name ||
+					!name_unique(irq, action))
+		return;
+
+	memset(name, 0, MAX_NAMELEN);
+	snprintf(name, MAX_NAMELEN, "%s", action->name);
+
+	/* create /proc/irq/1234/handler/ */
+	action->dir = proc_mkdir(name, irq_dir[irq]);
+	if (!action->dir)
+		return;
+#ifdef CONFIG_PREEMPT_REALTIME
+	/*
+	 * Hack: only allow the keyboard IRQ to be non-threaded:
+	 */
+# ifdef CONFIG_X86
+#  if 0
+	if (irq != 1)
+		return;
+#  endif
+# else
+	return;
+# endif
+#endif
+	/* create /proc/irq/1234/handler/threaded */
+	entry = create_proc_entry("threaded", 0600, action->dir);
+	if (!entry)
+		return;
+	entry->nlink = 1;
+	entry->data = (void *)action;
+	entry->read_proc = threaded_read_proc;
+	entry->write_proc = threaded_write_proc;
+	action->threaded = entry;
+}
+
+#undef MAX_NAMELEN
+
+
 void init_irq_proc(void)
 {
 	int i;
Index: xx-sources/kernel/latency.c
===================================================================
--- xx-sources.orig/kernel/latency.c	2004-10-14 14:49:41.073057592 -0400
+++ xx-sources/kernel/latency.c	2004-10-16 20:57:01.000000000 -0400
@@ -0,0 +1,645 @@
+/*
+ *  kernel/latency.c
+ *
+ *  Copyright (C) 2004 Ingo Molnar
+ *  Copyright (C) 2004 William Lee Irwin III
+ */
+
+#include <linux/mm.h>
+#include <linux/nmi.h>
+#include <linux/sched.h>
+#include <linux/percpu.h>
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/profile.h>
+#include <linux/bootmem.h>
+#include <linux/version.h>
+#include <linux/notifier.h>
+#include <linux/kallsyms.h>
+#include <linux/seq_file.h>
+#include <linux/interrupt.h>
+#include <asm/rtc.h>
+
+#ifdef CONFIG_PREEMPT_TIMING
+
+unsigned long preempt_thresh;
+
+/*
+ * Maximum preemption latency measured. Initialize to maximum,
+ * we clear it after bootup.
+ */
+unsigned long preempt_max_latency = ULONG_MAX;
+
+/*
+ * Track maximum latencies and save the trace:
+ */
+static DECLARE_MUTEX(max_mutex);
+
+#ifdef CONFIG_LATENCY_TRACE
+
+#define MAX_TRACE 4000UL
+
+struct trace_entry {
+	unsigned long preempt_count;
+	unsigned long eip;
+	unsigned long parent_eip;
+	cycles_t timestamp;
+};
+
+#endif
+
+struct cpu_trace {
+	atomic_t disabled;
+	unsigned long trace_idx;
+	cycles_t preempt_timestamp;
+	unsigned long critical_start, critical_end;
+	int early_warning;
+
+#ifdef CONFIG_LATENCY_TRACE
+	struct trace_entry trace[MAX_TRACE];
+#endif
+};
+
+static DEFINE_PER_CPU(struct cpu_trace, trace);
+
+static unsigned long notrace cycles_to_usecs(cycles_t delta)
+{
+#ifdef CONFIG_X86
+	do_div(delta, cpu_khz/1000+1);
+#elif defined(CONFIG_PPC)
+	delta = mulhwu(tb_to_us, delta);
+#else
+	#error Implement cycles_to_usecs.
+#endif
+
+	return (unsigned long) delta;
+}
+
+#ifdef CONFIG_LATENCY_TRACE
+
+int trace_enabled = 1;
+
+static struct cpu_trace max_trace;
+static char max_comm[16];
+static pid_t max_pid;
+static unsigned long max_uid;
+static unsigned long max_nice;
+static unsigned long max_policy;
+static unsigned long max_rt_priority;
+
+
+static inline void notrace
+____trace(struct cpu_trace *tr, unsigned long eip, unsigned long parent_eip)
+{
+	struct trace_entry *entry;
+
+	if ((tr->critical_start || (trace_enabled >= 2)) &&
+			(tr->trace_idx < MAX_TRACE)) {
+		entry = tr->trace + tr->trace_idx;
+		entry->eip = eip;
+		entry->parent_eip = parent_eip;
+		entry->timestamp = get_cycles();
+		entry->preempt_count = preempt_count();
+	}
+	tr->trace_idx++;
+	if ((trace_enabled == 3) && (tr->trace_idx >= MAX_TRACE))
+		tr->trace_idx = 0;
+}
+
+void notrace ___trace(unsigned long eip, unsigned long parent_eip)
+{
+	unsigned long flags;
+
+	if (unlikely(trace_enabled <= 0))
+		return;
+
+	local_irq_save(flags);
+	____trace(&__get_cpu_var(trace), eip, parent_eip);
+	local_irq_restore(flags);
+}
+
+/*
+ * Non-inlined function:
+ */
+void notrace __trace(unsigned long eip, unsigned long parent_eip)
+{
+	___trace(eip, parent_eip);
+}
+
+extern void mcount(void);
+
+EXPORT_SYMBOL(mcount);
+
+void notrace __mcount(void)
+{
+	___trace((unsigned long) __builtin_return_address(1),
+		(unsigned long) __builtin_return_address(2));
+}
+
+static void notrace print_name(struct seq_file *m, unsigned long eip)
+{
+	char namebuf[KSYM_NAME_LEN+1];
+	unsigned long size, offset;
+	const char *sym_name;
+	char *modname;
+
+	sym_name = kallsyms_lookup(eip, &size, &offset, &modname, namebuf);
+	if (sym_name)
+		seq_puts(m, sym_name);
+	else
+		seq_printf(m, "<%08lx>", eip);
+}
+
+static void notrace printk_name(unsigned long eip)
+{
+	char namebuf[KSYM_NAME_LEN+1];
+	unsigned long size, offset;
+	const char *sym_name;
+	char *modname;
+
+	sym_name = kallsyms_lookup(eip, &size, &offset, &modname, namebuf);
+	if (sym_name)
+		printk("%s+%#lx/%#lx", sym_name, offset, size);
+	else
+		printk("<%08lx>", eip);
+}
+
+
+static void notrace print_name_offset(struct seq_file *m, unsigned long eip)
+{
+	char namebuf[KSYM_NAME_LEN+1];
+	unsigned long size, offset;
+	const char *sym_name;
+	char *modname;
+
+	sym_name = kallsyms_lookup(eip, &size, &offset, &modname, namebuf);
+	if (sym_name)
+		seq_printf(m, "%s+%#lx/%#lx", sym_name, offset, size);
+	else
+		seq_printf(m, "<%08lx>", eip);
+}
+
+static void * notrace l_start(struct seq_file *m, loff_t *pos)
+{
+	loff_t n = *pos;
+	unsigned long entries;
+
+	down(&max_mutex);
+
+	entries = min(max_trace.trace_idx, MAX_TRACE);
+
+	if (!n) {
+		seq_printf(m, "preemption latency trace v1.0.7 on %s\n", UTS_RELEASE);
+		seq_puts(m, "-------------------------------------------------------\n");
+		seq_printf(m, " latency: %lu us, entries: %lu (%lu)   |   [VP:%d KP:%d SP:%d HP:%d #CPUS:%d]\n",
+			preempt_max_latency, entries, max_trace.trace_idx,
+			voluntary_preemption, kernel_preemption,
+			softirq_preemption, hardirq_preemption,
+			num_online_cpus());
+		seq_puts(m, "    -----------------\n");
+		seq_printf(m, "    | task: %.16s/%d, uid:%ld nice:%ld policy:%ld rt_prio:%ld\n",
+			max_comm, max_pid, max_uid, max_nice,
+			max_policy, max_rt_priority);
+		seq_puts(m, "    -----------------\n");
+		seq_puts(m, " => started at: ");
+		print_name_offset(m, max_trace.critical_start);
+		seq_puts(m, "\n => ended at:   ");
+		print_name_offset(m, max_trace.critical_end);
+		seq_puts(m, "\n=======>\n");
+	}
+	if (n >= entries)
+		return NULL;
+
+	return max_trace.trace + n;
+}
+
+static void * notrace l_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	unsigned long entries = min(max_trace.trace_idx, MAX_TRACE);
+
+	if (++*pos >= entries)
+		return NULL;
+	return max_trace.trace + *pos;
+}
+
+static void notrace l_stop(struct seq_file *m, void *p)
+{
+	up(&max_mutex);
+}
+
+static int notrace l_show(struct seq_file *m, void *p)
+{
+	struct trace_entry *entry, *entry0, *next_entry;
+	unsigned long abs_usecs, rel_usecs, trace_idx;
+
+	entry0 = max_trace.trace;
+	entry = p;
+	trace_idx = entry - entry0;
+
+	if (trace_idx + 1 < max_trace.trace_idx)
+		next_entry = entry + 1;
+	else
+		next_entry = entry;
+
+	abs_usecs = cycles_to_usecs(entry->timestamp - entry0->timestamp);
+	rel_usecs = cycles_to_usecs(next_entry->timestamp - entry->timestamp);
+
+        seq_printf(m, "%08lx %ld.%03ldms (+%ld.%03ldms): ",
+		entry->preempt_count, abs_usecs/1000, abs_usecs % 1000,
+			rel_usecs/1000, rel_usecs % 1000);
+
+	print_name(m, entry->eip);
+	seq_puts(m, " (");
+	print_name(m, entry->parent_eip);
+	seq_puts(m, ")\n");
+
+	return 0;
+}
+
+struct seq_operations latency_trace_op = {
+	.start	= l_start,
+	.next	= l_next,
+	.stop	= l_stop,
+	.show	= l_show
+};
+
+static void update_max_trace(struct cpu_trace *tr)
+{
+	memcpy(&max_trace, tr, sizeof (max_trace));
+
+	memcpy(max_comm, current->comm, 16);
+	max_pid = current->pid;
+	max_uid = current->uid;
+	max_nice = current->static_prio - 20 - MAX_RT_PRIO;
+	max_policy = current->policy;
+	max_rt_priority = current->rt_priority;
+}
+
+#endif
+
+static int setup_preempt_thresh(char *s)
+{
+	int thresh;
+
+	get_option(&s, &thresh);
+	if (thresh > 0) {
+		preempt_thresh = thresh;
+		printk("Preemption threshold = %lu us\n", preempt_thresh);
+	}
+	return 1;
+}
+__setup("preempt_thresh=", setup_preempt_thresh);
+
+void notrace
+debug_preempt_timing(struct cpu_trace *tr, unsigned long parent_eip)
+{
+	unsigned long latency, t0, t1;
+
+	if (tr->early_warning)
+		return;
+
+	atomic_inc(&tr->disabled);
+	if (atomic_read(&tr->disabled) != 1)
+		goto out;
+	t0 = cycles_to_usecs(tr->preempt_timestamp);
+	t1 = cycles_to_usecs(get_cycles());
+	latency = t1-t0;
+
+	if (((latency >= preempt_max_latency) && (latency > 10000))
+						&& !hardirq_count()) {
+		tr->early_warning = 1;
+		if (!down_trylock(&max_mutex)) {
+			printk("hm, larger than 10 msec latency in the making.\n");
+			nmi_show_all_regs();
+			up(&max_mutex);
+		}
+	}
+out:
+	atomic_dec(&tr->disabled);
+}
+
+static void notrace
+check_preempt_timing(struct cpu_trace *tr, unsigned long parent_eip)
+{
+#ifdef CONFIG_LATENCY_TRACE
+	unsigned long eip = (unsigned long)__builtin_return_address(0);
+#endif
+	unsigned long latency, t0, t1;
+
+#ifdef CONFIG_LATENCY_TRACE
+	if (trace_enabled >= 2)
+		return;
+#endif
+	atomic_inc(&tr->disabled);
+	if (atomic_read(&tr->disabled) != 1)
+		goto out;
+	t0 = cycles_to_usecs(tr->preempt_timestamp);
+	t1 = cycles_to_usecs(get_cycles());
+	latency = t1-t0;
+
+	if (preempt_thresh) {
+		if (latency < preempt_thresh)
+			goto out;
+	} else {
+		if (latency <= preempt_max_latency)
+			goto out;
+	}
+
+	if (down_trylock(&max_mutex))
+		goto out;
+
+	tr->critical_end = parent_eip;
+	preempt_max_latency = latency;
+
+//	if (latency >= 1000000 && !hardirq_count())
+//		nmi_show_all_regs();
+//
+#ifdef CONFIG_LATENCY_TRACE
+	____trace(tr, eip, parent_eip);
+	update_max_trace(tr);
+#endif
+
+	if (preempt_thresh)
+		printk("(%.16s/%d/CPU#%d): %lu us critical section "
+			"violates %lu us threshold.\n"
+			" => started at timestamp %lu: ",
+				current->comm, current->pid,
+				_smp_processor_id(),
+				latency, preempt_thresh, t0);
+	else
+		printk("(%.16s/%d/CPU#%d): new %lu us maximum-latency "
+			"critical section.\n => started at timestamp %lu: ",
+				current->comm, current->pid,
+				_smp_processor_id(),
+				latency, t0);
+
+	print_symbol("<%s>\n", tr->critical_start);
+	printk(" =>   ended at timestamp %lu: ", t1);
+	print_symbol("<%s>\n", tr->critical_end);
+	dump_stack();
+	t1 = cycles_to_usecs(get_cycles());
+	printk(" =>   dump-end timestamp %lu\n\n", t1);
+
+	up(&max_mutex);
+out:
+#ifdef CONFIG_LATENCY_TRACE
+	tr->trace_idx = 0;
+#endif
+	tr->preempt_timestamp = get_cycles();
+	tr->critical_start = parent_eip;
+	tr->early_warning = 0;
+	__trace(eip, parent_eip);
+
+	atomic_dec(&tr->disabled);
+}
+
+void notrace touch_preempt_timing(void)
+{
+	struct cpu_trace *tr = &per_cpu(trace, _smp_processor_id());
+	unsigned long flags;
+
+	if (preempt_count() > 0 && tr->critical_start) {
+		local_irq_save(flags);
+		check_preempt_timing(tr,
+			(unsigned long)__builtin_return_address(0));
+		local_irq_restore(flags);
+	}
+}
+EXPORT_SYMBOL(touch_preempt_timing);
+
+void notrace stop_preempt_timing(void)
+{
+	struct cpu_trace *tr;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	tr = &__get_cpu_var(trace);
+	tr->critical_start = 0;
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(stop_preempt_timing);
+
+#endif /* PREEMPT_TIMING */
+
+void notrace add_preempt_count(int val)
+{
+	unsigned long eip = (unsigned long)__builtin_return_address(0);
+#if defined(CONFIG_FRAME_POINTER) || defined(CONFIG_MCOUNT)
+	unsigned long parent_eip = (unsigned long)__builtin_return_address(1);
+#endif
+
+	/*
+	 * Underflow?
+	 */
+	BUG_ON(((int)preempt_count() < 0));
+	preempt_count() += val;
+	if (val <= 10) {
+		unsigned int idx = preempt_count() & PREEMPT_MASK;
+		if (idx < MAX_PREEMPT_TRACE) {
+			current->preempt_trace_eip[idx] = eip;
+#if defined(CONFIG_FRAME_POINTER) || defined(CONFIG_MCOUNT)
+			current->preempt_trace_parent_eip[idx] = parent_eip;
+#else
+			current->preempt_trace_parent_eip[idx] = 0;
+#endif
+		}
+	}
+	/*
+	 * Spinlock count overflowing soon?
+	 */
+	BUG_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
+#ifdef CONFIG_LATENCY_TRACE
+	if (trace_enabled >= 2)
+		return;
+#endif
+#ifdef CONFIG_PREEMPT_TIMING
+	if (preempt_count() == val) {
+		struct cpu_trace *tr = &__get_cpu_var(trace);
+		unsigned long flags;
+
+		local_irq_save(flags);
+		tr->preempt_timestamp = get_cycles();
+		tr->critical_start = eip;
+#ifdef CONFIG_LATENCY_TRACE
+		tr->trace_idx = 0;
+		____trace(tr, eip, parent_eip);
+#endif
+		local_irq_restore(flags);
+	}
+#endif
+}
+EXPORT_SYMBOL(add_preempt_count);
+
+void notrace sub_preempt_count(int val)
+{
+	/*
+	 * Underflow?
+	 */
+	BUG_ON(val > preempt_count());
+
+	/*
+	 * Is the spinlock portion underflowing?
+	 */
+	BUG_ON((val < PREEMPT_MASK) && !(preempt_count() & PREEMPT_MASK));
+
+#ifdef CONFIG_PREEMPT_TIMING
+	{
+		struct cpu_trace *tr = &per_cpu(trace, _smp_processor_id());
+		unsigned long flags;
+
+		if (preempt_count() == val && tr->critical_start) {
+			local_irq_save(flags);
+			check_preempt_timing(tr,
+				(unsigned long)
+					__builtin_return_address(0));
+			tr->critical_start = 0;
+			local_irq_restore(flags);
+		}
+#if 0
+		local_irq_save(flags);
+		debug_preempt_timing(tr,
+			(unsigned long)
+				__builtin_return_address(0));
+		local_irq_restore(flags);
+#endif
+	}
+#endif
+	preempt_count() -= val;
+}
+
+EXPORT_SYMBOL(sub_preempt_count);
+
+#ifdef CONFIG_LATENCY_TRACE
+void user_trace_start(void)
+{
+	struct cpu_trace *tr;
+
+	if (trace_enabled != 2)
+		return;
+	tr = &get_cpu_var(trace);
+	tr->trace_idx = 0;
+	mcount();
+	put_cpu_var(trace);
+}
+
+void user_trace_stop(void)
+{
+	struct cpu_trace *tr;
+
+	if (trace_enabled != 2)
+		return;
+	tr = &get_cpu_var(trace);
+	mcount();
+	update_max_trace(tr);
+	put_cpu_var(trace);
+}
+
+void stop_trace(void)
+{
+	if (trace_enabled != 3)
+		return;
+	trace_enabled = -1;
+}
+
+void print_last_trace(void)
+{
+	struct trace_entry *entry;
+	struct cpu_trace *tr;
+	unsigned int idx, i;
+
+	if (trace_enabled != -1)
+		return;
+	tr = &get_cpu_var(trace);
+
+	printk("Last %ld trace entries:\n", MAX_TRACE/4);
+	idx = tr->trace_idx;
+	printk("curr idx: %d\n", idx);
+	if (idx >= MAX_TRACE)
+		idx = MAX_TRACE-1;
+	for (i = 0; i < MAX_TRACE/4; i++) {
+		entry = tr->trace + idx;
+	        printk("%08lx: ", entry->preempt_count);
+		printk_name(entry->eip);
+		printk("  <= (");
+		printk_name(entry->parent_eip);
+		printk(")\n");
+		if (!idx)
+			idx = MAX_TRACE-1;
+		else
+			idx--;
+	}
+	trace_enabled = 3;
+	put_cpu_var(trace);
+}
+
+#ifdef CONFIG_SMP
+/*
+ * On SMP, try to 'peek' on other CPU's traces and record them
+ * in this CPU's trace. This way we get a rough idea about what's
+ * going on there, without the overhead of global tracing.
+ *
+ * (no need to make this PER_CPU, we bounce it around anyway.)
+ */
+unsigned long nmi_eips[NR_CPUS];
+unsigned long nmi_flags[NR_CPUS];
+
+void notrace nmi_trace(unsigned long eip, unsigned long parent_eip,
+			unsigned long flags)
+{
+	int cpu, this_cpu = smp_processor_id();
+
+	__trace(eip, parent_eip);
+
+	nmi_eips[this_cpu] = parent_eip;
+	nmi_flags[this_cpu] = flags;
+	for (cpu = 0; cpu < NR_CPUS; cpu++)
+		if (cpu_online(cpu) && cpu != this_cpu) {
+			__trace(eip, nmi_eips[cpu]);
+			__trace(eip, nmi_flags[cpu]);
+		}
+}
+#else
+/*
+ * On UP, NMI tracing is quite simple:
+ */
+void notrace nmi_trace(unsigned long eip, unsigned long parent_eip, unsigned long flags)
+{
+	__trace(eip, parent_eip);
+}
+#endif
+
+#endif
+
+#ifdef CONFIG_PREEMPT_TRACE
+
+static void print_preempt_trace(void)
+{
+	unsigned int i, lim = preempt_count() & PREEMPT_MASK;
+	if (lim >= MAX_PREEMPT_TRACE)
+		lim = MAX_PREEMPT_TRACE-1;
+	printk("preempt count: %08x\n", preempt_count());
+	printk(". %d-level deep critical section nesting:\n", lim);
+	for (i = 1; i <= lim; i++) {
+		printk(".. entry %d: ", i);
+		print_symbol("%s / (", current->preempt_trace_eip[i]);
+		print_symbol("%s)\n", current->preempt_trace_parent_eip[i]);
+	}
+	printk("\n");
+}
+
+#endif
+
+#if defined(CONFIG_PREEMPT_TRACE) || defined(CONFIG_LATENCY_TRACE)
+void print_traces(void)
+{
+	preempt_disable();
+#ifdef CONFIG_PREEMPT_TRACE
+	print_preempt_trace();
+#endif
+#ifdef CONFIG_LATENCY_TRACE
+	print_last_trace();
+#endif
+	preempt_enable();
+}
+#endif
+
Index: xx-sources/kernel/module.c
===================================================================
--- xx-sources.orig/kernel/module.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/module.c	2004-10-16 20:57:01.000000000 -0400
@@ -53,7 +53,7 @@
 #define INIT_OFFSET_MASK (1UL << (BITS_PER_LONG-1))
 
 /* Protects module list */
-static spinlock_t modlist_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(modlist_lock);
 
 /* List of modules, protected by module_mutex AND modlist_lock */
 static DECLARE_MUTEX(module_mutex);
Index: xx-sources/kernel/mutex.c
===================================================================
--- xx-sources.orig/kernel/mutex.c	2004-10-14 14:49:41.073057592 -0400
+++ xx-sources/kernel/mutex.c	2004-10-16 20:57:01.000000000 -0400
@@ -0,0 +1,406 @@
+/*
+ * Real-Time Preemption support
+ *
+ * Copyright (c) 2004 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+
+#ifdef CONFIG_PREEMPT_REALTIME
+
+static void mutex_initialize(_mutex_t *mutex)
+{
+	if (!mutex->initialized) {
+		mutex->initialized = 1;
+		init_MUTEX(&mutex->lock);
+//		WARN_ON(1);
+	}
+}
+
+void _mutex_lock(_mutex_t *mutex)
+{
+	might_sleep();
+	mutex_initialize(mutex);
+	down(&mutex->lock);
+}
+
+EXPORT_SYMBOL(_mutex_lock);
+
+void _mutex_lock_bh(_mutex_t *mutex)
+{
+	_mutex_lock(mutex);
+//	local_bh_disable();
+}
+
+EXPORT_SYMBOL(_mutex_lock_bh);
+
+void _mutex_lock_irq(_mutex_t *mutex)
+{
+	_mutex_lock(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_lock_irq);
+
+unsigned long _mutex_lock_irqsave(_mutex_t *mutex)
+{
+	unsigned long flags;
+
+	_mutex_lock(mutex);
+	local_save_flags(flags);
+
+	return flags;
+}
+
+EXPORT_SYMBOL(_mutex_lock_irqsave);
+
+void _mutex_unlock(_mutex_t *mutex)
+{
+	if (!mutex->initialized) {
+		mutex_initialize(mutex);
+		WARN_ON(1);
+	};
+	up(&mutex->lock);
+}
+
+EXPORT_SYMBOL(_mutex_unlock);
+
+void _mutex_unlock_wait(_mutex_t *mutex)
+{
+	BUG_ON(!mutex->initialized);
+	do {
+		barrier();
+	} while (_mutex_is_locked(mutex));
+}
+
+EXPORT_SYMBOL(_mutex_unlock_wait);
+
+void _mutex_unlock_bh(_mutex_t *mutex)
+{
+	BUG_ON(!mutex->initialized);
+	up(&mutex->lock);
+}
+
+EXPORT_SYMBOL(_mutex_unlock_bh);
+
+void _mutex_unlock_irq(_mutex_t *mutex)
+{
+	BUG_ON(!mutex->initialized);
+	up(&mutex->lock);
+}
+
+EXPORT_SYMBOL(_mutex_unlock_irq);
+
+void _mutex_unlock_irqrestore(_mutex_t *mutex, unsigned long flags)
+{
+	BUG_ON(!mutex->initialized);
+	up(&mutex->lock);
+}
+
+EXPORT_SYMBOL(_mutex_unlock_irqrestore);
+
+int _mutex_trylock(_mutex_t *mutex)
+{
+	mutex_initialize(mutex);
+	return !down_trylock(&mutex->lock);
+}
+
+EXPORT_SYMBOL(_mutex_trylock);
+
+int _mutex_trylock_bh(_mutex_t *mutex)
+{
+	return _mutex_trylock(mutex);
+}
+EXPORT_SYMBOL(_mutex_trylock_bh);
+
+int _mutex_is_locked(_mutex_t *mutex)
+{
+	return atomic_read(&mutex->lock.count) != 1;
+}
+
+EXPORT_SYMBOL(_mutex_is_locked);
+
+int atomic_dec_and_mutex_lock(atomic_t *atomic, _mutex_t *mutex)
+{
+	might_sleep();
+	_mutex_lock(mutex);
+	if (atomic_dec_and_test(atomic))
+		return 1;
+	_mutex_unlock(mutex);
+
+	return 0;
+}
+
+EXPORT_SYMBOL(atomic_dec_and_mutex_lock);
+
+void _mutex_lock_init(_mutex_t *mutex)
+{
+	mutex->initialized = 1;
+	init_MUTEX(&mutex->lock);
+}
+
+EXPORT_SYMBOL(_mutex_lock_init);
+
+
+
+static void rw_mutex_initialize(rw_mutex_t *rw_mutex)
+{
+	if (!rw_mutex->initialized) {
+		rw_mutex->initialized = 1;
+		init_rwsem(&rw_mutex->lock);
+//		WARN_ON(1);
+	}
+}
+
+int _rw_mutex_read_trylock(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	return down_read_trylock(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_trylock);
+
+int _rw_mutex_write_trylock(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	return down_write_trylock(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_trylock);
+
+void _rw_mutex_write_lock(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	down_write(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_lock);
+
+void _rw_mutex_read_lock(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	down_read(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_lock);
+
+void _rw_mutex_write_unlock(rw_mutex_t *rw_mutex)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_write(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_unlock);
+
+void _rw_mutex_read_unlock(rw_mutex_t *rw_mutex)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_read(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_unlock);
+
+unsigned long _rw_mutex_write_lock_irqsave(rw_mutex_t *rw_mutex)
+{
+	unsigned long flags;
+
+	rw_mutex_initialize(rw_mutex);
+	down_write(&rw_mutex->lock);
+
+	local_save_flags(flags);
+	return flags;
+}
+EXPORT_SYMBOL(_rw_mutex_write_lock_irqsave);
+
+unsigned long _rw_mutex_read_lock_irqsave(rw_mutex_t *rw_mutex)
+{
+	unsigned long flags;
+
+	rw_mutex_initialize(rw_mutex);
+	down_read(&rw_mutex->lock);
+
+	local_save_flags(flags);
+	return flags;
+}
+EXPORT_SYMBOL(_rw_mutex_read_lock_irqsave);
+
+void _rw_mutex_write_lock_irq(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	down_write(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_lock_irq);
+
+void _rw_mutex_read_lock_irq(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	down_read(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_lock_irq);
+
+void _rw_mutex_write_lock_bh(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	down_write(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_lock_bh);
+
+void _rw_mutex_read_lock_bh(rw_mutex_t *rw_mutex)
+{
+	rw_mutex_initialize(rw_mutex);
+	down_read(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_lock_bh);
+
+void _rw_mutex_write_unlock_irq(rw_mutex_t *rw_mutex)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_write(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_unlock_irq);
+
+void _rw_mutex_read_unlock_irq(rw_mutex_t *rw_mutex)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_read(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_unlock_irq);
+
+void _rw_mutex_write_unlock_bh(rw_mutex_t *rw_mutex)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_write(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_unlock_bh);
+
+void _rw_mutex_read_unlock_bh(rw_mutex_t *rw_mutex)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_read(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_unlock_bh);
+
+void _rw_mutex_write_unlock_irqrestore(rw_mutex_t *rw_mutex, unsigned long flags)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_write(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_write_unlock_irqrestore);
+
+void _rw_mutex_read_unlock_irqrestore(rw_mutex_t *rw_mutex, unsigned long flags)
+{
+	BUG_ON(!rw_mutex->initialized);
+	up_read(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_read_unlock_irqrestore);
+
+void _rw_mutex_init(rw_mutex_t *rw_mutex)
+{
+	rw_mutex->initialized = 1;
+	init_rwsem(&rw_mutex->lock);
+}
+EXPORT_SYMBOL(_rw_mutex_init);
+
+int _rw_mutex_is_locked(rw_mutex_t *rw_mutex)
+{
+	int ret;
+
+	spin_lock(&rw_mutex->lock.wait_lock);
+	ret = rw_mutex->lock.activity != 0;
+	spin_unlock(&rw_mutex->lock.wait_lock);
+
+	return ret;
+}
+
+EXPORT_SYMBOL(_rw_mutex_is_locked);
+
+
+#else
+void _mutex_lock(_mutex_t *mutex)
+{
+	_spin_lock(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_lock);
+
+void _mutex_lock_bh(_mutex_t *mutex)
+{
+	_spin_lock_bh(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_lock_bh);
+
+void _mutex_lock_irq(_mutex_t *mutex)
+{
+	_spin_lock_irq(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_lock_irq);
+
+unsigned long _mutex_lock_irqsave(_mutex_t *mutex)
+{
+	return _spin_lock_irqsave(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_lock_irqsave);
+
+void _mutex_unlock(_mutex_t *mutex)
+{
+	_spin_unlock(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_unlock);
+
+void _mutex_unlock_wait(_mutex_t *mutex)
+{
+	_spin_unlock_wait(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_unlock_wait);
+
+void _mutex_unlock_bh(_mutex_t *mutex)
+{
+	_spin_unlock_bh(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_unlock_bh);
+
+void _mutex_unlock_irq(_mutex_t *mutex)
+{
+	_spin_unlock_irq(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_unlock_irq);
+
+void _mutex_unlock_irqrestore(_mutex_t *mutex, unsigned long flags)
+{
+	_spin_unlock_irqrestore(mutex, flags);
+}
+
+EXPORT_SYMBOL(_mutex_unlock_irqrestore);
+
+int _mutex_trylock(_mutex_t *mutex)
+{
+	return _spin_trylock(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_trylock);
+
+int _mutex_is_locked(_mutex_t *mutex)
+{
+	return _spin_is_locked(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_is_locked);
+
+int atomic_dec_and_mutex_lock(atomic_t *atomic, _mutex_t *mutex)
+{
+	return _atomic_dec_and_spin_lock(atomic, mutex);
+}
+
+EXPORT_SYMBOL(atomic_dec_and_mutex_lock);
+
+void _mutex_lock_init(_mutex_t *mutex)
+{
+	_spin_lock_init(mutex);
+}
+
+EXPORT_SYMBOL(_mutex_lock_init);
+
+#endif
Index: xx-sources/kernel/nicksched-sched.c
===================================================================
--- xx-sources.orig/kernel/nicksched-sched.c	2004-10-16 20:52:28.000000000 -0400
+++ xx-sources/kernel/nicksched-sched.c	2004-10-16 22:00:12.435369200 -0400
@@ -34,6 +34,7 @@
 #include <linux/syscalls.h>
 #include <linux/times.h>
 #include <linux/sysctl.h>
+#include <linux/kallsyms.h>
 
 #include <asm/tlb.h>
 
@@ -133,7 +134,7 @@
  * acquire operations must be ordered by ascending &runqueue.
  */
 struct runqueue {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
@@ -1208,6 +1209,7 @@
 	 */
 	prev_task_flags = prev->flags;
 	finish_arch_switch(rq, prev);
+	preempt_enable_no_resched();
 	if (mm)
 		mmdrop(mm);
 	if (unlikely(prev_task_flags & PF_DEAD))
@@ -2274,17 +2276,21 @@
 	unsigned long run_time;
 	int cpu, idx;
 
+	WARN_ON(system_state == SYSTEM_BOOTING);
 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
 	 * nicksched_schedule() atomically, we ignore that path for now.
 	 * Otherwise, whine if we are scheduling when we should not be.
 	 */
 	if (unlikely(in_atomic()) &&
-			likely(!(current->state & (TASK_DEAD | TASK_ZOMBIE)))) {
+			likely(!(current->exit_state & (__TASK_DEAD | __TASK_ZOMBIE)))) {
+			stop_trace();
 			printk(KERN_ERR "scheduling while atomic: "
 				"%s/0x%08x/%d\n",
 				current->comm, preempt_count(), current->pid);
-		dump_stack();
+			print_symbol("caller is %s\n",
+				(long)__builtin_return_address(0));
+			dump_stack();
 	}
 
 need_resched:
@@ -2310,6 +2316,8 @@
 
 	spin_lock_irq(&rq->lock);
 
+	if (unlikely(current->flags & PF_DEAD))
+		current->state = __TASK_DEAD;
 	/*
 	 * if entering off of a kernel preemption go straight
 	 * to picking the next task.
@@ -2411,12 +2419,14 @@
 		prev = context_switch(rq, prev, next);
 		barrier();
 
+		// re-enables preemption
 		finish_task_switch(prev);
-	} else
+	} else {
 		spin_unlock_irq(&rq->lock);
+		preempt_enable_no_resched();
+	}
 
 	reacquire_kernel_sem(current);
-	preempt_enable_no_resched();
 	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
 		goto need_resched;
 }
@@ -3328,7 +3338,7 @@
 	struct runqueue *rq = cpu_rq(dead_cpu);
 
 	/* Must be exiting, otherwise would be on tasklist. */
-	BUG_ON(tsk->state != TASK_ZOMBIE && tsk->state != TASK_DEAD);
+	BUG_ON(tsk->exit_state != __TASK_ZOMBIE && tsk->exit_state != __TASK_DEAD);
 
 	/* Cannot have done final schedule yet: would have vanished. */
 	BUG_ON(tsk->flags & PF_DEAD);
Index: xx-sources/kernel/power/process.c
===================================================================
--- xx-sources.orig/kernel/power/process.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/power/process.c	2004-10-16 20:57:01.000000000 -0400
@@ -23,8 +23,8 @@
 {
 	if ((p == current) || 
 	    (p->flags & PF_NOFREEZE) ||
-	    (p->state == TASK_ZOMBIE) ||
-	    (p->state == TASK_DEAD) ||
+	    (p->exit_state == __TASK_ZOMBIE) ||
+	    (p->exit_state == __TASK_DEAD) ||
 	    (p->state == TASK_STOPPED) ||
 	    (p->state == TASK_TRACED))
 		return 0;
Index: xx-sources/kernel/power/swsusp.c
===================================================================
--- xx-sources.orig/kernel/power/swsusp.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/power/swsusp.c	2004-10-16 20:57:01.000000000 -0400
@@ -246,7 +246,7 @@
 	swp_entry_t entry;
 	int error = 0;
 
-	entry = get_swap_page();
+	entry = get_swap_page(NULL, addr >> PAGE_SHIFT);
 	if (swp_offset(entry) && 
 	    swapfile_used[swp_type(entry)] == SWAPFILE_SUSPEND) {
 		error = rw_swap_page_sync(WRITE, entry,
Index: xx-sources/kernel/printk.c
===================================================================
--- xx-sources.orig/kernel/printk.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/printk.c	2004-10-16 22:00:56.435680136 -0400
@@ -78,7 +78,7 @@
  * It is also used in interesting ways to provide interlocking in
  * release_console_sem().
  */
-static spinlock_t logbuf_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(logbuf_lock);
 
 static char __log_buf[__LOG_BUF_LEN];
 static char *log_buf = __log_buf;
@@ -389,10 +389,12 @@
 {
 	struct console *con;
 
+	touch_preempt_timing();
 	for (con = console_drivers; con; con = con->next) {
 		if ((con->flags & CON_ENABLED) && con->write)
 			con->write(con, &LOG_BUF(start), end - start);
 	}
+	touch_preempt_timing();
 }
 
 /*
@@ -871,7 +873,7 @@
  */
 int __printk_ratelimit(int ratelimit_jiffies, int ratelimit_burst)
 {
-	static spinlock_t ratelimit_lock = SPIN_LOCK_UNLOCKED;
+	static DECLARE_RAW_SPINLOCK(ratelimit_lock);
 	static unsigned long toks = 10*5*HZ;
 	static unsigned long last_msg;
 	static int missed;
Index: xx-sources/kernel/profile.c
===================================================================
--- xx-sources.orig/kernel/profile.c	2004-10-16 20:52:28.000000000 -0400
+++ xx-sources/kernel/profile.c	2004-10-16 20:57:01.000000000 -0400
@@ -80,7 +80,7 @@
 #ifdef CONFIG_PROFILING
  
 static DECLARE_RWSEM(profile_rwsem);
-static rwlock_t handoff_lock = RW_LOCK_UNLOCKED;
+static DECLARE_RAW_RWLOCK(handoff_lock);
 static struct notifier_block * task_exit_notifier;
 static struct notifier_block * task_free_notifier;
 static struct notifier_block * munmap_notifier;
@@ -169,7 +169,7 @@
 }
 
 static struct notifier_block * profile_listeners;
-static rwlock_t profile_lock = RW_LOCK_UNLOCKED;
+static raw_rwlock_t profile_lock = RAW_RW_LOCK_UNLOCKED;
  
 int register_profile_notifier(struct notifier_block * nb)
 {
@@ -385,6 +385,8 @@
 {
 	unsigned long pc;
 
+	if (prof_on != type || !prof_buffer)
+		return;
 	pc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;
 	atomic_inc(&prof_buffer[min(pc, prof_len - 1)]);
 }
@@ -394,8 +396,6 @@
 {
 	if (type == CPU_PROFILING)
 		profile_hook(regs);
-	if (prof_on != type || !prof_buffer)
-		return;
 	if (!user_mode(regs) && cpu_isset(smp_processor_id(), prof_cpu_mask))
 		profile_hit(type, (void *)profile_pc(regs));
 }
Index: xx-sources/kernel/ptrace.c
===================================================================
--- xx-sources.orig/kernel/ptrace.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/kernel/ptrace.c	2004-10-16 20:57:01.000000000 -0400
@@ -163,7 +163,7 @@
 	write_lock_irq(&tasklist_lock);
 	__ptrace_unlink(child);
 	/* .. and wake it up. */
-	if (child->state != TASK_ZOMBIE)
+	if (child->exit_state != __TASK_ZOMBIE)
 		wake_up_process(child);
 	write_unlock_irq(&tasklist_lock);
 
Index: xx-sources/kernel/sched.c
===================================================================
--- xx-sources.orig/kernel/sched.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/sched.c	2004-10-16 22:00:12.431369808 -0400
@@ -4,6 +4,7 @@
  *  Kernel scheduler and related syscalls
  *
  *  Copyright (C) 1991-2002  Linus Torvalds
+ *  Copyright (C) 2004 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
  *
  *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and
  *		make semaphores SMP safe
@@ -18,6 +19,7 @@
  *  2004-04-02	Scheduler domains code by Nick Piggin
  *  2004-10-04  Modular scheduler subsystem rewrite/hack by Chris Han
  *  		(xiphux), largely based on the modular block elevator code.
+ *  2004-10-13  Real-Time Preemption support by Ingo Molnar
  */
 
 #include <linux/mm.h>
@@ -48,6 +50,7 @@
 #include <linux/seq_file.h>
 #include <linux/syscalls.h>
 #include <linux/times.h>
+#include <linux/kallsyms.h>
 
 #include <asm/tlb.h>
 
@@ -133,6 +136,7 @@
 
 inline asmlinkage void schedule_tail(task_t *prev)
 {
+	preempt_disable(); // TODO: move this to fork setup
 	BUG_ON(!current_scheduler->schedule_tail_fn);
 	current_scheduler->schedule_tail_fn(prev);
 }
@@ -231,11 +235,9 @@
 		return;
 
 	task->lock_depth = -1;
-	preempt_enable_no_resched();
 
 	down(&kernel_sem);
 
-	preempt_disable();
 	task->lock_depth = saved_lock_depth;
 }
 
@@ -268,7 +270,7 @@
 
 #else
 
-static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(kernel_flag);
 
 int kernel_locked(void)
 {
@@ -666,40 +668,12 @@
 
 
 #if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
-
-void fastcall add_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(((int)preempt_count() < 0));
-	preempt_count() += val;
-	/*
-	 * Spinlock count overflowing soon?
-	 */
-	BUG_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
-}
-EXPORT_SYMBOL(add_preempt_count);
-
-void fastcall sub_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(val > preempt_count());
-	/*
-	 * Is the spinlock portion underflowing?
-	 */
-	BUG_ON((val < PREEMPT_MASK) && !(preempt_count() & PREEMPT_MASK));
-	preempt_count() -= val;
-}
-EXPORT_SYMBOL(sub_preempt_count);
-
 #ifdef __smp_processor_id
+
 /*
  * Debugging check.
  */
-unsigned int smp_processor_id(void)
+unsigned int notrace smp_processor_id(void)
 {
 	unsigned long preempt_count = preempt_count();
 	int this_cpu = __smp_processor_id();
@@ -734,8 +708,8 @@
 	if (!printk_ratelimit())
 		goto out_enable;
 
-	printk(KERN_ERR "using smp_processor_id() in preemptible code: %s/%d\n",
-		current->comm, current->pid);
+	printk(KERN_ERR "using smp_processor_id() in preemptible [%08x] code: %s/%d\n", preempt_count(), current->comm, current->pid);
+	print_symbol("caller is %s\n", (long)__builtin_return_address(0));
 	dump_stack();
 
 out_enable:
@@ -751,6 +725,32 @@
 #endif /* PREEMPT && DEBUG_PREEMPT */
 
 #ifdef CONFIG_PREEMPT
+
+int kernel_preemption = 1;
+
+static int __init preempt_setup (char *str)
+{
+	if (!strncmp(str, "off", 3)) {
+		if (kernel_preemption) {
+			printk("turning off kernel preemption!\n");
+			kernel_preemption = 0;
+		}
+		return 1;
+	}
+	if (!strncmp(str, "on", 2)) {
+		if (!kernel_preemption) {
+			printk("turning on kernel preemption!\n");
+			kernel_preemption = 1;
+		}
+		return 1;
+	}
+	get_option(&str, &kernel_preemption);
+
+	return 1;
+}
+
+__setup("preempt=", preempt_setup);
+
 /*
  * this is is the entry point to schedule() from in-kernel preemption
  * off of preempt_enable.  Kernel preemptions off return from interrupt
@@ -764,8 +764,8 @@
 	int saved_lock_depth;
 #endif
 
-
-
+	if (!kernel_preemption)
+		return;
 	/*
 	 * If there is a non-zero preempt_count or interrupts are disabled,
 	 * we do not want to preempt the current task.  Just return..
@@ -804,8 +804,8 @@
 
 static inline void __cond_resched(void)
 {
-
-
+	if (system_state == SYSTEM_BOOTING || !current->pid)
+		return;
 	if (preempt_count() & PREEMPT_ACTIVE)
 		return;
 	do {
@@ -817,7 +817,7 @@
 
 int __sched cond_resched(void)
 {
-	if (need_resched()) {
+	if (voluntary_preemption && need_resched()) {
 		__cond_resched();
 		return 1;
 	}
@@ -834,7 +834,7 @@
  * operations here to prevent schedule() from being called twice (once via
  * spin_unlock(), once by hand).
  */
-int cond_resched_lock(spinlock_t * lock)
+int __cond_resched_raw_spinlock(raw_spinlock_t *lock)
 {
 #if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT)
 	if (lock->break_lock) {
@@ -854,23 +854,101 @@
 	return 0;
 }
 
-EXPORT_SYMBOL(cond_resched_lock);
+EXPORT_SYMBOL(__cond_resched_raw_spinlock);
 
+int __cond_resched_mutex(_mutex_t *mutex)
+{
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT)
+	if (mutex->break_lock) {
+		mutex->break_lock = 0;
+		_mutex_unlock(mutex);
+		__cond_resched();
+		_mutex_lock(mutex);
+	}
+#endif
+	return 0;
+}
+
+EXPORT_SYMBOL(__cond_resched_mutex);
+
+
+/*
+ * Preempt a softirq context if necessary:
+ */
 int __sched cond_resched_softirq(void)
 {
+#ifndef CONFIG_PREEMPT_REALTIME
 	BUG_ON(!in_softirq());
 
-	if (need_resched()) {
+	if (softirq_need_resched()) {
 		__local_bh_enable();
 		__cond_resched();
 		local_bh_disable();
 		return 1;
 	}
+#endif
 	return 0;
 }
 
 EXPORT_SYMBOL(cond_resched_softirq);
 
+/*
+ * Preempt a hardirq context if necessary:
+ */
+int cond_resched_hardirq(void)
+{
+	unsigned long flags;
+
+	BUG_ON(!in_irq());
+	if (hardirq_need_resched()) {
+		local_save_flags(flags);
+		irq_exit();
+		__cond_resched();
+		local_irq_restore(flags);
+		irq_enter();
+		return 1;
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL(cond_resched_hardirq);
+
+/*
+ * Preempt any context:
+ */
+int cond_resched_all(void)
+{
+	if (hardirq_count())
+		return cond_resched_hardirq();
+	if (softirq_count())
+		return cond_resched_softirq();
+	return cond_resched();
+}
+
+EXPORT_SYMBOL(cond_resched_all);
+
+#ifdef CONFIG_PREEMPT_VOLUNTARY
+
+int voluntary_preemption = 1;
+
+EXPORT_SYMBOL(voluntary_preemption);
+
+static int __init voluntary_preempt_setup (char *str)
+{
+	if (!strncmp(str, "off", 3))
+		voluntary_preemption = 0;
+	else
+		get_option(&str, &voluntary_preemption);
+	if (!voluntary_preemption)
+		printk("turning off voluntary preemption!\n");
+
+	return 1;
+}
+
+__setup("voluntary-preempt=", voluntary_preempt_setup);
+
+#endif
+
 /**
  * sys_sched_get_priority_max - return maximum RT priority.
  * @policy: scheduling class.
@@ -978,12 +1056,12 @@
 	else
 		printk("?");
 #if (BITS_PER_LONG == 32)
-	if (state == TASK_RUNNING)
+	if (0 && (state == TASK_RUNNING))
 		printk(" running ");
 	else
 		printk(" %08lX ", thread_saved_pc(p));
 #else
-	if (state == TASK_RUNNING)
+	if (0 && (state == TASK_RUNNING))
 		printk("  running task   ");
 	else
 		printk(" %016lx ", thread_saved_pc(p));
@@ -1014,13 +1092,14 @@
 	else
 		printk(" (NOTLB)\n");
 
-	if (state != TASK_RUNNING)
+	if (1 || state != TASK_RUNNING)
 		show_stack(p, NULL);
 }
 
 void show_state(void)
 {
 	task_t *g, *p;
+	int do_unlock = 1;
 
 #if (BITS_PER_LONG == 32)
 	printk("\n"
@@ -1031,7 +1110,16 @@
 	       "                                                       sibling\n");
 	printk("  task                 PC          pid father child younger older\n");
 #endif
+#ifdef CONFIG_PREEMPT_REALTIME
+	if (!read_trylock(&tasklist_lock)) {
+		printk("hm, tasklist_lock write-locked.\n");
+		printk("ignoring ...\n");
+		do_unlock = 0;
+	}
+#else
 	read_lock(&tasklist_lock);
+#endif
+
 	do_each_thread(g, p) {
 		/*
 		 * reset the NMI-timeout, listing all files on a slow
@@ -1041,7 +1129,8 @@
 		show_task(p);
 	} while_each_thread(g, p);
 
-	read_unlock(&tasklist_lock);
+	if (do_unlock)
+		read_unlock(&tasklist_lock);
 }
 
 /*
@@ -1077,7 +1166,7 @@
 		&& addr < (unsigned long)__sched_text_end);
 }
 
-#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
+#if defined(CONFIG_DEBUG_SPINLOCK_SLEEP) || defined(CONFIG_DEBUG_PREEMPT)
 void __might_sleep(char *file, int line)
 {
 #if defined(in_atomic)
@@ -1088,10 +1177,12 @@
 		if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
 			return;
 		prev_jiffy = jiffies;
+		stop_trace();
 		printk(KERN_ERR "Debug: sleeping function called from invalid"
-				" context at %s:%d\n", file, line);
-		printk("in_atomic():%d, irqs_disabled():%d\n",
-			in_atomic(), irqs_disabled());
+				" context %s(%d) at %s:%d\n",
+				current->comm, current->pid, file, line);
+		printk("in_atomic():%d [%08x], irqs_disabled():%d\n",
+			in_atomic(), preempt_count(), irqs_disabled());
 		dump_stack();
 	}
 #endif
Index: xx-sources/kernel/signal.c
===================================================================
--- xx-sources.orig/kernel/signal.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/signal.c	2004-10-16 22:00:54.566964224 -0400
@@ -822,11 +822,11 @@
 {
 	int ret = 0;
 
-	if (!irqs_disabled())
-		BUG();
+#ifndef CONFIG_PREEMPT_REALTIME
+	BUG_ON(!irqs_disabled());
+#endif
 #ifdef CONFIG_SMP
-	if (!spin_is_locked(&t->sighand->siglock))
-		BUG();
+	BUG_ON(!spin_is_locked(&t->sighand->siglock));
 #endif
 
 	if (((unsigned long)info > 2) && (info->si_code == SI_TIMER))
@@ -914,7 +914,8 @@
 	 * Don't bother zombies and stopped tasks (but
 	 * SIGKILL will punch through stopped state)
 	 */
-	mask = TASK_DEAD | TASK_ZOMBIE | TASK_TRACED;
+	// FIXME: remove __TASK_ZOMBIE & __TASK_DEAD
+	mask = __TASK_DEAD | __TASK_ZOMBIE | TASK_TRACED;
 	if (sig != SIGKILL)
 		mask |= TASK_STOPPED;
 
@@ -1070,7 +1071,8 @@
 		/*
 		 * Don't bother with already dead threads
 		 */
-		if (t->state & (TASK_ZOMBIE|TASK_DEAD))
+		// FIXME: remove
+		if (t->exit_state & (__TASK_ZOMBIE|__TASK_DEAD))
 			continue;
 
 		/*
@@ -1799,6 +1801,9 @@
 	sigset_t *mask = &current->blocked;
 	int signr = 0;
 
+#ifdef CONFIG_PREEMPT_REALTIME
+	might_sleep();
+#endif
 relock:
 	spin_lock_irq(&current->sighand->siglock);
 	for (;;) {
Index: xx-sources/kernel/softirq.c
===================================================================
--- xx-sources.orig/kernel/softirq.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/softirq.c	2004-10-16 20:57:01.000000000 -0400
@@ -16,6 +16,7 @@
 #include <linux/cpu.h>
 #include <linux/kthread.h>
 #include <linux/rcupdate.h>
+#include <linux/kallsyms.h>
 
 #include <asm/irq.h>
 /*
@@ -71,7 +72,7 @@
  */
 #define MAX_SOFTIRQ_RESTART 10
 
-asmlinkage void __do_softirq(void)
+asmlinkage void ___do_softirq(void)
 {
 	struct softirq_action *h;
 	__u32 pending;
@@ -80,7 +81,6 @@
 
 	pending = local_softirq_pending();
 
-	local_bh_disable();
 	cpu = smp_processor_id();
 restart:
 	/* Reset the pending bitmask before enabling irqs */
@@ -92,8 +92,17 @@
 
 	do {
 		if (pending & 1) {
-			h->action(h);
+			{
+				u32 preempt_count = preempt_count();
+				h->action(h);
+				if (preempt_count != preempt_count()) {
+					print_symbol("softirq preempt bug: exited %s with wrong preemption count!\n", (unsigned long) h->action);
+					printk("entered with %08x, exited with %08x.\n", preempt_count, preempt_count());
+					preempt_count() = preempt_count;
+				}
+			}
 			rcu_bh_qsctr_inc(cpu);
+			cond_resched_all();
 		}
 		h++;
 		pending >>= 1;
@@ -107,10 +116,51 @@
 
 	if (pending)
 		wakeup_softirqd();
+}
+
+asmlinkage void __do_softirq(void)
+{
+	unsigned long p_flags;
 
+#ifdef CONFIG_PREEMPT_SOFTIRQS
+	/*
+	 * 'preempt harder'. Push all softirq processing off to ksoftirqd.
+	 */
+	if (softirq_preemption) {
+		if (local_softirq_pending())
+			wakeup_softirqd();
+		return;
+	}
+#endif
+	/*
+	 * 'immediate' softirq execution:
+	 */
+	local_bh_disable();
+	p_flags = current->flags & PF_HARDIRQ;
+	current->flags &= ~PF_HARDIRQ;
+
+	___do_softirq();
 	__local_bh_enable();
+
+	current->flags |= p_flags;
 }
 
+/*
+ * 'delayed' softirq execution. Does not disable bhs and thus
+ * makes most of the softirq handlers preemptable - as long as
+ * they are not executed 'directly'.
+ */
+asmlinkage void _do_softirq(void)
+{
+	local_irq_disable();
+	if (!softirq_preemption)
+		__do_softirq();
+	else
+		___do_softirq();
+	local_irq_enable();
+}
+
+
 #ifndef __ARCH_HAS_DO_SOFTIRQ
 
 asmlinkage void do_softirq(void)
@@ -135,6 +185,8 @@
 
 #endif
 
+#ifndef CONFIG_PREEMPT_REALTIME
+
 void local_bh_enable(void)
 {
 	WARN_ON(irqs_disabled());
@@ -152,6 +204,8 @@
 }
 EXPORT_SYMBOL(local_bh_enable);
 
+#endif
+
 /*
  * This function must run with irqs disabled!
  */
@@ -333,8 +387,9 @@
 
 static int ksoftirqd(void * __bind_cpu)
 {
-	set_user_nice(current, 19);
-	current->flags |= PF_NOFREEZE;
+	printk("ksoftirqd started up.\n");
+	set_user_nice(current, -10);
+	current->flags |= PF_NOFREEZE | PF_SOFTIRQ;
 
 	set_current_state(TASK_INTERRUPTIBLE);
 
@@ -351,8 +406,8 @@
 			preempt_disable();
 			if (cpu_is_offline((long)__bind_cpu))
 				goto wait_to_die;
-			do_softirq();
 			preempt_enable();
+			_do_softirq();
 			cond_resched();
 		}
 
@@ -474,3 +529,33 @@
 	register_cpu_notifier(&cpu_nfb);
 	return 0;
 }
+
+#ifdef CONFIG_PREEMPT_SOFTIRQS
+
+int softirq_preemption = 1;
+
+EXPORT_SYMBOL(softirq_preemption);
+
+/*
+ * Real-Time Preemption depends on softirq threading:
+ */
+#ifndef CONFIG_PREEMPT_REALTIME
+
+static int __init softirq_preempt_setup (char *str)
+{
+	if (!strncmp(str, "off", 3))
+		softirq_preemption = 0;
+	else
+		get_option(&str, &softirq_preemption);
+	if (!softirq_preemption)
+		printk("turning off softirq preemption!\n");
+
+	return 1;
+}
+
+__setup("softirq-preempt=", softirq_preempt_setup);
+
+#endif
+
+#endif
+
Index: xx-sources/kernel/spa-sched.c
===================================================================
--- xx-sources.orig/kernel/spa-sched.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/spa-sched.c	2004-10-16 20:57:02.000000000 -0400
@@ -47,6 +47,7 @@
 #include <linux/syscalls.h>
 #include <linux/times.h>
 #include <linux/sysctl.h>
+#include <linux/kallsyms.h>
 
 #include <asm/tlb.h>
 
@@ -360,7 +361,7 @@
  * acquire operations must be ordered by ascending &runqueue.
  */
 struct runqueue {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
@@ -1835,6 +1836,7 @@
 	 */
 	prev_task_flags = prev->flags;
 	finish_arch_switch(rq, prev);
+	preempt_enable_no_resched();
 	if (mm)
 		mmdrop(mm);
 	if (unlikely(prev_task_flags & PF_DEAD))
@@ -3184,16 +3186,20 @@
 	unsigned long long now;
 	int cpu, idx;
 
+	WARN_ON(system_state == SYSTEM_BOOTING);
 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
 	 * schedule() atomically, we ignore that path for now.
 	 * Otherwise, whine if we are scheduling when we should not be.
 	 */
-	if (likely(!(current->state & (TASK_DEAD | TASK_ZOMBIE)))) {
+	if (likely(!(current->exit_state & (__TASK_DEAD | __TASK_ZOMBIE)))) {
 		if (unlikely(in_atomic())) {
+			stop_trace();
 			printk(KERN_ERR "scheduling while atomic: "
 				"%s/0x%08x/%d\n",
 				current->comm, preempt_count(), current->pid);
+			print_symbol("caller is %s\n",
+				(long)__builtin_return_address(0));
 			dump_stack();
 		}
 	}
@@ -3218,6 +3224,8 @@
 
 	spin_lock_irq(&rq->lock);
 
+	if (unlikely(current->flags & PF_DEAD))
+		current->state = __TASK_DEAD;
 	/*
 	 * if entering off of a kernel preemption go straight
 	 * to picking the next task.
@@ -3290,12 +3298,14 @@
 		prev = context_switch(rq, prev, next);
 		barrier();
 
+		// re-enables preemption
 		finish_task_switch(prev);
-	} else
+	} else {
 		spin_unlock_irq(&rq->lock);
+		preempt_enable_no_resched();
+	}
 
 	reacquire_kernel_sem(current);
-	preempt_enable_no_resched();
 	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
 		goto need_resched;
 }
@@ -4919,7 +4929,7 @@
 	struct runqueue *rq = cpu_rq(dead_cpu);
 
 	/* Must be exiting, otherwise would be on tasklist. */
-	BUG_ON(tsk->state != TASK_ZOMBIE && tsk->state != TASK_DEAD);
+	BUG_ON(tsk->exit_state != __TASK_ZOMBIE && tsk->exit_state != __TASK_DEAD);
 
 	/* Cannot have done final schedule yet: would have vanished. */
 	BUG_ON(tsk->flags & PF_DEAD);
Index: xx-sources/kernel/spinlock.c
===================================================================
--- xx-sources.orig/kernel/spinlock.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/spinlock.c	2004-10-16 20:57:02.000000000 -0400
@@ -17,14 +17,14 @@
  * Generic declaration of the raw read_trylock() function,
  * architectures are supposed to optimize this:
  */
-int __lockfunc generic_raw_read_trylock(rwlock_t *lock)
+int __lockfunc generic_raw_read_trylock(raw_rwlock_t *lock)
 {
 	_raw_read_lock(lock);
 	return 1;
 }
 EXPORT_SYMBOL(generic_raw_read_trylock);
 
-int __lockfunc _spin_trylock(spinlock_t *lock)
+int __lockfunc _spin_trylock(raw_spinlock_t *lock)
 {
 	preempt_disable();
 	if (_raw_spin_trylock(lock))
@@ -35,7 +35,7 @@
 }
 EXPORT_SYMBOL(_spin_trylock);
 
-int __lockfunc _read_trylock(rwlock_t *lock)
+int __lockfunc _read_trylock(raw_rwlock_t *lock)
 {
 	preempt_disable();
 	if (_raw_read_trylock(lock))
@@ -46,7 +46,7 @@
 }
 EXPORT_SYMBOL(_read_trylock);
 
-int __lockfunc _write_trylock(rwlock_t *lock)
+int __lockfunc _write_trylock(raw_rwlock_t *lock)
 {
 	preempt_disable();
 	if (_raw_write_trylock(lock))
@@ -59,14 +59,14 @@
 
 #ifndef CONFIG_PREEMPT
 
-void __lockfunc _read_lock(rwlock_t *lock)
+void __lockfunc _read_lock(raw_rwlock_t *lock)
 {
 	preempt_disable();
 	_raw_read_lock(lock);
 }
 EXPORT_SYMBOL(_read_lock);
 
-unsigned long __lockfunc _spin_lock_irqsave(spinlock_t *lock)
+unsigned long __lockfunc _spin_lock_irqsave(raw_spinlock_t *lock)
 {
 	unsigned long flags;
 
@@ -77,7 +77,7 @@
 }
 EXPORT_SYMBOL(_spin_lock_irqsave);
 
-void __lockfunc _spin_lock_irq(spinlock_t *lock)
+void __lockfunc _spin_lock_irq(raw_spinlock_t *lock)
 {
 	local_irq_disable();
 	preempt_disable();
@@ -85,7 +85,7 @@
 }
 EXPORT_SYMBOL(_spin_lock_irq);
 
-void __lockfunc _spin_lock_bh(spinlock_t *lock)
+void __lockfunc _spin_lock_bh(raw_spinlock_t *lock)
 {
 	local_bh_disable();
 	preempt_disable();
@@ -93,7 +93,7 @@
 }
 EXPORT_SYMBOL(_spin_lock_bh);
 
-unsigned long __lockfunc _read_lock_irqsave(rwlock_t *lock)
+unsigned long __lockfunc _read_lock_irqsave(raw_rwlock_t *lock)
 {
 	unsigned long flags;
 
@@ -104,7 +104,7 @@
 }
 EXPORT_SYMBOL(_read_lock_irqsave);
 
-void __lockfunc _read_lock_irq(rwlock_t *lock)
+void __lockfunc _read_lock_irq(raw_rwlock_t *lock)
 {
 	local_irq_disable();
 	preempt_disable();
@@ -112,7 +112,7 @@
 }
 EXPORT_SYMBOL(_read_lock_irq);
 
-void __lockfunc _read_lock_bh(rwlock_t *lock)
+void __lockfunc _read_lock_bh(raw_rwlock_t *lock)
 {
 	local_bh_disable();
 	preempt_disable();
@@ -120,7 +120,7 @@
 }
 EXPORT_SYMBOL(_read_lock_bh);
 
-unsigned long __lockfunc _write_lock_irqsave(rwlock_t *lock)
+unsigned long __lockfunc _write_lock_irqsave(raw_rwlock_t *lock)
 {
 	unsigned long flags;
 
@@ -131,7 +131,7 @@
 }
 EXPORT_SYMBOL(_write_lock_irqsave);
 
-void __lockfunc _write_lock_irq(rwlock_t *lock)
+void __lockfunc _write_lock_irq(raw_rwlock_t *lock)
 {
 	local_irq_disable();
 	preempt_disable();
@@ -139,7 +139,7 @@
 }
 EXPORT_SYMBOL(_write_lock_irq);
 
-void __lockfunc _write_lock_bh(rwlock_t *lock)
+void __lockfunc _write_lock_bh(raw_rwlock_t *lock)
 {
 	local_bh_disable();
 	preempt_disable();
@@ -147,7 +147,7 @@
 }
 EXPORT_SYMBOL(_write_lock_bh);
 
-void __lockfunc _spin_lock(spinlock_t *lock)
+void __lockfunc _spin_lock(raw_spinlock_t *lock)
 {
 	preempt_disable();
 	_raw_spin_lock(lock);
@@ -155,7 +155,7 @@
 
 EXPORT_SYMBOL(_spin_lock);
 
-void __lockfunc _write_lock(rwlock_t *lock)
+void __lockfunc _write_lock(raw_rwlock_t *lock)
 {
 	preempt_disable();
 	_raw_write_lock(lock);
@@ -244,34 +244,34 @@
  *         _[spin|read|write]_lock_irqsave()
  *         _[spin|read|write]_lock_bh()
  */
-BUILD_LOCK_OPS(spin, spinlock_t);
-BUILD_LOCK_OPS(read, rwlock_t);
-BUILD_LOCK_OPS(write, rwlock_t);
+BUILD_LOCK_OPS(spin, raw_spinlock_t);
+BUILD_LOCK_OPS(read, raw_rwlock_t);
+BUILD_LOCK_OPS(write, raw_rwlock_t);
 
 #endif /* CONFIG_PREEMPT */
 
-void __lockfunc _spin_unlock(spinlock_t *lock)
+void __lockfunc _spin_unlock(raw_spinlock_t *lock)
 {
 	_raw_spin_unlock(lock);
 	preempt_enable();
 }
 EXPORT_SYMBOL(_spin_unlock);
 
-void __lockfunc _write_unlock(rwlock_t *lock)
+void __lockfunc _write_unlock(raw_rwlock_t *lock)
 {
 	_raw_write_unlock(lock);
 	preempt_enable();
 }
 EXPORT_SYMBOL(_write_unlock);
 
-void __lockfunc _read_unlock(rwlock_t *lock)
+void __lockfunc _read_unlock(raw_rwlock_t *lock)
 {
 	_raw_read_unlock(lock);
 	preempt_enable();
 }
 EXPORT_SYMBOL(_read_unlock);
 
-void __lockfunc _spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
+void __lockfunc _spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
 {
 	_raw_spin_unlock(lock);
 	local_irq_restore(flags);
@@ -279,7 +279,7 @@
 }
 EXPORT_SYMBOL(_spin_unlock_irqrestore);
 
-void __lockfunc _spin_unlock_irq(spinlock_t *lock)
+void __lockfunc _spin_unlock_irq(raw_spinlock_t *lock)
 {
 	_raw_spin_unlock(lock);
 	local_irq_enable();
@@ -287,7 +287,7 @@
 }
 EXPORT_SYMBOL(_spin_unlock_irq);
 
-void __lockfunc _spin_unlock_bh(spinlock_t *lock)
+void __lockfunc _spin_unlock_bh(raw_spinlock_t *lock)
 {
 	_raw_spin_unlock(lock);
 	preempt_enable();
@@ -295,7 +295,7 @@
 }
 EXPORT_SYMBOL(_spin_unlock_bh);
 
-void __lockfunc _read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
+void __lockfunc _read_unlock_irqrestore(raw_rwlock_t *lock, unsigned long flags)
 {
 	_raw_read_unlock(lock);
 	local_irq_restore(flags);
@@ -303,7 +303,7 @@
 }
 EXPORT_SYMBOL(_read_unlock_irqrestore);
 
-void __lockfunc _read_unlock_irq(rwlock_t *lock)
+void __lockfunc _read_unlock_irq(raw_rwlock_t *lock)
 {
 	_raw_read_unlock(lock);
 	local_irq_enable();
@@ -311,7 +311,7 @@
 }
 EXPORT_SYMBOL(_read_unlock_irq);
 
-void __lockfunc _read_unlock_bh(rwlock_t *lock)
+void __lockfunc _read_unlock_bh(raw_rwlock_t *lock)
 {
 	_raw_read_unlock(lock);
 	preempt_enable();
@@ -319,7 +319,7 @@
 }
 EXPORT_SYMBOL(_read_unlock_bh);
 
-void __lockfunc _write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
+void __lockfunc _write_unlock_irqrestore(raw_rwlock_t *lock, unsigned long flags)
 {
 	_raw_write_unlock(lock);
 	local_irq_restore(flags);
@@ -327,7 +327,7 @@
 }
 EXPORT_SYMBOL(_write_unlock_irqrestore);
 
-void __lockfunc _write_unlock_irq(rwlock_t *lock)
+void __lockfunc _write_unlock_irq(raw_rwlock_t *lock)
 {
 	_raw_write_unlock(lock);
 	local_irq_enable();
@@ -335,7 +335,7 @@
 }
 EXPORT_SYMBOL(_write_unlock_irq);
 
-void __lockfunc _write_unlock_bh(rwlock_t *lock)
+void __lockfunc _write_unlock_bh(raw_rwlock_t *lock)
 {
 	_raw_write_unlock(lock);
 	preempt_enable();
@@ -343,7 +343,7 @@
 }
 EXPORT_SYMBOL(_write_unlock_bh);
 
-int __lockfunc _spin_trylock_bh(spinlock_t *lock)
+int __lockfunc _spin_trylock_bh(raw_spinlock_t *lock)
 {
 	local_bh_disable();
 	preempt_disable();
Index: xx-sources/kernel/staircase-sched.c
===================================================================
--- xx-sources.orig/kernel/staircase-sched.c	2004-10-16 20:52:28.000000000 -0400
+++ xx-sources/kernel/staircase-sched.c	2004-10-16 22:00:12.437368896 -0400
@@ -34,6 +34,7 @@
 #include <linux/syscalls.h>
 #include <linux/times.h>
 #include <linux/sysctl.h>
+#include <linux/kallsyms.h>
 
 #include <asm/tlb.h>
 
@@ -91,7 +92,7 @@
  * acquire operations must be ordered by ascending &runqueue.
  */
 struct runqueue {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
@@ -1115,6 +1116,7 @@
 	 */
 	prev_task_flags = prev->flags;
 	finish_arch_switch(rq, prev);
+	preempt_enable_no_resched();
 	if (mm)
 		mmdrop(mm);
 	if (unlikely(prev_task_flags & PF_DEAD))
@@ -2183,16 +2185,20 @@
 	unsigned long long now;
 	int cpu, idx;
 
+	WARN_ON(system_state == SYSTEM_BOOTING);
 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
 	 * schedule() atomically, we ignore that path for now.
 	 * Otherwise, whine if we are scheduling when we should not be.
 	 */
-	if (likely(!(current->state & (TASK_DEAD | TASK_ZOMBIE)))) {
+	if (likely(!(current->exit_state & (__TASK_DEAD | __TASK_ZOMBIE)))) {
 		if (unlikely(in_atomic())) {
+			stop_trace();
 			printk(KERN_ERR "scheduling while atomic: "
 				"%s/0x%08x/%d\n",
 				current->comm, preempt_count(), current->pid);
+			print_symbol("caller is %s\n",
+				(long)__builtin_return_address(0));
 			dump_stack();
 		}
 	}
@@ -2227,6 +2233,8 @@
 	prev->timestamp = now;
 	spin_lock_irq(&rq->lock);
 
+	if (unlikely(current->flags & PF_DEAD))
+		current->state = __TASK_DEAD;
 	/*
 	 * if entering off of a kernel preemption go straight
 	 * to picking the next task.
@@ -2302,12 +2310,14 @@
 		prev = context_switch(rq, prev, next);
 		barrier();
 
+		// re-enables preemption
 		finish_task_switch(prev);
-	} else
+	} else {
 		spin_unlock_irq(&rq->lock);
+		preempt_enable_no_resched();
+	}
 
 	reacquire_kernel_sem(current);
-	preempt_enable_no_resched();
 	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
 		goto need_resched;
 }
@@ -3182,7 +3192,7 @@
 	struct runqueue *rq = cpu_rq(dead_cpu);
 
 	/* Must be exiting, otherwise would be on tasklist. */
-	BUG_ON(tsk->state != TASK_ZOMBIE && tsk->state != TASK_DEAD);
+	BUG_ON(tsk->exit_state != __TASK_ZOMBIE && tsk->exit_state != __TASK_DEAD);
 
 	/* Cannot have done final schedule yet: would have vanished. */
 	BUG_ON(tsk->flags & PF_DEAD);
Index: xx-sources/kernel/sysctl.c
===================================================================
--- xx-sources.orig/kernel/sysctl.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/sysctl.c	2004-10-16 20:57:02.000000000 -0400
@@ -279,6 +279,74 @@
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec,
 	},
+#ifdef CONFIG_PREEMPT
+	{
+		.ctl_name	= KERN_PANIC,
+		.procname	= "kernel_preemption",
+		.data		= &kernel_preemption,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif
+#ifdef CONFIG_PREEMPT_VOLUNTARY
+	{
+		.ctl_name	= KERN_PANIC,
+		.procname	= "voluntary_preemption",
+		.data		= &voluntary_preemption,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif
+#if defined(CONFIG_PREEMPT_SOFTIRQS) && !defined(CONFIG_PREEMPT_REALTIME)
+	{
+		.ctl_name	= KERN_PANIC,
+		.procname	= "softirq_preemption",
+		.data		= &softirq_preemption,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif
+#if defined(CONFIG_PREEMPT_HARDIRQS) && !defined(CONFIG_PREEMPT_REALTIME)
+	{
+		.ctl_name	= KERN_PANIC,
+		.procname	= "hardirq_preemption",
+		.data		= &hardirq_preemption,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif
+#ifdef CONFIG_PREEMPT_TIMING
+	{
+		.ctl_name	= KERN_PANIC,
+		.procname	= "preempt_thresh",
+		.data		= &preempt_thresh,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+		.ctl_name	= KERN_PANIC,
+		.procname	= "preempt_max_latency",
+		.data		= &preempt_max_latency,
+		.maxlen		= sizeof(unsigned long),
+		.mode		= 0644,
+		.proc_handler	= &proc_doulongvec_minmax,
+	},
+#ifdef CONFIG_LATENCY_TRACE
+	{
+		.ctl_name	= KERN_PANIC,
+		.procname	= "trace_enabled",
+		.data		= &trace_enabled,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+#endif
+#endif
 	{
 		.ctl_name	= KERN_CORE_USES_PID,
 		.procname	= "core_uses_pid",
Index: xx-sources/kernel/time.c
===================================================================
--- xx-sources.orig/kernel/time.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/time.c	2004-10-16 20:57:02.000000000 -0400
@@ -97,8 +97,17 @@
 
 #endif /* __ARCH_WANT_SYS_TIME */
 
+extern void user_trace_start(void);
+extern void user_trace_stop(void);
+
 asmlinkage long sys_gettimeofday(struct timeval __user *tv, struct timezone __user *tz)
 {
+#ifdef CONFIG_LATENCY_TRACE
+	if (!tv && ((long)tz == 1))
+		user_trace_start();
+	if (!tv && !tz)
+		user_trace_stop();
+#endif
 	if (likely(tv != NULL)) {
 		struct timeval ktv;
 		do_gettimeofday(&ktv);
Index: xx-sources/kernel/timer.c
===================================================================
--- xx-sources.orig/kernel/timer.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/timer.c	2004-10-16 20:57:02.000000000 -0400
@@ -33,6 +33,7 @@
 #include <linux/cpu.h>
 #include <linux/perfctr.h>
 #include <linux/syscalls.h>
+#include <linux/kallsyms.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -65,9 +66,10 @@
 } tvec_root_t;
 
 struct tvec_t_base_s {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	unsigned long timer_jiffies;
 	struct timer_list *running_timer;
+	wait_queue_head_t wait_for_running_timer;
 	tvec_root_t tv1;
 	tvec_t tv2;
 	tvec_t tv3;
@@ -86,7 +88,7 @@
 }
 
 /* Fake initialization */
-static DEFINE_PER_CPU(tvec_base_t, tvec_bases) = { SPIN_LOCK_UNLOCKED };
+static DEFINE_PER_CPU(tvec_base_t, tvec_bases) = { RAW_SPIN_LOCK_UNLOCKED };
 
 static void check_timer_failed(struct timer_list *timer)
 {
@@ -353,10 +355,8 @@
 	for_each_online_cpu(i) {
 		base = &per_cpu(tvec_bases, i);
 		if (base->running_timer == timer) {
-			while (base->running_timer == timer) {
-				cpu_relax();
-				preempt_check_resched();
-			}
+			wait_event(base->wait_for_running_timer,
+				base->running_timer != timer);
 			break;
 		}
 	}
@@ -440,7 +440,23 @@
 		struct list_head work_list = LIST_HEAD_INIT(work_list);
 		struct list_head *head = &work_list;
  		int index = base->timer_jiffies & TVR_MASK;
- 
+
+		if (softirq_need_resched()) {
+			/* running_timer might be stale: */
+			set_running_timer(base, NULL);
+//			if (waitqueue_active(&base->wait_running_timer))
+				wake_up(&base->wait_for_running_timer);
+			spin_unlock_irq(&base->lock);
+			cond_resched_all();
+			cpu_relax();
+			spin_lock_irq(&base->lock);
+			/*
+			 * We can simply continue after preemption, nobody
+			 * else can touch timer_jiffies so 'index' is still
+			 * valid. Any new jiffy will be taken care of in
+			 * subsequent loops:
+			 */
+		}
 		/*
 		 * Cascade timers:
 		 */
@@ -469,16 +485,20 @@
 				u32 preempt_count = preempt_count();
 				fn(data);
 				if (preempt_count != preempt_count()) {
-					printk("huh, entered %p with %08x, exited with %08x?\n", fn, preempt_count, preempt_count());
-					BUG();
+					print_symbol("timer preempt bug: exited %s with wrong preemption count!\n", (unsigned long) fn);
+					printk("entered with %08x, exited with %08x.\n", preempt_count, preempt_count());
+					preempt_count() = preempt_count;
 				}
 			}
+			cond_resched_all();
 			spin_lock_irq(&base->lock);
 			goto repeat;
 		}
 	}
 	set_running_timer(base, NULL);
 	spin_unlock_irq(&base->lock);
+//	if (waitqueue_active(&base->wait_running_timer))
+		wake_up(&base->wait_for_running_timer);
 }
 
 #ifdef CONFIG_NO_IDLE_HZ
@@ -813,7 +833,8 @@
 
 	psecs = (p->utime += user);
 	psecs += (p->stime += system);
-	if (!unlikely(p->state & (TASK_DEAD|TASK_ZOMBIE)) &&
+#ifndef CONFIG_PREEMPT_REALTIME
+	if (p->signal && !unlikely(p->exit_state & (__TASK_DEAD|__TASK_ZOMBIE)) &&
 	    psecs / HZ >= p->signal->rlim[RLIMIT_CPU].rlim_cur) {
 		/* Send SIGXCPU every second.. */
 		if (!(psecs % HZ))
@@ -822,10 +843,12 @@
 		if (psecs / HZ >= p->signal->rlim[RLIMIT_CPU].rlim_max)
 			send_sig(SIGKILL, p, 1);
 	}
+#endif
 }
 
 static inline void do_it_virt(struct task_struct * p, unsigned long ticks)
 {
+#ifndef CONFIG_PREEMPT_REALTIME
 	unsigned long it_virt = p->it_virt_value;
 
 	if (it_virt) {
@@ -836,10 +859,12 @@
 		}
 		p->it_virt_value = it_virt;
 	}
+#endif
 }
 
 static inline void do_it_prof(struct task_struct *p)
 {
+#ifndef CONFIG_PREEMPT_REALTIME
 	unsigned long it_prof = p->it_prof_value;
 
 	if (it_prof) {
@@ -849,6 +874,7 @@
 		}
 		p->it_prof_value = it_prof;
 	}
+#endif
 }
 
 static void update_one_process(struct task_struct *p, unsigned long user,
@@ -919,7 +945,7 @@
  * playing with xtime and avenrun.
  */
 #ifndef ARCH_HAVE_XTIME_LOCK
-seqlock_t xtime_lock __cacheline_aligned_in_smp = SEQLOCK_UNLOCKED;
+DECLARE_RAW_SEQLOCK(xtime_lock);
 
 EXPORT_SYMBOL(xtime_lock);
 #endif
@@ -1332,6 +1358,8 @@
        
 	base = &per_cpu(tvec_bases, cpu);
 	spin_lock_init(&base->lock);
+	init_waitqueue_head(&base->wait_for_running_timer);
+
 	for (j = 0; j < TVN_SIZE; j++) {
 		INIT_LIST_HEAD(base->tv5.vec + j);
 		INIT_LIST_HEAD(base->tv4.vec + j);
@@ -1446,7 +1474,7 @@
 
 struct time_interpolator *time_interpolator;
 static struct time_interpolator *time_interpolator_list;
-static spinlock_t time_interpolator_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(time_interpolator_lock);
 
 static inline unsigned long time_interpolator_get_cycles(unsigned int src)
 {
Index: xx-sources/kernel/user.c
===================================================================
--- xx-sources.orig/kernel/user.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/user.c	2004-10-16 20:57:02.000000000 -0400
@@ -26,7 +26,10 @@
 
 static kmem_cache_t *uid_cachep;
 static struct list_head uidhash_table[UIDHASH_SZ];
-static spinlock_t uidhash_lock = SPIN_LOCK_UNLOCKED;
+/*
+ * Needs to be a raw spinlock, referenced from within the scheduler:
+ */
+static DECLARE_RAW_SPINLOCK(uidhash_lock);
 
 struct user_struct root_user = {
 	.__count	= ATOMIC_INIT(1),
Index: xx-sources/kernel/workqueue.c
===================================================================
--- xx-sources.orig/kernel/workqueue.c	2004-10-16 20:52:28.000000000 -0400
+++ xx-sources/kernel/workqueue.c	2004-10-16 20:57:02.000000000 -0400
@@ -37,7 +37,7 @@
  */
 struct cpu_workqueue_struct {
 
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	long remove_sequence;	/* Least-recently added (next to run) */
 	long insert_sequence;	/* Next to add */
@@ -64,7 +64,7 @@
 
 /* All the per-cpu workqueues on the system, for hotplug cpu to add/remove
    threads to each one as cpus come/go. */
-static spinlock_t workqueue_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_RAW_SPINLOCK(workqueue_lock);
 static LIST_HEAD(workqueues);
 
 /* If it's single threaded, it isn't in the list of workqueues. */
Index: xx-sources/kernel/xsched-sched.c
===================================================================
--- xx-sources.orig/kernel/xsched-sched.c	2004-10-16 20:52:27.000000000 -0400
+++ xx-sources/kernel/xsched-sched.c	2004-10-16 22:00:12.433369504 -0400
@@ -41,6 +41,7 @@
 #include <linux/syscalls.h>
 #include <linux/times.h>
 #include <linux/sysctl.h>
+#include <linux/kallsyms.h>
 
 #include <asm/tlb.h>
 
@@ -162,7 +163,7 @@
  * acquire operations must be ordered by ascending &runqueue.
  */
 struct runqueue {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
@@ -1310,6 +1311,7 @@
 	 */
 	prev_task_flags = prev->flags;
 	finish_arch_switch(rq, prev);
+	preempt_enable_no_resched();
 	if (mm)
 		mmdrop(mm);
 	if (unlikely(prev_task_flags & PF_DEAD))
@@ -2387,17 +2389,21 @@
 	unsigned long run_time = 0;
 	int cpu;
 
+	WARN_ON(system_state == SYSTEM_BOOTING);
 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
 	 * schedule() atomically, we ignore that path for now.
 	 * Otherwise, whine if we are scheduling when we should not be.
 	 */
 	if (unlikely(in_atomic()) &&
-			likely(!(current->state & (TASK_DEAD | TASK_ZOMBIE)))) {
+			likely(!(current->exit_state & (__TASK_DEAD | __TASK_ZOMBIE)))) {
+			stop_trace();
 			printk(KERN_ERR "scheduling while atomic: "
 				"%s/0x%08x/%d\n",
 				current->comm, preempt_count(), current->pid);
-		dump_stack();
+			print_symbol("caller is %s\n",
+				(long)__builtin_return_address(0));
+			dump_stack();
 	}
 
 need_resched:
@@ -2423,6 +2429,8 @@
 
 	spin_lock_irq(&rq->lock);
 
+	if (unlikely(current->flags & PF_DEAD))
+		current->state = __TASK_DEAD;
 	/*
 	 * if entering off of a kernel preemption go straight
 	 * to picking the next task.
@@ -2528,12 +2536,14 @@
 		prev = context_switch(rq, prev, next);
 		barrier();
 
+		// re-enables preemption
 		finish_task_switch(prev);
-	} else
+	} else {
 		spin_unlock_irq(&rq->lock);
+		preempt_enable_no_resched();
+	}
 
 	reacquire_kernel_sem(current);
-	preempt_enable_no_resched();
 	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
 		goto need_resched;
 }
@@ -3443,7 +3453,7 @@
 	struct runqueue *rq = cpu_rq(dead_cpu);
 
 	/* Must be exiting, otherwise would be on tasklist. */
-	BUG_ON(tsk->state != TASK_ZOMBIE && tsk->state != TASK_DEAD);
+	BUG_ON(tsk->exit_state != __TASK_ZOMBIE && tsk->exit_state != __TASK_DEAD);
 
 	/* Cannot have done final schedule yet: would have vanished. */
 	BUG_ON(tsk->flags & PF_DEAD);
Index: xx-sources/lib/Kconfig.debug
===================================================================
--- xx-sources.orig/lib/Kconfig.debug	2004-10-16 20:56:45.000000000 -0400
+++ xx-sources/lib/Kconfig.debug	2004-10-16 20:57:02.000000000 -0400
@@ -36,21 +36,16 @@
 	  allocation as well as poisoning memory on free to catch use of freed
 	  memory.
 
-config DEBUG_SPINLOCK
-	bool "Spinlock debugging"
-	depends on DEBUG_KERNEL && (ALPHA || ARM || X86 || IA64 || MIPS || PPC32 || (SUPERH && !SUPERH64) || SPARC32 || SPARC64 || USERMODE || X86_64)
-	help
-	  Say Y here and build SMP to catch missing spinlock initialization
-	  and certain other kinds of spinlock errors commonly made.  This is
-	  best used in conjunction with the NMI watchdog so that spinlock
-	  deadlocks are also debuggable.
+# broken for the time being
 
-config DEBUG_SPINLOCK_SLEEP
-	bool "Sleep-inside-spinlock checking"
-	depends on DEBUG_KERNEL && (X86 || IA64 || MIPS || PPC32 || PPC64 || ARCH_S390 || SPARC32 || SPARC64)
-	help
-	  If you say Y here, various routines which may sleep will become very
-	  noisy if they are called with a spinlock held.
+# config DEBUG_SPINLOCK
+#	bool "Spinlock debugging"
+#	depends on DEBUG_KERNEL && (ALPHA || ARM || X86 || IA64 || MIPS || PPC32 || (SUPERH && !SUPERH64) || SPARC32 || SPARC64 || USERMODE || X86_64)
+#	help
+#	  Say Y here and build SMP to catch missing spinlock initialization
+#	  and certain other kinds of spinlock errors commonly made.  This is
+#	  best used in conjunction with the NMI watchdog so that spinlock
+#	  deadlocks are also debuggable.
 
 config DEBUG_PREEMPT
 	bool "Debug preemptible kernel"
@@ -60,7 +55,44 @@
 	  If you say Y here then the kernel will use a debug variant of the
 	  commonly used smp_processor_id() function and will print warnings
 	  if kernel code uses it in a preemption-unsafe way. Also, the kernel
-	  will detect preemption count underflows.
+	  will detect preemption count underflows and will trace critical
+	  section entries and print that info when an illegal sleep happens.
+
+config DEBUG_SPINLOCK_SLEEP
+	bool "Sleep-inside-spinlock checking"
+	default y
+	depends on DEBUG_KERNEL && !DEBUG_PREEMPT && (X86 || IA64 || MIPS || PPC32 || PPC64 || ARCH_S390 || SPARC32 || SPARC64)
+	help
+	  If you say Y here, various routines which may sleep will become very
+	  noisy if they are called with a spinlock held.
+
+config PREEMPT_TIMING
+	bool "Non-preemptible critical section timing"
+	default y
+	depends on PREEMPT
+	help
+	  This option measures the time spent in non-preemptible critical
+	  sections and reports warnings when a boot-time configurable
+	  latency threshold is exceeded.
+
+config PREEMPT_TRACE
+	bool
+	default y
+	depends on DEBUG_PREEMPT
+
+config LATENCY_TRACE
+	bool "Non-preemptible critical section tracing"
+	default n
+	depends on PREEMPT_TIMING
+	help
+	  This option enables a kernel tracing mechanism that will track
+	  precise kernel execution during critical sections. Note that
+	  kernel size and overhead increases noticeably with this option.
+
+config MCOUNT
+	bool
+	depends on LATENCY_TRACE
+	default y
 
 config DEBUG_HIGHMEM
 	bool "Highmem debugging"
@@ -101,7 +133,8 @@
 if !X86_64
 config FRAME_POINTER
 	bool "Compile the kernel with frame pointers"
-	depends on X86 || CRIS || M68KNOMMU || PARISC
+	depends on (X86 || CRIS || M68KNOMMU || PARISC) && !MCOUNT
+	default y
 	help
 	  If you say Y here the resulting kernel image will be slightly larger
 	  and slower, but it will give very useful debugging information.
Index: xx-sources/lib/dec_and_lock.c
===================================================================
--- xx-sources.orig/lib/dec_and_lock.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/lib/dec_and_lock.c	2004-10-16 20:57:02.000000000 -0400
@@ -27,14 +27,14 @@
  */
 
 #ifndef ATOMIC_DEC_AND_LOCK
-int atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock)
+int _atomic_dec_and_spin_lock(atomic_t *atomic, raw_spinlock_t *lock)
 {
-	spin_lock(lock);
+	_spin_lock(lock);
 	if (atomic_dec_and_test(atomic))
 		return 1;
-	spin_unlock(lock);
+	_spin_unlock(lock);
 	return 0;
 }
 
-EXPORT_SYMBOL(atomic_dec_and_lock);
+EXPORT_SYMBOL(_atomic_dec_and_spin_lock);
 #endif
Index: xx-sources/lib/radix-tree.c
===================================================================
--- xx-sources.orig/lib/radix-tree.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/lib/radix-tree.c	2004-10-16 20:57:02.000000000 -0400
@@ -103,6 +103,8 @@
 	kmem_cache_free(radix_tree_node_cachep, node);
 }
 
+#ifndef CONFIG_PREEMPT_REALTIME
+
 /*
  * Load up this CPU's radix_tree_node buffer with sufficient objects to
  * ensure that the addition of a single element in the tree cannot fail.  On
@@ -135,6 +137,8 @@
 }
 EXPORT_SYMBOL(radix_tree_preload);
 
+#endif
+
 static inline void tag_set(struct radix_tree_node *node, int tag, int offset)
 {
 	if (!test_bit(offset, &node->tags[tag][0]))
Index: xx-sources/mm/highmem.c
===================================================================
--- xx-sources.orig/mm/highmem.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/highmem.c	2004-10-16 20:57:02.000000000 -0400
@@ -53,7 +53,7 @@
 #ifdef CONFIG_HIGHMEM
 static int pkmap_count[LAST_PKMAP];
 static unsigned int last_pkmap_nr;
-static spinlock_t kmap_lock __cacheline_aligned_in_smp = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(kmap_lock);
 
 pte_t * pkmap_page_table;
 
@@ -500,7 +500,7 @@
  * page_address_map freelist, allocated from page_address_maps.
  */
 static struct list_head page_address_pool;	/* freelist */
-static spinlock_t pool_lock;			/* protects page_address_pool */
+static DECLARE_SPINLOCK(pool_lock);		/* protects page_address_pool */
 
 /*
  * Hash table bucket
Index: xx-sources/mm/memory.c
===================================================================
--- xx-sources.orig/mm/memory.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/memory.c	2004-10-16 20:57:02.000000000 -0400
@@ -114,7 +114,7 @@
 	page = pmd_page(*dir);
 	pmd_clear(dir);
 	dec_page_state(nr_page_table_pages);
-	tlb->mm->nr_ptes--;
+	tlb_mm(tlb)->nr_ptes--;
 	pte_free_tlb(tlb, page);
 }
 
@@ -145,7 +145,7 @@
  */
 void clear_page_tables(struct mmu_gather *tlb, unsigned long first, int nr)
 {
-	pgd_t * page_dir = tlb->mm->pgd;
+	pgd_t * page_dir = tlb_mm(tlb)->pgd;
 
 	page_dir += first;
 	do {
@@ -445,7 +445,7 @@
 				set_page_dirty(page);
 			if (pte_young(pte) && !PageAnon(page))
 				mark_page_accessed(page);
-			tlb->freed++;
+			tlb_free(tlb);
 			page_remove_rmap(page);
 			tlb_remove_page(tlb, page);
 			continue;
Index: xx-sources/mm/mmap.c
===================================================================
--- xx-sources.orig/mm/mmap.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/mmap.c	2004-10-16 20:57:02.000000000 -0400
@@ -1501,7 +1501,7 @@
 	unsigned long first = start & PGDIR_MASK;
 	unsigned long last = end + PGDIR_SIZE - 1;
 	unsigned long start_index, end_index;
-	struct mm_struct *mm = tlb->mm;
+	struct mm_struct *mm = tlb_mm(tlb);
 
 	if (!prev) {
 		prev = mm->mmap;
Index: xx-sources/mm/rmap.c
===================================================================
--- xx-sources.orig/mm/rmap.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/rmap.c	2004-10-16 20:57:02.000000000 -0400
@@ -189,8 +189,8 @@
  */
 static struct anon_vma *page_lock_anon_vma(struct page *page)
 {
-	struct anon_vma *anon_vma = NULL;
 	unsigned long anon_mapping;
+	struct anon_vma *anon_vma;
 
 	rcu_read_lock();
 	anon_mapping = (unsigned long) page->mapping;
@@ -200,10 +200,13 @@
 		goto out;
 
 	anon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);
+	rcu_read_unlock();
 	spin_lock(&anon_vma->lock);
+
+	return anon_vma;
 out:
 	rcu_read_unlock();
-	return anon_vma;
+	return NULL;
 }
 
 /*
Index: xx-sources/mm/shmem.c
===================================================================
--- xx-sources.orig/mm/shmem.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/shmem.c	2004-10-16 20:57:02.000000000 -0400
@@ -100,17 +100,17 @@
 
 static struct page **shmem_dir_map(struct page *page)
 {
-	return (struct page **)kmap_atomic(page, KM_USER0);
+	return (struct page **)kmap_atomic_rt(page, KM_USER0);
 }
 
 static inline void shmem_dir_unmap(struct page **dir)
 {
-	kunmap_atomic(dir, KM_USER0);
+	kunmap_atomic_rt(dir, KM_USER0);
 }
 
 static swp_entry_t *shmem_swp_map(struct page *page)
 {
-	return (swp_entry_t *)kmap_atomic(page, KM_USER1);
+	return (swp_entry_t *)kmap_atomic_rt(page, KM_USER1);
 }
 
 static inline void shmem_swp_balance_unmap(void)
@@ -119,15 +119,15 @@
 	 * When passing a pointer to an i_direct entry, to code which
 	 * also handles indirect entries and so will shmem_swp_unmap,
 	 * we must arrange for the preempt count to remain in balance.
-	 * What kmap_atomic of a lowmem page does depends on config
-	 * and architecture, so pretend to kmap_atomic some lowmem page.
+	 * What kmap_atomic_rt of a lowmem page does depends on config
+	 * and architecture, so pretend to kmap_atomic_rt some lowmem page.
 	 */
-	(void) kmap_atomic(ZERO_PAGE(0), KM_USER1);
+	(void) kmap_atomic_rt(ZERO_PAGE(0), KM_USER1);
 }
 
 static inline void shmem_swp_unmap(swp_entry_t *entry)
 {
-	kunmap_atomic(entry, KM_USER1);
+	kunmap_atomic_rt(entry, KM_USER1);
 }
 
 static inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)
@@ -330,7 +330,7 @@
 	entry->val = value;
 	info->swapped += incdec;
 	if ((unsigned long)(entry - info->i_direct) >= SHMEM_NR_DIRECT)
-		kmap_atomic_to_page(entry)->nr_swapped += incdec;
+		kmap_atomic_to_page_rt(entry)->nr_swapped += incdec;
 }
 
 /*
@@ -757,7 +757,7 @@
 	struct shmem_inode_info *info;
 	swp_entry_t *entry, swap;
 	struct address_space *mapping;
-	unsigned long index;
+	pgoff_t index;
 	struct inode *inode;
 
 	BUG_ON(!PageLocked(page));
@@ -769,7 +769,7 @@
 	info = SHMEM_I(inode);
 	if (info->flags & VM_LOCKED)
 		goto redirty;
-	swap = get_swap_page();
+	swap = get_swap_page(mapping, index);
 	if (!swap.val)
 		goto redirty;
 
@@ -1359,10 +1359,10 @@
 			__get_user(dummy, buf);
 			__get_user(dummy, buf + bytes - 1);
 
-			kaddr = kmap_atomic(page, KM_USER0);
+			kaddr = kmap_atomic_rt(page, KM_USER0);
 			left = __copy_from_user_inatomic(kaddr + offset,
 							buf, bytes);
-			kunmap_atomic(kaddr, KM_USER0);
+			kunmap_atomic_rt(kaddr, KM_USER0);
 		}
 		if (left) {
 			kaddr = kmap(page);
@@ -1716,9 +1716,9 @@
 			return error;
 		}
 		inode->i_op = &shmem_symlink_inode_operations;
-		kaddr = kmap_atomic(page, KM_USER0);
+		kaddr = kmap_atomic_rt(page, KM_USER0);
 		memcpy(kaddr, symname, len);
-		kunmap_atomic(kaddr, KM_USER0);
+		kunmap_atomic_rt(kaddr, KM_USER0);
 		set_page_dirty(page);
 		page_cache_release(page);
 	}
Index: xx-sources/mm/slab.c
===================================================================
--- xx-sources.orig/mm/slab.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/slab.c	2004-10-16 20:57:02.000000000 -0400
@@ -292,7 +292,7 @@
 	unsigned int	 	flags;	/* constant flags */
 	unsigned int		num;	/* # of objs per slab */
 	unsigned int		free_limit; /* upper limit of objects in the lists */
-	spinlock_t		spinlock;
+	raw_spinlock_t		spinlock;
 
 /* 3) cache_grow/shrink */
 	/* order of pgs per slab (2^n) */
@@ -519,7 +519,7 @@
 	.limit		= BOOT_CPUCACHE_ENTRIES,
 	.objsize	= sizeof(kmem_cache_t),
 	.flags		= SLAB_NO_REAP,
-	.spinlock	= SPIN_LOCK_UNLOCKED,
+	.spinlock	= RAW_SPIN_LOCK_UNLOCKED,
 	.name		= "kmem_cache",
 #if DEBUG
 	.reallen	= sizeof(kmem_cache_t),
@@ -2708,6 +2708,10 @@
 	if (limit > 32)
 		limit = 32;
 #endif
+#ifdef CONFIG_PREEMPT
+	if (limit > 16)
+		limit = 16;
+#endif
 	err = do_tune_cpucache(cachep, limit, (limit+1)/2, shared);
 	if (err)
 		printk(KERN_ERR "enable_cpucache failed for %s, error %d.\n",
Index: xx-sources/mm/swap.c
===================================================================
--- xx-sources.orig/mm/swap.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/swap.c	2004-10-16 22:00:23.649664368 -0400
@@ -136,39 +136,45 @@
  * lru_cache_add: add a page to the page lists
  * @page: the page to add
  */
-static DEFINE_PER_CPU(struct pagevec, lru_add_pvecs) = { 0, };
-static DEFINE_PER_CPU(struct pagevec, lru_add_active_pvecs) = { 0, };
+static DEFINE_PER_CPU_LOCKED(struct pagevec, lru_add_pvecs) = { 0, };
+static DEFINE_PER_CPU_LOCKED(struct pagevec, lru_add_active_pvecs) = { 0, };
 
 void fastcall lru_cache_add(struct page *page)
 {
-	struct pagevec *pvec = &get_cpu_var(lru_add_pvecs);
+	int cpu = _smp_processor_id();
+	struct pagevec *pvec = &get_cpu_var_locked(lru_add_pvecs, cpu);
 
 	page_cache_get(page);
 	if (!pagevec_add(pvec, page))
 		__pagevec_lru_add(pvec);
-	put_cpu_var(lru_add_pvecs);
+	put_cpu_var_locked(lru_add_pvecs, cpu);
 }
 
 void fastcall lru_cache_add_active(struct page *page)
 {
-	struct pagevec *pvec = &get_cpu_var(lru_add_active_pvecs);
+	int cpu = _smp_processor_id();
+	struct pagevec *pvec = &get_cpu_var_locked(lru_add_active_pvecs, cpu);
 
 	page_cache_get(page);
 	if (!pagevec_add(pvec, page))
 		__pagevec_lru_add_active(pvec);
-	put_cpu_var(lru_add_active_pvecs);
+	put_cpu_var_locked(lru_add_active_pvecs, cpu);
 }
 
 void lru_add_drain(void)
 {
-	struct pagevec *pvec = &get_cpu_var(lru_add_pvecs);
+	int cpu = _smp_processor_id();
+	struct pagevec *pvec;
 
+	pvec = &get_cpu_var_locked(lru_add_pvecs, cpu);
 	if (pagevec_count(pvec))
 		__pagevec_lru_add(pvec);
-	pvec = &__get_cpu_var(lru_add_active_pvecs);
+	put_cpu_var_locked(lru_add_pvecs, cpu);
+
+	pvec = &get_cpu_var_locked(lru_add_active_pvecs, cpu);
 	if (pagevec_count(pvec))
 		__pagevec_lru_add_active(pvec);
-	put_cpu_var(lru_add_pvecs);
+	put_cpu_var_locked(lru_add_active_pvecs, cpu);
 }
 
 /*
Index: xx-sources/mm/swap_state.c
===================================================================
--- xx-sources.orig/mm/swap_state.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/swap_state.c	2004-10-16 20:57:02.000000000 -0400
@@ -137,8 +137,12 @@
  *
  * Allocate swap space for the page and add the page to the
  * swap cache.  Caller needs to hold the page lock. 
+ *
+ * We attempt to lay pages out on swap to that virtually-contiguous pages are
+ * contiguous on-disk.  To do this we utilise page->index (offset into vma) and
+ * page->mapping (the anon_vma's address).
  */
-int add_to_swap(struct page * page)
+int add_to_swap(struct page *page, void *cookie, pgoff_t index)
 {
 	swp_entry_t entry;
 	int pf_flags;
@@ -148,7 +152,7 @@
 		BUG();
 
 	for (;;) {
-		entry = get_swap_page();
+		entry = get_swap_page(cookie, index);
 		if (!entry.val)
 			return 0;
 
Index: xx-sources/mm/swapfile.c
===================================================================
--- xx-sources.orig/mm/swapfile.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/swapfile.c	2004-10-16 20:57:02.000000000 -0400
@@ -25,6 +25,7 @@
 #include <linux/rmap.h>
 #include <linux/security.h>
 #include <linux/backing-dev.h>
+#include <linux/hash.h>
 #include <linux/syscalls.h>
 
 #include <asm/pgtable.h>
@@ -84,71 +85,51 @@
 	up_read(&swap_unplug_sem);
 }
 
-static inline int scan_swap_map(struct swap_info_struct *si)
-{
-	unsigned long offset;
-	/* 
-	 * We try to cluster swap pages by allocating them
-	 * sequentially in swap.  Once we've allocated
-	 * SWAPFILE_CLUSTER pages this way, however, we resort to
-	 * first-free allocation, starting a new cluster.  This
-	 * prevents us from scattering swap pages all over the entire
-	 * swap partition, so that we reduce overall disk seek times
-	 * between swap pages.  -- sct */
-	if (si->cluster_nr) {
-		while (si->cluster_next <= si->highest_bit) {
-			offset = si->cluster_next++;
-			if (si->swap_map[offset])
-				continue;
-			si->cluster_nr--;
-			goto got_page;
-		}
-	}
-	si->cluster_nr = SWAPFILE_CLUSTER;
+int akpm;
 
-	/* try to find an empty (even not aligned) cluster. */
-	offset = si->lowest_bit;
- check_next_cluster:
-	if (offset+SWAPFILE_CLUSTER-1 <= si->highest_bit)
-	{
-		unsigned long nr;
-		for (nr = offset; nr < offset+SWAPFILE_CLUSTER; nr++)
-			if (si->swap_map[nr])
-			{
-				offset = nr+1;
-				goto check_next_cluster;
-			}
-		/* We found a completly empty cluster, so start
-		 * using it.
-		 */
-		goto got_page;
-	}
-	/* No luck, so now go finegrined as usual. -Andrea */
-	for (offset = si->lowest_bit; offset <= si->highest_bit ; offset++) {
-		if (si->swap_map[offset])
+/*
+ * We divide the swapdev into 1024 kilobyte chunks.  We use the cookie and the
+ * upper bits of the index to select a chunk and the rest of the index as the
+ * offset into the selected chunk.
+ */
+#define CHUNK_SHIFT	(20 - PAGE_SHIFT)
+#define CHUNK_MASK	(-1UL << CHUNK_SHIFT)
+
+static int
+scan_swap_map(struct swap_info_struct *si, void *cookie, pgoff_t index)
+{
+	unsigned long chunk;
+	unsigned long nchunks;
+	unsigned long block;
+	unsigned long scan;
+
+	nchunks = si->max >> CHUNK_SHIFT;
+	chunk = 0;
+	if (nchunks)
+		chunk = hash_long((unsigned long)cookie + (index & CHUNK_MASK),
+					BITS_PER_LONG) % nchunks;
+
+	block = (chunk << CHUNK_SHIFT) + (index & ~CHUNK_MASK);
+
+	for (scan = 0; scan < si->max; scan++, block++) {
+		if (block == si->max)
+			block = 0;
+		if (block == 0)
 			continue;
-		si->lowest_bit = offset+1;
-	got_page:
-		if (offset == si->lowest_bit)
-			si->lowest_bit++;
-		if (offset == si->highest_bit)
-			si->highest_bit--;
-		if (si->lowest_bit > si->highest_bit) {
-			si->lowest_bit = si->max;
-			si->highest_bit = 0;
-		}
-		si->swap_map[offset] = 1;
-		si->inuse_pages++;
+		if (si->swap_map[block])
+			continue;
+		si->swap_map[block] = 1;
 		nr_swap_pages--;
-		si->cluster_next = offset+1;
-		return offset;
+		if (akpm)
+			printk("cookie:%p, index:%lu, chunk:%lu nchunks:%lu "
+				"block:%lu\n",
+				cookie, index, chunk, nchunks, block);
+		return block;
 	}
-	si->lowest_bit = si->max;
-	si->highest_bit = 0;
 	return 0;
 }
 
-swp_entry_t get_swap_page(void)
+swp_entry_t get_swap_page(void *cookie, pgoff_t index)
 {
 	struct swap_info_struct * p;
 	unsigned long offset;
@@ -167,7 +148,7 @@
 		p = &swap_info[type];
 		if ((p->flags & SWP_ACTIVE) == SWP_ACTIVE) {
 			swap_device_lock(p);
-			offset = scan_swap_map(p);
+			offset = scan_swap_map(p, cookie, index);
 			swap_device_unlock(p);
 			if (offset) {
 				entry = swp_entry(type,offset);
Index: xx-sources/mm/vmscan.c
===================================================================
--- xx-sources.orig/mm/vmscan.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/mm/vmscan.c	2004-10-16 20:57:02.000000000 -0400
@@ -391,7 +391,10 @@
 		 * Try to allocate it some swap space here.
 		 */
 		if (PageAnon(page) && !PageSwapCache(page)) {
-			if (!add_to_swap(page))
+			void *cookie = page->mapping;
+			pgoff_t index = page->index;
+
+			if (!add_to_swap(page, cookie, index))
 				goto activate_locked;
 		}
 #endif /* CONFIG_SWAP */
Index: xx-sources/net/core/dev.c
===================================================================
--- xx-sources.orig/net/core/dev.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/core/dev.c	2004-10-16 20:57:02.000000000 -0400
@@ -1067,7 +1067,7 @@
 	struct packet_type *ptype;
 	net_timestamp(&skb->stamp);
 
-	rcu_read_lock();
+	rcu_read_lock_spin(&ptype_lock);
 	list_for_each_entry_rcu(ptype, &ptype_all, list) {
 		/* Never send packets back to the socket
 		 * they originated from - MvS (miquels@drinkel.ow.org)
@@ -1099,7 +1099,7 @@
 			ptype->func(skb2, skb->dev, ptype);
 		}
 	}
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&ptype_lock);
 }
 
 /*
@@ -1332,11 +1332,20 @@
 	   Check this and shot the lock. It is not prone from deadlocks.
 	   Either shot noqueue qdisc, it is even simpler 8)
 	 */
+#ifndef CONFIG_PREEMPT_REALTIME
+	preempt_disable();
+#endif
 	if (dev->flags & IFF_UP) {
-		int cpu = smp_processor_id(); /* ok because BHs are off */
+		int cpu = _smp_processor_id(); /* ok because BHs are off */
 
+		/*
+		 * No need to check for recursion with threaded interrupts:
+		 */
+#ifdef CONFIG_PREEMPT_REALTIME
+		if (1) {
+#else
 		if (dev->xmit_lock_owner != cpu) {
-
+#endif
 			HARD_TX_LOCK(dev, cpu);
 
 			if (!netif_queue_stopped(dev)) {
@@ -1346,6 +1355,9 @@
 				rc = 0;
 				if (!dev->hard_start_xmit(skb, dev)) {
 					HARD_TX_UNLOCK(dev);
+#ifndef CONFIG_PREEMPT_REALTIME
+					preempt_enable();
+#endif
 					goto out;
 				}
 			}
@@ -1363,6 +1375,9 @@
 		}
 	}
 out_enetdown:
+#ifndef CONFIG_PREEMPT_REALTIME
+	preempt_enable();
+#endif
 	rc = -ENETDOWN;
 out_kfree_skb:
 	kfree_skb(skb);
@@ -1614,6 +1629,11 @@
 
 			BUG_TRAP(!atomic_read(&skb->users));
 			__kfree_skb(skb);
+			/*
+			 * Safe to reschedule - the list is private
+			 * at this point.
+			 */
+			cond_resched_all();
 		}
 	}
 
@@ -1738,7 +1758,7 @@
 
 	pt_prev = NULL;
 
-	rcu_read_lock();
+	rcu_read_lock_spin(&ptype_lock);
 
 #ifdef CONFIG_NET_CLS_ACT
 	if (skb->tc_verd & TC_NCLS) {
@@ -1800,7 +1820,7 @@
 	}
 
 out:
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&ptype_lock);
 	return ret;
 }
 
@@ -1829,7 +1849,7 @@
 
 		work++;
 
-		if (work >= quota || jiffies - start_time > 1)
+		if (work >= quota || jiffies - start_time > 1 || softirq_need_resched())
 			break;
 
 #ifdef CONFIG_NET_HW_FLOWCONTROL
@@ -1869,11 +1889,12 @@
 
 static void net_rx_action(struct softirq_action *h)
 {
-	struct softnet_data *queue = &__get_cpu_var(softnet_data);
+	struct softnet_data *queue;
 	unsigned long start_time = jiffies;
 	int budget = netdev_max_backlog;
 
 	local_irq_disable();
+	queue = &__get_cpu_var(softnet_data);
 
 	while (!list_empty(&queue->poll_list)) {
 		struct net_device *dev;
@@ -1883,6 +1904,10 @@
 
 		local_irq_enable();
 
+		if (unlikely(cond_resched_all())) {
+			local_irq_disable();
+			continue;
+		}
 		dev = list_entry(queue->poll_list.next,
 				 struct net_device, poll_list);
 
@@ -1908,8 +1933,10 @@
 	return;
 
 softnet_break:
+	preempt_disable();
 	__get_cpu_var(netdev_rx_stat).time_squeeze++;
 	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
+	preempt_enable();
 	goto out;
 }
 
Index: xx-sources/net/core/netfilter.c
===================================================================
--- xx-sources.orig/net/core/netfilter.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/core/netfilter.c	2004-10-16 20:57:02.000000000 -0400
@@ -47,7 +47,7 @@
 
 struct list_head nf_hooks[NPROTO][NF_MAX_HOOKS];
 static LIST_HEAD(nf_sockopts);
-static spinlock_t nf_hook_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(nf_hook_lock);
 
 /* 
  * A queue handler may be registered for each protocol.  Each is protected by
@@ -504,8 +504,11 @@
 	unsigned int verdict;
 	int ret = 0;
 
+#ifdef CONFIG_PREEMPT_REALTIME
+	might_sleep();
+#endif
 	/* We may already have this, but read-locks nest anyway */
-	rcu_read_lock();
+	rcu_read_lock_spin(&nf_hook_lock);
 
 #ifdef CONFIG_NETFILTER_DEBUG
 	if (skb->nf_debug & (1 << hook)) {
@@ -527,16 +530,23 @@
 
 	switch (verdict) {
 	case NF_ACCEPT:
+		/*
+		 * FIXME: is this safe? We drop the lock to avoid
+		 * recursion when PREEMPT_REALTIME:
+		 */
+		rcu_read_unlock_spin(&nf_hook_lock);
 		ret = okfn(skb);
 		break;
 
 	case NF_DROP:
+		rcu_read_unlock_spin(&nf_hook_lock);
 		kfree_skb(skb);
 		ret = -EPERM;
 		break;
+	default:
+		rcu_read_unlock_spin(&nf_hook_lock);
 	}
 
-	rcu_read_unlock();
 	return ret;
 }
 
@@ -546,7 +556,7 @@
 	struct list_head *elem = &info->elem->list;
 	struct list_head *i;
 
-	rcu_read_lock();
+	rcu_read_lock_spin(&nf_hook_lock);
 
 	/* Release those devices we held, or Alexey will kill me. */
 	if (info->indev) dev_put(info->indev);
@@ -600,7 +610,7 @@
 			goto next_hook;
 		break;
 	}
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&nf_hook_lock);
 
 	if (verdict == NF_DROP)
 		kfree_skb(skb);
@@ -783,7 +793,7 @@
 	char prefix[NF_LOG_PREFIXLEN];
 	nf_logfn *logfn;
 	
-	rcu_read_lock();
+	rcu_read_lock_spin(&nf_hook_lock);
 	logfn = rcu_dereference(nf_logging[pf]);
 	if (logfn) {
 		va_start(args, fmt);
@@ -796,7 +806,7 @@
 		       "no backend logging module loaded in!\n");
 		reported++;
 	}
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&nf_hook_lock);
 }
 EXPORT_SYMBOL(nf_log_register);
 EXPORT_SYMBOL(nf_log_unregister);
Index: xx-sources/net/core/netpoll.c
===================================================================
--- xx-sources.orig/net/core/netpoll.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/core/netpoll.c	2004-10-16 20:57:02.000000000 -0400
@@ -608,18 +608,18 @@
 		memcpy(np->local_mac, ndev->dev_addr, 6);
 
 	if (!np->local_ip) {
-		rcu_read_lock();
+		rcu_read_lock_sem(&rtnl_sem);
 		in_dev = __in_dev_get(ndev);
 
 		if (!in_dev) {
-			rcu_read_unlock();
+			rcu_read_unlock_sem(&rtnl_sem);
 			printk(KERN_ERR "%s: no IP address for %s, aborting\n",
 			       np->name, np->dev_name);
 			goto release;
 		}
 
 		np->local_ip = ntohl(in_dev->ifa_list->ifa_local);
-		rcu_read_unlock();
+		rcu_read_unlock_sem(&rtnl_sem);
 		printk(KERN_INFO "%s: local IP %d.%d.%d.%d\n",
 		       np->name, HIPQUAD(np->local_ip));
 	}
Index: xx-sources/net/core/pktgen.c
===================================================================
--- xx-sources.orig/net/core/pktgen.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/core/pktgen.c	2004-10-16 20:57:02.000000000 -0400
@@ -269,7 +269,7 @@
 	if (strlen(info->src_min) == 0) {
 		struct in_device *in_dev;
 
-		rcu_read_lock();
+		rcu_read_lock_sem(&rtnl_sem);
 		in_dev = __in_dev_get(odev);
 		if (in_dev) {
 			if (in_dev->ifa_list) {
@@ -277,7 +277,7 @@
 				info->saddr_max = info->saddr_min;
 			}
 		}
-		rcu_read_unlock();
+		rcu_read_unlock_sem(&rtnl_sem);
 	}
 	else {
 		info->saddr_min = in_aton(info->src_min);
Index: xx-sources/net/ipv4/af_inet.c
===================================================================
--- xx-sources.orig/net/ipv4/af_inet.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/af_inet.c	2004-10-16 20:57:02.000000000 -0400
@@ -242,7 +242,7 @@
 
 	/* Look for the requested type/protocol pair. */
 	answer = NULL;
-	rcu_read_lock();
+	rcu_read_lock_spin(&inetsw_lock);
 	list_for_each_rcu(p, &inetsw[sock->type]) {
 		answer = list_entry(p, struct inet_protosw, list);
 
@@ -276,7 +276,7 @@
 	answer_prot = answer->prot;
 	answer_no_check = answer->no_check;
 	answer_flags = answer->flags;
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&inetsw_lock);
 
 	BUG_TRAP(answer_prot->slab != NULL);
 
@@ -345,7 +345,7 @@
 out:
 	return err;
 out_rcu_unlock:
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&inetsw_lock);
 	goto out;
 }
 
Index: xx-sources/net/ipv4/arp.c
===================================================================
--- xx-sources.orig/net/ipv4/arp.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/arp.c	2004-10-16 20:57:02.000000000 -0400
@@ -237,17 +237,17 @@
 
 	neigh->type = inet_addr_type(addr);
 
-	rcu_read_lock();
+	rcu_read_lock_sem(&rtnl_sem);
 	in_dev = rcu_dereference(__in_dev_get(dev));
 	if (in_dev == NULL) {
-		rcu_read_unlock();
+		rcu_read_unlock_sem(&rtnl_sem);
 		return -EINVAL;
 	}
 
 	parms = in_dev->arp_parms;
 	__neigh_parms_put(neigh->parms);
 	neigh->parms = neigh_parms_clone(parms);
-	rcu_read_unlock();
+	rcu_read_unlock_sem(&rtnl_sem);
 
 	if (dev->hard_header == NULL) {
 		neigh->nud_state = NUD_NOARP;
Index: xx-sources/net/ipv4/fib_frontend.c
===================================================================
--- xx-sources.orig/net/ipv4/fib_frontend.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/fib_frontend.c	2004-10-16 20:57:02.000000000 -0400
@@ -172,13 +172,13 @@
 	int ret;
 
 	no_addr = rpf = 0;
-	rcu_read_lock();
+	rcu_read_lock_sem(&rtnl_sem);
 	in_dev = __in_dev_get(dev);
 	if (in_dev) {
 		no_addr = in_dev->ifa_list == NULL;
 		rpf = IN_DEV_RPFILTER(in_dev);
 	}
-	rcu_read_unlock();
+	rcu_read_unlock_sem(&rtnl_sem);
 
 	if (in_dev == NULL)
 		goto e_inval;
Index: xx-sources/net/ipv4/icmp.c
===================================================================
--- xx-sources.orig/net/ipv4/icmp.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/icmp.c	2004-10-16 20:57:02.000000000 -0400
@@ -699,11 +699,20 @@
 	}
 	read_unlock(&raw_v4_lock);
 
-	rcu_read_lock();
+	/*
+	 * All callers must have this lock taken already:
+	 */
+#ifdef CONFIG_PREEMPT_REALTIME
+	WARN_ON(!spin_is_locked(&inet_proto_lock));
+#else
+	rcu_read_lock_spin(&inet_proto_lock);
+#endif
 	ipprot = rcu_dereference(inet_protos[hash]);
 	if (ipprot && ipprot->err_handler)
 		ipprot->err_handler(skb, info);
-	rcu_read_unlock();
+#ifndef CONFIG_PREEMPT_REALTIME
+	rcu_read_unlock_spin(&inet_proto_lock);
+#endif
 
 out:
 	return;
@@ -881,7 +890,7 @@
 	in_dev = in_dev_get(dev);
 	if (!in_dev)
 		goto out;
-	rcu_read_lock();
+	rcu_read_lock_sem(&rtnl_sem);
 	if (in_dev->ifa_list &&
 	    IN_DEV_LOG_MARTIANS(in_dev) &&
 	    IN_DEV_FORWARD(in_dev)) {
@@ -901,7 +910,7 @@
 			       NIPQUAD(*mp), dev->name, NIPQUAD(rt->rt_src));
 		}
 	}
-	rcu_read_unlock();
+	rcu_read_unlock_sem(&rtnl_sem);
 	in_dev_put(in_dev);
 out:;
 }
Index: xx-sources/net/ipv4/inetpeer.c
===================================================================
--- xx-sources.orig/net/ipv4/inetpeer.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/inetpeer.c	2004-10-16 22:00:25.108442600 -0400
@@ -70,7 +70,7 @@
  */
 
 /* Exported for inet_getid inline function.  */
-spinlock_t inet_peer_idlock = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(inet_peer_idlock);
 
 static kmem_cache_t *peer_cachep;
 
@@ -95,7 +95,7 @@
 /* Exported for inet_putpeer inline function.  */
 struct inet_peer *inet_peer_unused_head,
 		**inet_peer_unused_tailp = &inet_peer_unused_head;
-spinlock_t inet_peer_unused_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(inet_peer_unused_lock);
 #define PEER_MAX_CLEANUP_WORK 30
 
 static void peer_check_expire(unsigned long dummy);
Index: xx-sources/net/ipv4/ip_input.c
===================================================================
--- xx-sources.orig/net/ipv4/ip_input.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/ip_input.c	2004-10-16 20:57:02.000000000 -0400
@@ -213,7 +213,7 @@
         /* Point into the IP datagram, just past the header. */
         skb->h.raw = skb->data;
 
-	rcu_read_lock();
+	rcu_read_lock_spin(&inet_proto_lock);
 	{
 		/* Note: See raw.c and net/raw.h, RAWV4_HTABLE_SIZE==MAX_INET_PROTOS */
 		int protocol = skb->nh.iph->protocol;
@@ -258,7 +258,7 @@
 		}
 	}
  out:
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&inet_proto_lock);
 
 	return 0;
 }
Index: xx-sources/net/ipv4/netfilter/ip_conntrack_core.c
===================================================================
--- xx-sources.orig/net/ipv4/netfilter/ip_conntrack_core.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/netfilter/ip_conntrack_core.c	2004-10-16 20:57:02.000000000 -0400
@@ -361,16 +361,18 @@
 {
 	struct ip_conntrack_tuple_hash *h;
 	unsigned int hash = hash_conntrack(tuple);
-	/* use per_cpu() to avoid multiple calls to smp_processor_id() */
-	unsigned int cpu = smp_processor_id();
 
 	MUST_BE_READ_LOCKED(&ip_conntrack_lock);
 	list_for_each_entry(h, &ip_conntrack_hash[hash], list) {
 		if (conntrack_tuple_cmp(h, tuple, ignored_conntrack)) {
-			per_cpu(ip_conntrack_stat, cpu).found++;
+			preempt_disable();
+			per_cpu(ip_conntrack_stat, smp_processor_id()).found++;
+			preempt_enable();
 			return h;
 		}
-		per_cpu(ip_conntrack_stat, cpu).searched++;
+		preempt_disable();
+		per_cpu(ip_conntrack_stat, smp_processor_id()).searched++;
+		preempt_enable();
 	}
 
 	return NULL;
Index: xx-sources/net/ipv4/netfilter/ip_tables.c
===================================================================
--- xx-sources.orig/net/ipv4/netfilter/ip_tables.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/netfilter/ip_tables.c	2004-10-16 20:57:03.000000000 -0400
@@ -287,10 +287,14 @@
 	 * match it. */
 	offset = ntohs(ip->frag_off) & IP_OFFSET;
 
+#ifdef CONFIG_PREEMPT_REALTIME
+	write_lock_bh(&table->lock);
+#else
 	read_lock_bh(&table->lock);
+#endif
 	IP_NF_ASSERT(table->valid_hooks & (1 << hook));
 	table_base = (void *)table->private->entries
-		+ TABLE_OFFSET(table->private, smp_processor_id());
+		+ TABLE_OFFSET(table->private, _smp_processor_id());
 	e = get_entry(table_base, table->private->hook_entry[hook]);
 
 #ifdef CONFIG_NETFILTER_DEBUG
@@ -397,7 +401,11 @@
 #ifdef CONFIG_NETFILTER_DEBUG
 	((struct ipt_entry *)table_base)->comefrom = 0xdead57ac;
 #endif
+#ifdef CONFIG_PREEMPT_REALTIME
+	write_unlock_bh(&table->lock);
+#else
 	read_unlock_bh(&table->lock);
+#endif
 
 #ifdef DEBUG_ALLOW_ALL
 	return NF_ACCEPT;
Index: xx-sources/net/ipv4/protocol.c
===================================================================
--- xx-sources.orig/net/ipv4/protocol.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/protocol.c	2004-10-16 22:00:25.167433632 -0400
@@ -49,7 +49,7 @@
 #include <linux/igmp.h>
 
 struct net_protocol *inet_protos[MAX_INET_PROTOS];
-static spinlock_t inet_proto_lock = SPIN_LOCK_UNLOCKED;
+DECLARE_SPINLOCK(inet_proto_lock);
 
 /*
  *	Add a protocol handler to the hash tables
Index: xx-sources/net/ipv4/route.c
===================================================================
--- xx-sources.orig/net/ipv4/route.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/route.c	2004-10-16 22:00:25.145436976 -0400
@@ -226,11 +226,11 @@
 	struct rt_cache_iter_state *st = seq->private;
 
 	for (st->bucket = rt_hash_mask; st->bucket >= 0; --st->bucket) {
-		rcu_read_lock_bh();
+		rcu_read_lock_bh_spin(&rt_hash_table[st->bucket].lock);
 		r = rt_hash_table[st->bucket].chain;
 		if (r)
 			break;
-		rcu_read_unlock_bh();
+		rcu_read_unlock_bh_spin(&rt_hash_table[st->bucket].lock);
 	}
 	return r;
 }
@@ -241,10 +241,10 @@
 
 	r = r->u.rt_next;
 	while (!r) {
-		rcu_read_unlock_bh();
+		rcu_read_unlock_bh_spin(&rt_hash_table[st->bucket].lock);
 		if (--st->bucket < 0)
 			break;
-		rcu_read_lock_bh();
+		rcu_read_lock_bh_spin(&rt_hash_table[st->bucket].lock);
 		r = rt_hash_table[st->bucket].chain;
 	}
 	return r;
@@ -279,8 +279,10 @@
 
 static void rt_cache_seq_stop(struct seq_file *seq, void *v)
 {
+	struct rt_cache_iter_state *st = rcu_dereference(seq->private);
+
 	if (v && v != SEQ_START_TOKEN)
-		rcu_read_unlock_bh();
+		rcu_read_lock_bh_spin(&rt_hash_table[st->bucket].lock);
 }
 
 static int rt_cache_seq_show(struct seq_file *seq, void *v)
@@ -574,6 +576,7 @@
 		if (rth)
 			rt_hash_table[i].chain = NULL;
 		spin_unlock_bh(&rt_hash_table[i].lock);
+		cond_resched_all();
 
 		for (; rth; rth = next) {
 			next = rth->u.rt_next;
@@ -582,7 +585,7 @@
 	}
 }
 
-static spinlock_t rt_flush_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(rt_flush_lock);
 
 void rt_cache_flush(int delay)
 {
@@ -902,7 +905,7 @@
 
 void rt_bind_peer(struct rtable *rt, int create)
 {
-	static spinlock_t rt_peer_lock = SPIN_LOCK_UNLOCKED;
+	static DECLARE_SPINLOCK(rt_peer_lock);
 	struct inet_peer *peer;
 
 	peer = inet_getpeer(rt->rt_dst, create);
@@ -926,7 +929,7 @@
  */
 static void ip_select_fb_ident(struct iphdr *iph)
 {
-	static spinlock_t ip_fb_id_lock = SPIN_LOCK_UNLOCKED;
+	static DECLARE_SPINLOCK(ip_fb_id_lock);
 	static u32 ip_fallback_id;
 	u32 salt;
 
@@ -1010,7 +1013,7 @@
 
 			rthp=&rt_hash_table[hash].chain;
 
-			rcu_read_lock();
+			rcu_read_lock_spin(&rt_hash_table[hash].lock);
 			while ((rth = rcu_dereference(*rthp)) != NULL) {
 				struct rtable *rt;
 
@@ -1031,7 +1034,7 @@
 					break;
 
 				dst_hold(&rth->u.dst);
-				rcu_read_unlock();
+				rcu_read_unlock_spin(&rt_hash_table[hash].lock);
 
 				rt = dst_alloc(&ipv4_dst_ops);
 				if (rt == NULL) {
@@ -1083,7 +1086,7 @@
 					ip_rt_put(rt);
 				goto do_next;
 			}
-			rcu_read_unlock();
+			rcu_read_unlock_spin(&rt_hash_table[hash].lock);
 		do_next:
 			;
 		}
@@ -1264,7 +1267,7 @@
 	for (i = 0; i < 2; i++) {
 		unsigned hash = rt_hash_code(daddr, skeys[i], tos);
 
-		rcu_read_lock();
+		rcu_read_lock_spin(&rt_hash_table[hash].lock);
 		for (rth = rcu_dereference(rt_hash_table[hash].chain); rth;
 		     rth = rcu_dereference(rth->u.rt_next)) {
 			if (rth->fl.fl4_dst == daddr &&
@@ -1302,7 +1305,7 @@
 				}
 			}
 		}
-		rcu_read_unlock();
+		rcu_read_lock_spin(&rt_hash_table[hash].lock);
 	}
 	return est_mtu ? : new_mtu;
 }
@@ -1826,7 +1829,7 @@
 	tos &= IPTOS_RT_MASK;
 	hash = rt_hash_code(daddr, saddr ^ (iif << 5), tos);
 
-	rcu_read_lock();
+	rcu_read_lock_spin(&rt_hash_table[hash].lock);
 	for (rth = rcu_dereference(rt_hash_table[hash].chain); rth;
 	     rth = rcu_dereference(rth->u.rt_next)) {
 		if (rth->fl.fl4_dst == daddr &&
@@ -1841,13 +1844,13 @@
 			dst_hold(&rth->u.dst);
 			rth->u.dst.__use++;
 			RT_CACHE_STAT_INC(in_hit);
-			rcu_read_unlock();
+			rcu_read_unlock_spin(&rt_hash_table[hash].lock);
 			skb->dst = (struct dst_entry*)rth;
 			return 0;
 		}
 		RT_CACHE_STAT_INC(in_hlist_search);
 	}
-	rcu_read_unlock();
+	rcu_read_unlock_spin(&rt_hash_table[hash].lock);
 
 	/* Multicast recognition logic is moved from route cache to here.
 	   The problem was that too many Ethernet cards have broken/missing
@@ -1863,7 +1866,7 @@
 	if (MULTICAST(daddr)) {
 		struct in_device *in_dev;
 
-		rcu_read_lock();
+		rcu_read_lock_sem(&rtnl_sem);
 		if ((in_dev = __in_dev_get(dev)) != NULL) {
 			int our = ip_check_mc(in_dev, daddr, saddr,
 				skb->nh.iph->protocol);
@@ -1872,12 +1875,12 @@
 			    || (!LOCAL_MCAST(daddr) && IN_DEV_MFORWARD(in_dev))
 #endif
 			    ) {
-				rcu_read_unlock();
+				rcu_read_unlock_sem(&rtnl_sem);
 				return ip_route_input_mc(skb, daddr, saddr,
 							 tos, dev, our);
 			}
 		}
-		rcu_read_unlock();
+		rcu_read_unlock_sem(&rtnl_sem);
 		return -EINVAL;
 	}
 	return ip_route_input_slow(skb, daddr, saddr, tos, dev);
@@ -2187,7 +2190,7 @@
 
 	hash = rt_hash_code(flp->fl4_dst, flp->fl4_src ^ (flp->oif << 5), flp->fl4_tos);
 
-	rcu_read_lock_bh();
+	rcu_read_lock_spin(&rt_hash_table[hash].lock);
 	for (rth = rcu_dereference(rt_hash_table[hash].chain); rth;
 		rth = rcu_dereference(rth->u.rt_next)) {
 		if (rth->fl.fl4_dst == flp->fl4_dst &&
@@ -2203,13 +2206,13 @@
 			dst_hold(&rth->u.dst);
 			rth->u.dst.__use++;
 			RT_CACHE_STAT_INC(out_hit);
-			rcu_read_unlock_bh();
+			rcu_read_unlock_spin(&rt_hash_table[hash].lock);
 			*rp = rth;
 			return 0;
 		}
 		RT_CACHE_STAT_INC(out_hlist_search);
 	}
-	rcu_read_unlock_bh();
+	rcu_read_unlock_spin(&rt_hash_table[hash].lock);
 
 	return ip_route_output_slow(rp, flp);
 }
@@ -2424,7 +2427,7 @@
 		if (h < s_h) continue;
 		if (h > s_h)
 			s_idx = 0;
-		rcu_read_lock_bh();
+		rcu_read_lock_spin(&rt_hash_table[h].lock);
 		for (rt = rcu_dereference(rt_hash_table[h].chain), idx = 0; rt;
 		     rt = rcu_dereference(rt->u.rt_next), idx++) {
 			if (idx < s_idx)
@@ -2434,12 +2437,12 @@
 					 cb->nlh->nlmsg_seq,
 					 RTM_NEWROUTE, 1) <= 0) {
 				dst_release(xchg(&skb->dst, NULL));
-				rcu_read_unlock_bh();
+				rcu_read_unlock_spin(&rt_hash_table[h].lock);
 				goto done;
 			}
 			dst_release(xchg(&skb->dst, NULL));
 		}
-		rcu_read_unlock_bh();
+		rcu_read_unlock_spin(&rt_hash_table[h].lock);
 	}
 
 done:
Index: xx-sources/net/ipv4/tcp_input.c
===================================================================
--- xx-sources.orig/net/ipv4/tcp_input.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/tcp_input.c	2004-10-16 20:57:03.000000000 -0400
@@ -3737,6 +3737,11 @@
 tcp_collapse(struct sock *sk, struct sk_buff *head,
 	     struct sk_buff *tail, u32 start, u32 end)
 {
+/*
+ * Do not do any collapsing in the lowlatency case - we rather want to
+ * drop packets than introduce millisecs of latencies:
+ */
+#ifndef CONFIG_PREEMPT
 	struct sk_buff *skb;
 
 	/* First, check that queue is collapsable and find
@@ -3820,6 +3825,7 @@
 			}
 		}
 	}
+#endif
 }
 
 /* Collapse ofo queue. Algorithm: select contiguous sequence of skbs
@@ -3827,6 +3833,7 @@
  */
 static void tcp_collapse_ofo_queue(struct sock *sk)
 {
+#ifndef CONFIG_PREEMPT
 	struct tcp_opt *tp = tcp_sk(sk);
 	struct sk_buff *skb = skb_peek(&tp->out_of_order_queue);
 	struct sk_buff *head;
@@ -3861,6 +3868,7 @@
 				end = TCP_SKB_CB(skb)->end_seq;
 		}
 	}
+#endif
 }
 
 /* Reduce allocated memory if we can, trying to get
Index: xx-sources/net/ipv4/tcp_minisocks.c
===================================================================
--- xx-sources.orig/net/ipv4/tcp_minisocks.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/ipv4/tcp_minisocks.c	2004-10-16 20:57:03.000000000 -0400
@@ -417,7 +417,7 @@
 #define TCP_TWKILL_QUOTA	100
 
 static struct hlist_head tcp_tw_death_row[TCP_TWKILL_SLOTS];
-static spinlock_t tw_death_lock = SPIN_LOCK_UNLOCKED;
+static DECLARE_SPINLOCK(tw_death_lock);
 static struct timer_list tcp_tw_timer = TIMER_INITIALIZER(tcp_twkill, 0, 0);
 static void twkill_work(void *);
 static DECLARE_WORK(tcp_twkill_work, twkill_work, NULL);
@@ -512,7 +512,7 @@
 				continue;
 
 			while (tcp_do_twkill_work(i, TCP_TWKILL_QUOTA) != 0) {
-				if (need_resched()) {
+				if (softirq_need_resched()) {
 					spin_unlock_bh(&tw_death_lock);
 					schedule();
 					spin_lock_bh(&tw_death_lock);
Index: xx-sources/net/sched/sch_generic.c
===================================================================
--- xx-sources.orig/net/sched/sch_generic.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/net/sched/sch_generic.c	2004-10-16 20:57:05.000000000 -0400
@@ -108,6 +108,10 @@
 		 * will be requeued.
 		 */
 		if (!nolock) {
+#ifdef CONFIG_PREEMPT_REALTIME
+			spin_lock(&dev->xmit_lock);
+			dev->xmit_lock_owner = _smp_processor_id();
+#else
 			if (!spin_trylock(&dev->xmit_lock)) {
 			collision:
 				/* So, someone grabbed the driver. */
@@ -117,7 +121,7 @@
 				   it by checking xmit owner and drop the
 				   packet when deadloop is detected.
 				*/
-				if (dev->xmit_lock_owner == smp_processor_id()) {
+				if (dev->xmit_lock_owner == _smp_processor_id()) {
 					kfree_skb(skb);
 					if (net_ratelimit())
 						printk(KERN_DEBUG "Dead loop on netdevice %s, fix it urgently!\n", dev->name);
@@ -127,7 +131,8 @@
 				goto requeue;
 			}
 			/* Remember that the driver is grabbed by us. */
-			dev->xmit_lock_owner = smp_processor_id();
+			dev->xmit_lock_owner = _smp_processor_id();
+#endif
 		}
 		
 		{
@@ -150,7 +155,14 @@
 				}
 				if (ret == NETDEV_TX_LOCKED && nolock) {
 					spin_lock(&dev->queue_lock);
+#ifdef CONFIG_PREEMPT_REALTIME
+					preempt_disable();
+					__get_cpu_var(netdev_rx_stat).cpu_collision++;
+					preempt_enable();
+					goto requeue;
+#else
 					goto collision; 
+#endif
 				}
 			}
 
Index: xx-sources/security/selinux/avc.c
===================================================================
--- xx-sources.orig/security/selinux/avc.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/security/selinux/avc.c	2004-10-16 20:57:06.000000000 -0400
@@ -70,7 +70,7 @@
 	struct avc_callback_node *next;
 };
 
-static spinlock_t avc_lock = SPIN_LOCK_UNLOCKED;
+static raw_spinlock_t avc_lock = RAW_SPIN_LOCK_UNLOCKED;
 static struct avc_node *avc_node_freelist;
 static struct avc_cache avc_cache;
 static unsigned avc_cache_stats[AVC_NSTATS];
Index: xx-sources/security/selinux/hooks.c
===================================================================
--- xx-sources.orig/security/selinux/hooks.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/security/selinux/hooks.c	2004-10-16 20:57:07.000000000 -0400
@@ -110,7 +110,7 @@
 /* Lists of inode and superblock security structures initialized
    before the policy was loaded. */
 static LIST_HEAD(superblock_security_head);
-static spinlock_t sb_security_lock = SPIN_LOCK_UNLOCKED;
+static raw_spinlock_t sb_security_lock = RAW_SPIN_LOCK_UNLOCKED;
 
 /* Allocate and free functions for each kind of security blob. */
 
Index: xx-sources/security/selinux/netif.c
===================================================================
--- xx-sources.orig/security/selinux/netif.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/security/selinux/netif.c	2004-10-16 20:57:07.000000000 -0400
@@ -38,7 +38,7 @@
 
 static u32 sel_netif_total;
 static LIST_HEAD(sel_netif_list);
-static spinlock_t sel_netif_lock = SPIN_LOCK_UNLOCKED;
+static raw_spinlock_t sel_netif_lock = RAW_SPIN_LOCK_UNLOCKED;
 static struct sel_netif sel_netif_hash[SEL_NETIF_HASH_SIZE];
 
 static inline u32 sel_netif_hasfn(struct net_device *dev)
Index: xx-sources/sound/core/seq/seq_device.c
===================================================================
--- xx-sources.orig/sound/core/seq/seq_device.c	2004-10-16 20:52:26.000000000 -0400
+++ xx-sources/sound/core/seq/seq_device.c	2004-10-16 20:57:08.000000000 -0400
@@ -41,6 +41,7 @@
 #include <sound/core.h>
 #include <sound/info.h>
 #include <sound/seq_device.h>
+#include <sound/seq_kernel.h>
 #include <sound/initval.h>
 #include <linux/kmod.h>
 #include <linux/slab.h>
