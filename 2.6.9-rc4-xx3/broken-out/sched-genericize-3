Index: xx-sources/include/linux/sched.h
===================================================================
--- xx-sources.orig/include/linux/sched.h	2004-10-08 12:01:56.233598888 -0400
+++ xx-sources/include/linux/sched.h	2004-10-08 12:16:21.441067360 -0400
@@ -199,6 +199,8 @@
 #define	MAX_SCHEDULE_TIMEOUT	LONG_MAX
 extern signed long FASTCALL(schedule_timeout(signed long timeout));
 asmlinkage void schedule(void);
+extern void release_kernel_sem(struct task_struct *task);
+extern void reacquire_kernel_sem(struct task_struct *task);
 
 struct namespace;
 
Index: xx-sources/include/linux/scheduler.h
===================================================================
--- xx-sources.orig/include/linux/scheduler.h	2004-10-08 12:02:14.873765152 -0400
+++ xx-sources/include/linux/scheduler.h	2004-10-08 12:07:08.375146160 -0400
@@ -56,11 +56,6 @@
 typedef void (init_sched_build_groups_fn)(struct sched_group groups[], cpumask_t span, int (*group_fn)(int cpu));
 typedef void (cpu_attach_domain_fn)(struct sched_domain *, int);
 #endif
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-typedef int (kernel_locked_fn) (void);
-typedef void (lock_kernel_fn) (void);
-typedef void (unlock_kernel_fn) (void);
-#endif
 #if defined(CONFIG_SYSCTL)
 typedef void (sysctl_register_fn)(void);
 #endif
@@ -122,11 +117,6 @@
 	wait_task_inactive_fn *wait_task_inactive_fn;
 	kick_process_fn *kick_process_fn;
 #endif
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-	kernel_locked_fn *kernel_locked_fn;
-	lock_kernel_fn *lock_kernel_fn;
-	unlock_kernel_fn *unlock_kernel_fn;
-#endif
 #if defined(CONFIG_SYSCTL)
 	sysctl_register_fn *sysctl_register_fn;
 #endif
Index: xx-sources/kernel/default-sched.c
===================================================================
--- xx-sources.orig/kernel/default-sched.c	2004-10-08 12:02:14.877764544 -0400
+++ xx-sources/kernel/default-sched.c	2004-10-08 12:07:02.034110144 -0400
@@ -2441,155 +2441,6 @@
 }
 #endif
 
-
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-
-#ifdef CONFIG_PREEMPT_BKL
-/*
- * The 'big kernel semaphore'
- *
- * This mutex is taken and released recursively by lock_kernel()
- * and default_unlock_kernel().  It is transparently dropped and reaquired
- * over schedule().  It is used to protect legacy code that hasn't
- * been migrated to a proper locking design yet.
- *
- * Note: code locked by this semaphore will only be serialized against
- * other code using the same locking facility. The code guarantees that
- * the task remains on the same CPU.
- *
- * Don't use in new code.
- */
-static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
-
-int default_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-/*
- * Release global kernel semaphore:
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		up(&kernel_sem);
-}
-
-/*
- * Re-acquire the kernel semaphore.
- *
- * This function is called with preemption off.
- *
- * We are executing in schedule() so the code must be extremely careful
- * about recursion, both due to the down() and due to the enabling of
- * preemption. schedule() will re-check the preemption flag after
- * reacquiring the semaphore.
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	int saved_lock_depth = task->lock_depth;
-
-	if (likely(saved_lock_depth < 0))
-		return;
-
-	task->lock_depth = -1;
-	preempt_enable_no_resched();
-
-	down(&kernel_sem);
-
-	preempt_disable();
-	task->lock_depth = saved_lock_depth;
-}
-
-/*
- * Getting the big kernel semaphore.
- */
-void default_lock_kernel(void)
-{
-	struct task_struct *task = current;
-	int depth = task->lock_depth + 1;
-
-	if (likely(!depth))
-		/*
-		 * No recursion worries - we set up lock_depth _after_
-		 */
-		down(&kernel_sem);
-
-	task->lock_depth = depth;
-}
-
-void default_unlock_kernel(void)
-{
-	struct task_struct *task = current;
-
-	BUG_ON(task->lock_depth < 0);
-
-	if (likely(--task->lock_depth < 0))
-		up(&kernel_sem);
-}
-
-#else
-
-static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
-
-int default_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-#define get_kernel_lock()	spin_lock(&kernel_flag)
-#define put_kernel_lock()	spin_unlock(&kernel_flag)
-
-/*
- * Release global kernel lock.
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		put_kernel_lock();
-}
-
-/*
- * Re-acquire the kernel lock
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		get_kernel_lock();
-}
-
-/*
- * Getting the big kernel lock.
- *
- * This cannot happen asynchronously,
- * so we only need to worry about other
- * CPU's.
- */
-void default_lock_kernel(void)
-{
-	int depth = current->lock_depth+1;
-	if (likely(!depth))
-		get_kernel_lock();
-	current->lock_depth = depth;
-}
-
-void default_unlock_kernel(void)
-{
-	BUG_ON(current->lock_depth < 0);
-	if (likely(--current->lock_depth < 0))
-		put_kernel_lock();
-}
-
-#endif
-
-#else
-
-static inline void release_kernel_sem(struct task_struct *task) { }
-static inline void reacquire_kernel_sem(struct task_struct *task) { }
-
-#endif
-
-
 /*
  * default_schedule() is the main scheduler function.
  */
@@ -4427,11 +4278,6 @@
 	.wait_task_inactive_fn = 		default_wait_task_inactive,
 	.kick_process_fn = 			default_kick_process,
 #endif
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-	.kernel_locked_fn = 			default_kernel_locked,
-	.lock_kernel_fn = 			default_lock_kernel,
-	.unlock_kernel_fn = 			default_unlock_kernel,
-#endif
 	.name = 				DEFAULT_SCHEDULER_NAME,
 	.version = 				DEFAULT_SCHEDULER_VERSION,
 	.type = 				SCHED_DEFAULT,
Index: xx-sources/kernel/nicksched-sched.c
===================================================================
--- xx-sources.orig/kernel/nicksched-sched.c	2004-10-08 12:02:14.882763784 -0400
+++ xx-sources/kernel/nicksched-sched.c	2004-10-08 12:07:48.055113888 -0400
@@ -2260,165 +2260,6 @@
 }
 #endif
 
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-
-#ifdef CONFIG_PREEMPT_BKL
-/*
- * The 'big kernel semaphore'
- *
- * This mutex is taken and released recursively by lock_kernel()
- * and nicksched_unlock_kernel().  It is transparently dropped and reaquired
- * over schedule().  It is used to protect legacy code that hasn't
- * been migrated to a proper locking design yet.
- *
- * Note: code locked by this semaphore will only be serialized against
- * other code using the same locking facility. The code guarantees that
- * the task remains on the same CPU.
- *
- * Don't use in new code.
- */
-static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
-
-int nicksched_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-EXPORT_SYMBOL(nicksched_kernel_locked);
-
-/*
- * Release global kernel semaphore:
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		up(&kernel_sem);
-}
-
-/*
- * Re-acquire the kernel semaphore.
- *
- * This function is called with preemption off.
- *
- * We are executing in schedule() so the code must be extremely careful
- * about recursion, both due to the down() and due to the enabling of
- * preemption. schedule() will re-check the preemption flag after
- * reacquiring the semaphore.
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	int saved_lock_depth = task->lock_depth;
-
-	if (likely(saved_lock_depth < 0))
-		return;
-
-	task->lock_depth = -1;
-	preempt_enable_no_resched();
-
-	down(&kernel_sem);
-
-	preempt_disable();
-	task->lock_depth = saved_lock_depth;
-}
-
-/*
- * Getting the big kernel semaphore.
- */
-void nicksched_lock_kernel(void)
-{
-	struct task_struct *task = current;
-	int depth = task->lock_depth + 1;
-
-	if (likely(!depth))
-		/*
-		 * No recursion worries - we set up lock_depth _after_
-		 */
-		down(&kernel_sem);
-
-	task->lock_depth = depth;
-}
-
-EXPORT_SYMBOL(nicksched_lock_kernel);
-
-void nicksched_unlock_kernel(void)
-{
-	struct task_struct *task = current;
-
-	BUG_ON(task->lock_depth < 0);
-
-	if (likely(--task->lock_depth < 0))
-		up(&kernel_sem);
-}
-
-EXPORT_SYMBOL(nicksched_unlock_kernel);
-
-#else
-
-static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
-
-int nicksched_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-EXPORT_SYMBOL(nicksched_kernel_locked);
-
-#define get_kernel_lock()	spin_lock(&kernel_flag)
-#define put_kernel_lock()	spin_unlock(&kernel_flag)
-
-/*
- * Release global kernel lock.
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		put_kernel_lock();
-}
-
-/*
- * Re-acquire the kernel lock
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		get_kernel_lock();
-}
-
-/*
- * Getting the big kernel lock.
- *
- * This cannot happen asynchronously,
- * so we only need to worry about other
- * CPU's.
- */
-void nicksched_lock_kernel(void)
-{
-	int depth = current->lock_depth+1;
-	if (likely(!depth))
-		get_kernel_lock();
-	current->lock_depth = depth;
-}
-
-EXPORT_SYMBOL(nicksched_lock_kernel);
-
-void nicksched_unlock_kernel(void)
-{
-	BUG_ON(current->lock_depth < 0);
-	if (likely(--current->lock_depth < 0))
-		put_kernel_lock();
-}
-
-EXPORT_SYMBOL(nicksched_unlock_kernel);
-
-#endif
-
-#else
-
-static inline void release_kernel_sem(struct task_struct *task) { }
-static inline void reacquire_kernel_sem(struct task_struct *task) { }
-
-#endif
-
 /*
  * schedule() is the main scheduler function.
  */
@@ -4306,11 +4147,6 @@
 	.wait_task_inactive_fn = 		nicksched_wait_task_inactive,
 	.kick_process_fn = 			nicksched_kick_process,
 #endif
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-	.kernel_locked_fn = 			nicksched_kernel_locked,
-	.lock_kernel_fn = 			nicksched_lock_kernel,
-	.unlock_kernel_fn = 			nicksched_unlock_kernel,
-#endif
 #if defined(CONFIG_SYSCTL)
 	.sysctl_register_fn = 			nicksched_sysctl_register,
 #endif
Index: xx-sources/kernel/sched.c
===================================================================
--- xx-sources.orig/kernel/sched.c	2004-10-08 12:02:14.884763480 -0400
+++ xx-sources/kernel/sched.c	2004-10-08 12:32:58.237531264 -0400
@@ -178,25 +178,149 @@
 
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 
-inline int kernel_locked(void)
+#ifdef CONFIG_PREEMPT_BKL
+/*
+ * The 'big kernel semaphore'
+ *
+ * This mutex is taken and released recursively by lock_kernel()
+ * and default_unlock_kernel().  It is transparently dropped and reaquired
+ * over schedule().  It is used to protect legacy code that hasn't
+ * been migrated to a proper locking design yet.
+ *
+ * Note: code locked by this semaphore will only be serialized against
+ * other code using the same locking facility. The code guarantees that
+ * the task remains on the same CPU.
+ *
+ * Don't use in new code.
+ */
+static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
+
+int kernel_locked(void)
 {
-	if (current_scheduler->kernel_locked_fn)
-		return current_scheduler->kernel_locked_fn();
-	return 0;
+	return current->lock_depth >= 0;
+}
+
+/*
+ * Release global kernel semaphore:
+ */
+inline void release_kernel_sem(struct task_struct *task)
+{
+	if (unlikely(task->lock_depth >= 0))
+		up(&kernel_sem);
+}
+
+/*
+ * Re-acquire the kernel semaphore.
+ *
+ * This function is called with preemption off.
+ *
+ * We are executing in schedule() so the code must be extremely careful
+ * about recursion, both due to the down() and due to the enabling of
+ * preemption. schedule() will re-check the preemption flag after
+ * reacquiring the semaphore.
+ */
+inline void reacquire_kernel_sem(struct task_struct *task)
+{
+	int saved_lock_depth = task->lock_depth;
+
+	if (likely(saved_lock_depth < 0))
+		return;
+
+	task->lock_depth = -1;
+	preempt_enable_no_resched();
+
+	down(&kernel_sem);
+
+	preempt_disable();
+	task->lock_depth = saved_lock_depth;
+}
+
+/*
+ * Getting the big kernel semaphore.
+ */
+void lock_kernel(void)
+{
+	struct task_struct *task = current;
+	int depth = task->lock_depth + 1;
+
+	if (likely(!depth))
+		/*
+		 * No recursion worries - we set up lock_depth _after_
+		 */
+		down(&kernel_sem);
+
+	task->lock_depth = depth;
+}
+
+void unlock_kernel(void)
+{
+	struct task_struct *task = current;
+
+	BUG_ON(task->lock_depth < 0);
+
+	if (likely(--task->lock_depth < 0))
+		up(&kernel_sem);
 }
 
-inline void lock_kernel(void)
+#else
+
+static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
+
+int kernel_locked(void)
 {
-	if (current_scheduler->lock_kernel_fn)
-		current_scheduler->lock_kernel_fn();
+	return current->lock_depth >= 0;
 }
 
-inline void unlock_kernel(void)
+#define get_kernel_lock()	spin_lock(&kernel_flag)
+#define put_kernel_lock()	spin_unlock(&kernel_flag)
+
+/*
+ * Release global kernel lock.
+ */
+inline void release_kernel_sem(struct task_struct *task)
 {
-	if (current_scheduler->unlock_kernel_fn)
-		current_scheduler->unlock_kernel_fn();
+	if (unlikely(task->lock_depth >= 0))
+		put_kernel_lock();
 }
 
+/*
+ * Re-acquire the kernel lock
+ */
+inline void reacquire_kernel_sem(struct task_struct *task)
+{
+	if (unlikely(task->lock_depth >= 0))
+		get_kernel_lock();
+}
+
+/*
+ * Getting the big kernel lock.
+ *
+ * This cannot happen asynchronously,
+ * so we only need to worry about other
+ * CPU's.
+ */
+void lock_kernel(void)
+{
+	int depth = current->lock_depth+1;
+	if (likely(!depth))
+		get_kernel_lock();
+	current->lock_depth = depth;
+}
+
+void unlock_kernel(void)
+{
+	BUG_ON(current->lock_depth < 0);
+	if (likely(--current->lock_depth < 0))
+		put_kernel_lock();
+}
+
+#endif
+
+#else
+
+inline void release_kernel_sem(struct task_struct *task) { }
+inline void reacquire_kernel_sem(struct task_struct *task) { }
+
 #endif
 
 inline asmlinkage void __sched schedule(void)
Index: xx-sources/kernel/staircase-sched.c
===================================================================
--- xx-sources.orig/kernel/staircase-sched.c	2004-10-08 12:02:14.888762872 -0400
+++ xx-sources/kernel/staircase-sched.c	2004-10-08 12:08:23.831675024 -0400
@@ -2176,154 +2176,6 @@
 }
 #endif
 
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-
-#ifdef CONFIG_PREEMPT_BKL
-/*
- * The 'big kernel semaphore'
- *
- * This mutex is taken and released recursively by lock_kernel()
- * and unlock_kernel().  It is transparently dropped and reaquired
- * over schedule().  It is used to protect legacy code that hasn't
- * been migrated to a proper locking design yet.
- *
- * Note: code locked by this semaphore will only be serialized against
- * other code using the same locking facility. The code guarantees that
- * the task remains on the same CPU.
- *
- * Don't use in new code.
- */
-static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
-
-int staircase_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-/*
- * Release global kernel semaphore:
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		up(&kernel_sem);
-}
-
-/*
- * Re-acquire the kernel semaphore.
- *
- * This function is called with preemption off.
- *
- * We are executing in schedule() so the code must be extremely careful
- * about recursion, both due to the down() and due to the enabling of
- * preemption. schedule() will re-check the preemption flag after
- * reacquiring the semaphore.
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	int saved_lock_depth = task->lock_depth;
-
-	if (likely(saved_lock_depth < 0))
-		return;
-
-	task->lock_depth = -1;
-	preempt_enable_no_resched();
-
-	down(&kernel_sem);
-
-	preempt_disable();
-	task->lock_depth = saved_lock_depth;
-}
-
-/*
- * Getting the big kernel semaphore.
- */
-void staircase_lock_kernel(void)
-{
-	struct task_struct *task = current;
-	int depth = task->lock_depth + 1;
-
-	if (likely(!depth))
-		/*
-		 * No recursion worries - we set up lock_depth _after_
-		 */
-		down(&kernel_sem);
-
-	task->lock_depth = depth;
-}
-
-void staircase_unlock_kernel(void)
-{
-	struct task_struct *task = current;
-
-	BUG_ON(task->lock_depth < 0);
-
-	if (likely(--task->lock_depth < 0))
-		up(&kernel_sem);
-}
-
-#else
-
-static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
-
-int staircase_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-#define get_kernel_lock()	spin_lock(&kernel_flag)
-#define put_kernel_lock()	spin_unlock(&kernel_flag)
-
-/*
- * Release global kernel lock.
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		put_kernel_lock();
-}
-
-/*
- * Re-acquire the kernel lock
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		get_kernel_lock();
-}
-
-/*
- * Getting the big kernel lock.
- *
- * This cannot happen asynchronously,
- * so we only need to worry about other
- * CPU's.
- */
-void staircase_lock_kernel(void)
-{
-	int depth = current->lock_depth+1;
-	if (likely(!depth))
-		get_kernel_lock();
-	current->lock_depth = depth;
-}
-
-void staircase_unlock_kernel(void)
-{
-	BUG_ON(current->lock_depth < 0);
-	if (likely(--current->lock_depth < 0))
-		put_kernel_lock();
-}
-
-#endif
-
-#else
-
-static inline void release_kernel_sem(struct task_struct *task) { }
-static inline void reacquire_kernel_sem(struct task_struct *task) { }
-
-#endif
-
-
 /*
  * schedule() is the main scheduler function.
  */
@@ -4158,11 +4010,6 @@
 	.wait_task_inactive_fn = 		staircase_wait_task_inactive,
 	.kick_process_fn = 			staircase_kick_process,
 #endif
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-	.kernel_locked_fn = 			staircase_kernel_locked,
-	.lock_kernel_fn = 			staircase_lock_kernel,
-	.unlock_kernel_fn = 			staircase_unlock_kernel,
-#endif
 #if defined(CONFIG_SYSCTL)
 	.sysctl_register_fn = 			staircase_sysctl_register,
 #endif
Index: xx-sources/kernel/xsched-sched.c
===================================================================
--- xx-sources.orig/kernel/xsched-sched.c	2004-10-08 12:02:14.893762112 -0400
+++ xx-sources/kernel/xsched-sched.c	2004-10-08 12:08:51.970397288 -0400
@@ -2380,154 +2380,6 @@
 }
 #endif
 
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-
-#ifdef CONFIG_PREEMPT_BKL
-/*
- * The 'big kernel semaphore'
- *
- * This mutex is taken and released recursively by lock_kernel()
- * and unlock_kernel().  It is transparently dropped and reaquired
- * over schedule().  It is used to protect legacy code that hasn't
- * been migrated to a proper locking design yet.
- *
- * Note: code locked by this semaphore will only be serialized against
- * other code using the same locking facility. The code guarantees that
- * the task remains on the same CPU.
- *
- * Don't use in new code.
- */
-static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
-
-int xsched_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-/*
- * Release global kernel semaphore:
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		up(&kernel_sem);
-}
-
-/*
- * Re-acquire the kernel semaphore.
- *
- * This function is called with preemption off.
- *
- * We are executing in schedule() so the code must be extremely careful
- * about recursion, both due to the down() and due to the enabling of
- * preemption. schedule() will re-check the preemption flag after
- * reacquiring the semaphore.
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	int saved_lock_depth = task->lock_depth;
-
-	if (likely(saved_lock_depth < 0))
-		return;
-
-	task->lock_depth = -1;
-	preempt_enable_no_resched();
-
-	down(&kernel_sem);
-
-	preempt_disable();
-	task->lock_depth = saved_lock_depth;
-}
-
-/*
- * Getting the big kernel semaphore.
- */
-void xsched_lock_kernel(void)
-{
-	struct task_struct *task = current;
-	int depth = task->lock_depth + 1;
-
-	if (likely(!depth))
-		/*
-		 * No recursion worries - we set up lock_depth _after_
-		 */
-		down(&kernel_sem);
-
-	task->lock_depth = depth;
-}
-
-void xsched_unlock_kernel(void)
-{
-	struct task_struct *task = current;
-
-	BUG_ON(task->lock_depth < 0);
-
-	if (likely(--task->lock_depth < 0))
-		up(&kernel_sem);
-}
-
-#else
-
-static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
-
-int xsched_kernel_locked(void)
-{
-	return current->lock_depth >= 0;
-}
-
-#define get_kernel_lock()	spin_lock(&kernel_flag)
-#define put_kernel_lock()	spin_unlock(&kernel_flag)
-
-/*
- * Release global kernel lock.
- */
-static inline void release_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		put_kernel_lock();
-}
-
-/*
- * Re-acquire the kernel lock
- */
-static inline void reacquire_kernel_sem(struct task_struct *task)
-{
-	if (unlikely(task->lock_depth >= 0))
-		get_kernel_lock();
-}
-
-/*
- * Getting the big kernel lock.
- *
- * This cannot happen asynchronously,
- * so we only need to worry about other
- * CPU's.
- */
-void xsched_lock_kernel(void)
-{
-	int depth = current->lock_depth+1;
-	if (likely(!depth))
-		get_kernel_lock();
-	current->lock_depth = depth;
-}
-
-void xsched_unlock_kernel(void)
-{
-	BUG_ON(current->lock_depth < 0);
-	if (likely(--current->lock_depth < 0))
-		put_kernel_lock();
-}
-
-#endif
-
-#else
-
-static inline void release_kernel_sem(struct task_struct *task) { }
-static inline void reacquire_kernel_sem(struct task_struct *task) { }
-
-#endif
-
-
 /*
  * schedule() is the main scheduler function.
  */
@@ -4591,11 +4443,6 @@
 	.wait_task_inactive_fn = 		xsched_wait_task_inactive,
 	.kick_process_fn = 			xsched_kick_process,
 #endif
-#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
-	.kernel_locked_fn = 			xsched_kernel_locked,
-	.lock_kernel_fn = 			xsched_lock_kernel,
-	.unlock_kernel_fn = 			xsched_unlock_kernel,
-#endif
 #if defined(CONFIG_SYSCTL)
 	.sysctl_register_fn = 			xsched_sysctl_register,
 #endif
