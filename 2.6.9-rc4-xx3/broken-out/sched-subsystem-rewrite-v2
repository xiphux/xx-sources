Index: xx-sources/arch/ia64/kernel/domain.c
===================================================================
--- xx-sources.orig/arch/ia64/kernel/domain.c	2004-10-07 09:49:25.805100072 -0400
+++ xx-sources/arch/ia64/kernel/domain.c	2004-10-07 10:02:27.609247776 -0400
@@ -183,8 +183,9 @@
 		if (i != first_cpu(this_sibling_map))
 			continue;
 
-		init_sched_build_groups(sched_group_cpus, this_sibling_map,
-						&cpu_to_cpu_group);
+		if (current_scheduler->init_sched_build_groups_fn)
+			current_scheduler->init_sched_build_groups_fn(sched_group_cpus,
+					this_sibling_map, &cpu_to_cpu_group);
 	}
 #endif
 
@@ -195,9 +196,10 @@
 		cpus_and(nodemask, nodemask, cpu_default_map);
 		if (cpus_empty(nodemask))
 			continue;
-
-		init_sched_build_groups(sched_group_phys, nodemask,
-						&cpu_to_phys_group);
+		
+		if (current_scheduler->init_sched_build_groups_fn)
+			current_scheduler->init_sched_build_groups_fn(sched_group_phys,
+					nodemask, &cpu_to_phys_group);
 	}
 
 #ifdef CONFIG_NUMA
Index: xx-sources/fs/proc/array.c
===================================================================
--- xx-sources.orig/fs/proc/array.c	2004-10-07 10:02:06.571446008 -0400
+++ xx-sources/fs/proc/array.c	2004-10-07 10:02:27.611247472 -0400
@@ -162,15 +162,14 @@
 	read_lock(&tasklist_lock);
 	buffer += sprintf(buffer,
 		"State:\t%s\n"
-#if defined(CONFIG_STAIRCASE)
 		"Burst:\t%d\n"
-#elif defined(CONFIG_NICKSCHED) || defined(CONFIG_XSCHED)
-		"sleep_avg:\t%lu\n"
-		"sleep_time:\t%lu\n"
-		"total_time:\t%lu\n"
-#elif !defined(CONFIG_SPA)
+		"nicksched_sleep_avg:\t%lu\n"
+		"nicksched_sleep_time:\t%lu\n"
+		"nicksched_total_time:\t%lu\n"
+		"xsched_sleep_avg:\t%lu\n"
+		"xsched_sleep_time:\t%lu\n"
+		"xsched_total_time:\t%lu\n"
 		"SleepAVG:\t%lu%%\n"
-#endif
 		"Tgid:\t%d\n"
 		"Pid:\t%d\n"
 		"PPid:\t%d\n"
@@ -178,13 +177,10 @@
 		"Uid:\t%d\t%d\t%d\t%d\n"
 		"Gid:\t%d\t%d\t%d\t%d\n",
 		get_task_state(p),
-#if defined(CONFIG_STAIRCASE)
-		p->burst,
-#elif defined(CONFIG_NICKSCHED) || defined(CONFIG_XSCHED)
-		p->sleep_avg, p->sleep_time, p->total_time,
-#elif !defined(CONFIG_SPA)
-		(p->sleep_avg/1024)*100/(1020000000/1024),
-#endif
+		p->staircase_data.burst,
+		p->nicksched_data.sleep_avg, p->nicksched_data.sleep_time, p->nicksched_data.total_time,
+		p->xsched_data.sleep_avg, p->xsched_data.sleep_time, p->xsched_data.total_time,
+		(p->default_data.sleep_avg/1024)*100/(1020000000/1024),
 	       	p->tgid,
 		p->pid, p->pid ? p->real_parent->pid : 0,
 		p->pid && p->ptrace ? p->parent->pid : 0,
Index: xx-sources/fs/proc/proc_misc.c
===================================================================
--- xx-sources.orig/fs/proc/proc_misc.c	2004-10-07 10:02:06.577445096 -0400
+++ xx-sources/fs/proc/proc_misc.c	2004-10-07 10:02:27.612247320 -0400
@@ -162,8 +162,6 @@
 	;
 	const int jiffies = CONFIG_HERTZ;
 	const int idedelay = CONFIG_IDE_DELAY;
-	extern char *scheduler_name;
-	extern char *scheduler_version;
 #if defined(CONFIG_CFLAGS_EDIT)
 	const char *cflags = CONFIG_CFLAGS_STRING;
 #endif
@@ -192,8 +190,8 @@
 		"CFLAGS:				%s\n"
 #endif
 		,
-		scheduler_name,
-		scheduler_version,
+		current_scheduler->name,
+		current_scheduler->version,
 		elv,
 		jiffies,
 		idedelay
Index: xx-sources/include/linux/init_task.h
===================================================================
--- xx-sources.orig/include/linux/init_task.h	2004-10-07 10:02:06.579444792 -0400
+++ xx-sources/include/linux/init_task.h	2004-10-07 10:02:27.613247168 -0400
@@ -65,81 +65,37 @@
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
  */
-#if defined(CONFIG_NICKSCHED) || defined(CONFIG_XSCHED)
-#define SCHED_PRIO .prio = MAX_PRIO-29,
-#else
-#define SCHED_PRIO .prio = MAX_PRIO-20,
-#endif
-
-#if defined(CONFIG_NICKSCHED) || defined(CONFIG_XSCHED)
-#define SCHED_STATIC_PRIO .static_prio = MAX_PRIO-29,
-#else
-#define SCHED_STATIC_PRIO .static_prio = MAX_PRIO-20,
-#endif
-
-#if defined(CONFIG_NICKSCHED) || defined(CONFIG_XSCHED)
-#define SCHED_TIME_SLICE
-#else
-#define SCHED_TIME_SLICE .time_slice = HZ,
-#endif
-
-#if defined(CONFIG_SPA)
-#define SCHED_RQ .rq = NULL,
-#else
-#define SCHED_RQ
-#endif
-
-#if defined(CONFIG_SPA)
-#define SCHED_PRE_BONUS_PRIO .pre_bonus_priority = MAX_PRIO-20,
-#else
-#define SCHED_PRE_BONUS_PRIO
-#endif
-
-#if defined(CONFIG_SPA)
-#define SCHED_EB_SHARES .eb_shares = DEFAULT_EB_SHARES,
-#else
-#define SCHED_EB_SHARES
-#endif
-
-#if defined(CONFIG_SPA)
-#define SCHED_CPU_RATE_CAP .cpu_rate_cap = PROPORTION_ONE,
-#else
-#define SCHED_CPU_RATE_CAP
-#endif
-
-#if defined(CONFIG_SPA)
-#define SCHED_CPU_RATE_HARD_CAP .cpu_rate_hard_cap = PROPORTION_ONE,
-#else
-#define SCHED_CPU_RATE_HARD_CAP
-#endif
-
-#if defined(CONFIG_SPA)
-#define SCHED_SINBIN_TIMER .sinbin_timer = { .function = sinbin_release_fn },
-#else
-#define SCHED_SINBIN_TIMER
-#endif
 
 #define INIT_TASK(tsk)	\
 {									\
 	.state		= 0,						\
 	.thread_info	= &init_thread_info,				\
-	SCHED_RQ							\
 	.usage		= ATOMIC_INIT(2),				\
 	.flags		= 0,						\
 	.lock_depth	= -1,						\
-	SCHED_PRIO							\
-	SCHED_STATIC_PRIO						\
-	SCHED_PRE_BONUS_PRIO						\
-	SCHED_EB_SHARES							\
-	SCHED_CPU_RATE_CAP						\
-	SCHED_CPU_RATE_HARD_CAP						\
-	SCHED_SINBIN_TIMER						\
+	.default_data	= {						\
+		.prio			= MAX_PRIO-20,			\
+		.static_prio		= MAX_PRIO-20,			\
+		.time_slice		= HZ,				\
+	},								\
+	.nicksched_data	= {						\
+		.prio			= MAX_PRIO-29,			\
+		.static_prio		= MAX_PRIO-29,			\
+	},								\
+	.staircase_data	= {						\
+		.prio			= MAX_PRIO-20,			\
+		.static_prio		= MAX_PRIO-20,			\
+		.time_slice		= HZ,				\
+	},								\
+	.xsched_data	= {						\
+		.prio			= MAX_PRIO-29,			\
+		.static_prio		= MAX_PRIO-29,			\
+	},								\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
 	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\
-	SCHED_TIME_SLICE						\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
 	.ptrace_list	= LIST_HEAD_INIT(tsk.ptrace_list),		\
Index: xx-sources/include/linux/sched.h
===================================================================
--- xx-sources.orig/include/linux/sched.h	2004-10-07 10:02:06.581444488 -0400
+++ xx-sources/include/linux/sched.h	2004-10-07 10:05:50.396419440 -0400
@@ -32,8 +32,21 @@
 #include <linux/percpu.h>
 #include <linux/topology.h>
 
+#define DEFAULT_SCHEDULER_NAME "Default"
+#define NICKSCHED_SCHEDULER_NAME "Nicksched"
+#define STAIRCASE_SCHEDULER_NAME "Staircase"
+#define XSCHED_SCHEDULER_NAME "Xsched"
+
 struct exec_domain;
 
+enum {
+	SCHED_NONE=0,
+	SCHED_DEFAULT=1,
+	SCHED_NICKSCHED=2,
+	SCHED_STAIRCASE=3,
+	SCHED_XSCHED=4
+};
+
 /*
  * cloning flags:
  */
@@ -129,19 +142,6 @@
 #define SCHED_FIFO		1
 #define SCHED_RR		2
 
-#if defined(CONFIG_XSCHED)
-#define SCHED_BATCH		3
-#define SCHED_ISO		4
-
-#define SCHED_MIN		0
-#define SCHED_MAX		4
-
-#define SCHED_RANGE(policy)	((policy) >= SCHED_MIN && \
-					(policy) <= SCHED_MAX)
-#define SCHED_RT(policy)	((policy) == SCHED_FIFO || \
-					(policy) == SCHED_RR)
-#endif
-
 struct sched_param {
 	int sched_priority;
 };
@@ -161,6 +161,8 @@
 
 typedef struct task_struct task_t;
 
+extern void sched_reinit(void);
+extern void null_sched_init(void);
 extern void sched_init(void);
 extern void sched_init_smp(void);
 extern void init_idle(task_t *idle, int cpu);
@@ -179,9 +181,8 @@
 
 void io_schedule(void);
 long io_schedule_timeout(long timeout);
-#if defined(CONFIG_STAIRCASE)
-extern int sched_interactive, sched_compute;
-#endif
+extern int staircase_sched_interactive, staircase_sched_compute;
+extern int xsched_sched_interactive, xsched_sched_compute;
 
 extern void cpu_init (void);
 extern void trap_init(void);
@@ -357,19 +358,13 @@
 #define MAX_USER_RT_PRIO	100
 #define MAX_RT_PRIO		MAX_USER_RT_PRIO
 
-#if defined(CONFIG_NICKSCHED) || defined(CONFIG_XSCHED)
-#define PRIO_RANGE 59
-#else
-#define PRIO_RANGE 40
-#endif
+#define MAX_PRIO		(MAX_RT_PRIO + 40)
+#define NICK_MAX_PRIO		(MAX_RT_PRIO + 59)
 
-#define MAX_PRIO		(MAX_RT_PRIO + PRIO_RANGE)
-
-#define rt_task(p)		(unlikely((p)->prio < MAX_RT_PRIO))
-#if defined(CONFIG_XSCHED)
-#define batch_task(p)		((p)->policy == SCHED_BATCH)
-#define iso_task(p)		((p)->policy == SCHED_ISO)
-#endif
+#define default_rt_task(p)		(unlikely((p)->default_data.prio < MAX_RT_PRIO))
+#define nicksched_rt_task(p)		(unlikely((p)->nicksched_data.prio < MAX_RT_PRIO))
+#define staircase_rt_task(p)		(unlikely((p)->staircase_data.prio < MAX_RT_PRIO))
+#define xsched_rt_task(p)		(unlikely((p)->xsched_data.prio < MAX_RT_PRIO))
 
 /*
  * Some day this will be a full-fledged user tracking system..
@@ -398,11 +393,8 @@
 extern struct user_struct root_user;
 #define INIT_USER (&root_user)
 
-#if defined(CONFIG_SPA)
 typedef struct runqueue runqueue_t;
-#elif !defined(CONFIG_STAIRCASE) && !defined(CONFIG_XSCHED)
 typedef struct prio_array prio_array_t;
-#endif
 struct backing_dev_info;
 struct reclaim_state;
 
@@ -464,6 +456,7 @@
 	unsigned int busy_factor;	/* less balancing by factor if busy */
 	unsigned int imbalance_pct;	/* No balance until over watermark */
 	unsigned long long cache_hot_time; /* Task considered cache hot (ns) */
+	unsigned long long nick_cache_hot_time; /* Task considered cache hot (ns) */
 	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
 	unsigned int per_cpu_gain;	/* CPU % gained by adding domain cpus */
 	int flags;			/* See SD_* */
@@ -541,7 +534,6 @@
 struct audit_context;		/* See audit.c */
 struct mempolicy;
 
-#if defined(CONFIG_SPA)
 /*
  * For entitlemnet based scheduling a task's shares will be determined from
  * their "nice"ness
@@ -565,92 +557,65 @@
 int set_cpu_shares(struct task_struct *p, unsigned int new_shares);
 
 void sinbin_release_fn(unsigned long arg);
-#endif
 
-struct task_struct {
-	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
-	struct thread_info *thread_info;
-	atomic_t usage;
-	unsigned long flags;	/* per process flags, defined below */
-	unsigned long ptrace;
-
-	int lock_depth;		/* Lock depth */
-
-#if defined(CONFIG_SPA)
+struct default_data {
 	int prio, static_prio;
-	struct list_head run_list;
-	runqueue_t *rq;
+	prio_array_t *array;
 
-	unsigned long long timestamp;
+	unsigned long sleep_avg;
+	long interactive_credit;
+	int activated;
 
-	unsigned long long sched_timestamp;
-	unsigned long long avg_sleep_per_cycle;
-	unsigned long long avg_delay_per_cycle;
-	unsigned long long avg_cpu_per_cycle;
-	unsigned long interactive_bonus, throughput_bonus;
-	unsigned long long cycle_count, total_sleep, total_cpu, total_delay;
-	unsigned long long sleepiness, cpu_usage_rate;
-	unsigned int pre_bonus_priority;
-	unsigned int eb_shares;
-	unsigned long long intr_wake_ups;
-	unsigned long long cpu_rate_cap;
-	unsigned long long cpu_rate_hard_cap;
-	unsigned long long total_sinbin;
-	struct timer_list sinbin_timer;
+	unsigned int time_slice, first_time_slice;
+};
 
-	unsigned long policy;
-	cpumask_t cpus_allowed;
-	unsigned int time_slice;
-#elif defined(CONFIG_XSCHED)
+struct nicksched_data {
 	int prio, static_prio;
-	struct list_head run_list;
+	prio_array_t *array;
+
+	unsigned long array_sequence;
+	int used_slice;
 
-	unsigned long long timestamp;
 	unsigned long total_time, sleep_time;
 	unsigned long sleep_avg;
+};
 
-	unsigned long policy;
-	cpumask_t cpus_allowed;
-	unsigned int time_slice;
-#elif defined(CONFIG_STAIRCASE)
+struct staircase_data {
 	int prio, static_prio;
-	struct list_head run_list;
-	unsigned long long timestamp;
 	unsigned long runtime, totalrun;
 	unsigned int burst;
 
-	unsigned long policy;
-	cpumask_t cpus_allowed;
 	unsigned int slice, time_slice;
-#elif defined(CONFIG_NICKSCHED)
-	int prio, static_prio;
-	struct list_head run_list;
-	prio_array_t *array;
+};
 
-	/* Scheduler variables follow. kernel/sched.c */
-	unsigned long array_sequence;
-	unsigned long long timestamp;
-	int used_slice;
+struct xsched_data {
+	int prio, static_prio;
 
 	unsigned long total_time, sleep_time;
 	unsigned long sleep_avg;
 
-	unsigned long policy;
-	cpumask_t cpus_allowed;
-#else
-	int prio, static_prio;
-	struct list_head run_list;
-	prio_array_t *array;
+	unsigned int time_slice;
+};
 
-	unsigned long sleep_avg;
-	long interactive_credit;
+struct task_struct {
+	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
+	struct thread_info *thread_info;
+	atomic_t usage;
+	unsigned long flags;	/* per process flags, defined below */
+	unsigned long ptrace;
+
+	int lock_depth;		/* Lock depth */
+
+	struct list_head run_list;
 	unsigned long long timestamp, last_ran;
-	int activated;
+
+	struct default_data default_data;
+	struct nicksched_data nicksched_data;
+	struct staircase_data staircase_data;
+	struct xsched_data xsched_data;
 
 	unsigned long policy;
 	cpumask_t cpus_allowed;
-	unsigned int time_slice, first_time_slice;
-#endif
 
 #ifdef CONFIG_SCHEDSTATS
 	struct sched_info sched_info;
@@ -835,7 +800,6 @@
 #define PF_SINBINNED	0x04000000	/* I am sinbinned */
 #define PF_UNPRIV_RT	0x08000000	/* I wanted to be RT but had insufficient privilege*/
 
-#if defined(CONFIG_SPA)
 /*
  * Scheduling statistics for a task/thread
  */
@@ -874,7 +838,6 @@
  * Get scheduling statistics for the nominated CPU
  */
 extern void get_cpu_sched_stats(unsigned int cpu, struct cpu_sched_stats *stats);
-#endif
 
 #ifdef CONFIG_SMP
 extern int set_cpus_allowed(task_t *p, cpumask_t new_mask);
@@ -959,9 +922,7 @@
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
 extern void FASTCALL(sched_fork(task_t * p));
-#if !defined(CONFIG_STAIRCASE)
 extern void FASTCALL(sched_exit(task_t * p));
-#endif
 
 extern int in_group_p(gid_t);
 extern int in_egroup_p(gid_t);
@@ -1260,14 +1221,10 @@
 	return p->thread_info->cpu;
 }
 
-#if defined(CONFIG_SPA)
-void set_task_cpu(struct task_struct *p, unsigned int cpu);
-#else
 static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
 {
 	p->thread_info->cpu = cpu;
 }
-#endif
 
 #else
 
@@ -1296,6 +1253,19 @@
 extern long sched_setaffinity(pid_t pid, cpumask_t new_mask);
 extern long sched_getaffinity(pid_t pid, cpumask_t *mask);
 
+#include <linux/scheduler.h>
+
+extern scheduler_t *current_scheduler;
+static inline int rt_task(const struct task_struct *p)
+{
+	if (current_scheduler->type == SCHED_NICKSCHED)
+		return nicksched_rt_task(p);
+	else if (current_scheduler->type == SCHED_DEFAULT)
+		return default_rt_task(p);
+	else
+		return 0;
+}
+
 #endif /* __KERNEL__ */
 
 #endif
Index: xx-sources/include/linux/scheduler.h
===================================================================
--- xx-sources.orig/include/linux/scheduler.h	2004-05-31 17:36:38.000000000 -0400
+++ xx-sources/include/linux/scheduler.h	2004-10-07 10:02:27.617246560 -0400
@@ -0,0 +1,144 @@
+#ifndef _LINUX_SCHEDULER_H
+#define _LINUX_SCHEDULER_H
+
+typedef asmlinkage void (schedule_tail_fn) (task_t *);
+typedef asmlinkage void (schedule_fn) (void);
+typedef void (scheduler_tick_fn) (int, int);
+typedef void (yield_fn) (void);
+typedef void FASTCALL((wait_for_completion_fn)(struct completion *));
+typedef int (idle_cpu_fn) (int);
+
+typedef int (default_wake_function_fn) (wait_queue_t *, unsigned, int, void *);
+typedef void FASTCALL((__wake_up_fn)(wait_queue_head_t *, unsigned int, int, void *));
+typedef void FASTCALL((__wake_up_locked_fn)(wait_queue_head_t *, unsigned int));
+typedef void FASTCALL((__wake_up_sync_fn)(wait_queue_head_t *, unsigned int, int));
+typedef void FASTCALL((complete_fn)(struct completion *));
+typedef void FASTCALL((complete_all_fn)(struct completion *));
+typedef void FASTCALL((interruptible_sleep_on_fn)(wait_queue_head_t *));
+typedef long FASTCALL((interruptible_sleep_on_timeout_fn)(wait_queue_head_t *, signed long));
+typedef void FASTCALL((sleep_on_fn)(wait_queue_head_t *));
+typedef long FASTCALL((sleep_on_timeout_fn)(wait_queue_head_t *, signed long));
+typedef void (set_user_nice_fn)(task_t *, long);
+typedef int (task_nice_fn)(const task_t *);
+typedef void (io_schedule_fn)(void);
+typedef long (io_schedule_timeout_fn)(long);
+typedef int (task_curr_fn)(const task_t *);
+typedef int FASTCALL((wake_up_process_fn)(struct task_struct *));
+typedef int FASTCALL((wake_up_state_fn)(struct task_struct *, unsigned int));
+typedef unsigned long (nr_running_fn)(void);
+typedef unsigned long (nr_uninterruptible_fn)(void);
+typedef unsigned long (nr_iowait_fn)(void);
+typedef unsigned long long (nr_context_switches_fn)(void);
+typedef void (sched_exec_fn)(void);
+typedef long (sched_setaffinity_fn)(pid_t, cpumask_t);
+typedef long (sched_getaffinity_fn)(pid_t, cpumask_t *);
+typedef asmlinkage long (sys_nice_fn)(int);
+typedef asmlinkage long (sys_sched_setscheduler_fn)(pid_t, int, struct sched_param __user *);
+typedef asmlinkage long (sys_sched_setparam_fn)(pid_t, struct sched_param __user *);
+typedef asmlinkage long (sys_sched_getscheduler_fn)(pid_t);
+typedef asmlinkage long (sys_sched_getparam_fn)(pid_t, struct sched_param __user *);
+typedef asmlinkage long (sys_sched_getaffinity_fn)(pid_t, unsigned int, unsigned long __user *);
+typedef asmlinkage long (sys_sched_yield_fn)(void);
+typedef asmlinkage long (sys_sched_rr_get_interval_fn)(pid_t, struct timespec __user *);
+typedef void (sched_init_fn)(void);
+typedef void (sched_init_smp_fn)(void);
+typedef int (migration_init_fn)(void);
+typedef void FASTCALL((sched_fork_fn)(task_t *));
+typedef void FASTCALL((sched_exit_fn)(task_t *));
+typedef void (init_idle_fn)(task_t *, int);
+typedef void FASTCALL((wake_up_new_task_fn)(struct task_struct *, unsigned long));
+typedef int (task_prio_fn)(const task_t *);
+#if defined(CONFIG_SMP)
+typedef int (set_cpus_allowed_fn) (task_t *, cpumask_t);
+typedef void (wait_task_inactive_fn)(task_t *);
+typedef void (kick_process_fn)(struct task_struct *);
+typedef void (init_sched_build_groups_fn)(struct sched_group groups[], cpumask_t span, int (*group_fn)(int cpu));
+typedef void (cpu_attach_domain_fn)(struct sched_domain *, int);
+#endif
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+typedef int (kernel_locked_fn) (void);
+typedef void (lock_kernel_fn) (void);
+typedef void (unlock_kernel_fn) (void);
+#endif
+#if defined(CONFIG_PREEMPT)
+typedef asmlinkage void (preempt_schedule_fn) (void);
+#endif
+
+struct scheduler_s
+{
+	schedule_tail_fn *schedule_tail_fn;
+	schedule_fn *schedule_fn;
+	scheduler_tick_fn *scheduler_tick_fn;
+	yield_fn *yield_fn;
+	wait_for_completion_fn *wait_for_completion_fn;
+	idle_cpu_fn *idle_cpu_fn;
+
+	default_wake_function_fn *default_wake_function_fn;
+	__wake_up_fn *__wake_up_fn;
+	__wake_up_locked_fn *__wake_up_locked_fn;
+	__wake_up_sync_fn *__wake_up_sync_fn;
+	complete_fn *complete_fn;
+	complete_all_fn *complete_all_fn;
+	interruptible_sleep_on_fn *interruptible_sleep_on_fn;
+	interruptible_sleep_on_timeout_fn *interruptible_sleep_on_timeout_fn;
+	sleep_on_fn *sleep_on_fn;
+	sleep_on_timeout_fn *sleep_on_timeout_fn;
+	set_user_nice_fn *set_user_nice_fn;
+	task_nice_fn *task_nice_fn;
+	io_schedule_fn *io_schedule_fn;
+	io_schedule_timeout_fn *io_schedule_timeout_fn;
+	task_curr_fn *task_curr_fn;
+	wake_up_process_fn *wake_up_process_fn;
+	wake_up_state_fn *wake_up_state_fn;
+	nr_running_fn *nr_running_fn;
+	nr_uninterruptible_fn *nr_uninterruptible_fn;
+	nr_iowait_fn *nr_iowait_fn;
+	nr_context_switches_fn *nr_context_switches_fn;
+	sched_exec_fn *sched_exec_fn;
+	sched_setaffinity_fn *sched_setaffinity_fn;
+	sched_getaffinity_fn *sched_getaffinity_fn;
+	sys_nice_fn *sys_nice_fn;
+	sys_sched_setscheduler_fn *sys_sched_setscheduler_fn;
+	sys_sched_setparam_fn *sys_sched_setparam_fn;
+	sys_sched_getscheduler_fn *sys_sched_getscheduler_fn;
+	sys_sched_getparam_fn *sys_sched_getparam_fn;
+	sys_sched_getaffinity_fn *sys_sched_getaffinity_fn;
+	sys_sched_yield_fn *sys_sched_yield_fn;
+	sys_sched_rr_get_interval_fn *sys_sched_rr_get_interval_fn;
+	sched_init_fn *sched_init_fn;
+	sched_init_smp_fn *sched_init_smp_fn;
+	migration_init_fn *migration_init_fn;
+	sched_fork_fn *sched_fork_fn;
+	sched_exit_fn *sched_exit_fn;
+	init_idle_fn *init_idle_fn;
+	wake_up_new_task_fn *wake_up_new_task_fn;
+	task_prio_fn *task_prio_fn;
+#if defined(CONFIG_SMP)
+	init_sched_build_groups_fn *init_sched_build_groups_fn;
+	cpu_attach_domain_fn *cpu_attach_domain_fn;
+	set_cpus_allowed_fn *set_cpus_allowed_fn;
+	wait_task_inactive_fn *wait_task_inactive_fn;
+	kick_process_fn *kick_process_fn;
+#endif
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+	kernel_locked_fn *kernel_locked_fn;
+	lock_kernel_fn *lock_kernel_fn;
+	unlock_kernel_fn *unlock_kernel_fn;
+#endif
+#if defined(CONFIG_PREEMPT)
+	preempt_schedule_fn *preempt_schedule_fn;
+#endif
+
+	const char *name;
+	const char *version;
+	const int type;
+};
+
+typedef struct scheduler_s scheduler_t;
+
+extern scheduler_t sched_default;
+extern scheduler_t sched_nicksched;
+extern scheduler_t sched_staircase;
+extern scheduler_t sched_xsched;
+
+#endif
Index: xx-sources/include/linux/smp_lock.h
===================================================================
--- xx-sources.orig/include/linux/smp_lock.h	2004-10-07 09:49:31.000000000 -0400
+++ xx-sources/include/linux/smp_lock.h	2004-10-07 10:02:27.618246408 -0400
@@ -7,9 +7,9 @@
 
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 
-extern int kernel_locked(void);
-extern void lock_kernel(void);
-extern void unlock_kernel(void);
+extern int kernel_locked (void);
+extern void lock_kernel (void);
+extern void unlock_kernel (void);
 
 #else
 
Index: xx-sources/include/linux/sysctl.h
===================================================================
--- xx-sources.orig/include/linux/sysctl.h	2004-10-07 09:59:57.524064192 -0400
+++ xx-sources/include/linux/sysctl.h	2004-10-07 10:02:27.620246104 -0400
@@ -61,7 +61,8 @@
 	CTL_DEV=7,		/* Devices */
 	CTL_BUS=8,		/* Busses */
 	CTL_ABI=9,		/* Binary emulation */
-	CTL_CPU=10		/* CPU stuff (speed scaling, etc) */
+	CTL_CPU=10,		/* CPU stuff (speed scaling, etc) */
+	CTL_SCHED=11
 };
 
 /* CTL_BUS names: */
@@ -134,10 +135,7 @@
 	KERN_SPARC_SCONS_PWROFF=64, /* int: serial console power-off halt */
 	KERN_HZ_TIMER=65,	/* int: hz timer on or off */
 	KERN_UNKNOWN_NMI_PANIC=66, /* int: unknown nmi panic flag */
-	KERN_SCHED_TIMESLICE=67, /* int: base timeslice for scheduler */
-	KERN_INTERACTIVE=68,	/* interactive tasks can have cpu bursts */
-	KERN_COMPUTE=69,	/* adjust timeslices for a compute server */
-	KERN_CPU_SCHED=70,	/* CPU scheduler stuff */
+	KERN_CPU_SCHED=67,	/* CPU scheduler stuff */
 };
 
 
Index: xx-sources/init/main.c
===================================================================
--- xx-sources.orig/init/main.c	2004-10-07 10:01:20.974377816 -0400
+++ xx-sources/init/main.c	2004-10-07 10:02:27.621245952 -0400
@@ -584,6 +584,8 @@
 	signals_init();
 	/* rootfs populating might need page-writeback */
 	page_writeback_init();
+
+	
 #ifdef CONFIG_PROC_FS
 	proc_root_init();
 #endif
@@ -708,9 +710,7 @@
 static int init(void * unused)
 {
 	lock_kernel();
-#if defined(CONFIG_STAIRCASE)
-	current->prio = MAX_PRIO - 1;
-#endif
+	current->staircase_data.prio = MAX_PRIO - 1;
 	/*
 	 * Tell the world that we're going to be the grim
 	 * reaper of innocent orphaned children.
Index: xx-sources/ipc/mqueue.c
===================================================================
--- xx-sources.orig/ipc/mqueue.c	2004-10-07 09:48:30.877450344 -0400
+++ xx-sources/ipc/mqueue.c	2004-10-07 10:02:27.623245648 -0400
@@ -383,6 +383,16 @@
 	return retval;
 }
 
+static inline int wq_static_prio_test(task_t *a, task_t *b)
+{
+	if (current_scheduler->type == SCHED_NICKSCHED)
+		return (a->nicksched_data.static_prio <= b->nicksched_data.static_prio);
+	if (current_scheduler->type == SCHED_DEFAULT)
+		return (a->default_data.static_prio <= b->default_data.static_prio);
+
+	return 0;
+}
+
 /* Adds current to info->e_wait_q[sr] before element with smaller prio */
 static void wq_add(struct mqueue_inode_info *info, int sr,
 			struct ext_wait_queue *ewp)
@@ -392,7 +402,7 @@
 	ewp->task = current;
 
 	list_for_each_entry(walk, &info->e_wait_q[sr].list, list) {
-		if (walk->task->static_prio <= current->static_prio) {
+		if (wq_static_prio_test(walk->task, current)) {
 			list_add_tail(&ewp->list, &walk->list);
 			return;
 		}
Index: xx-sources/kernel/Kconfig-extra.xx
===================================================================
--- xx-sources.orig/kernel/Kconfig-extra.xx	2004-10-07 09:51:55.820294296 -0400
+++ xx-sources/kernel/Kconfig-extra.xx	2004-10-07 10:02:27.624245496 -0400
@@ -1,5 +1,132 @@
 menu "Experimental xx options"
 
-source "kernel/sched/Kconfig"
+menu "Process scheduling policies"
+
+config SCHED_NONE
+	bool "Default"
+	default y
+	help
+	  This is the default scheduler as is included in the -mm kernels.
+	  It contains the sched domains code by Nick Piggin and some tweaks
+	  to the scheduling code, but no significant changes.
+
+config NICKSCHED
+	bool "Nicksched"
+	default n
+	help
+	  This is a scheduler written by Nick Piggin.  It is quite fast,
+	  responsive, and does well under heavy loads.
+
+	  In this scheduler, the architecture is still pretty similar
+	  to the original scheduler - there are still two priority
+	  arrays, for example.  The differences are in how the bonuses
+ 	  are given.
+
+	  In Nicksched, a task's priority and bonuses are based on its
+	  "sleep time."  The scheduler keeps track of a task's history -
+	  how long the task was running.  Each task is also assigned one
+	  of three running modes: sleeping (not active), running (using
+	  the CPU), and waiting (waiting for CPU time).  Tasks are given
+	  priority bonuses based on these two factors.  This is a lot
+	  simpler than the original interactivity/credit model, since
+	  it mostly uses linear functions and bit shifts to keep
+	  calculations simple and quick.  And with this method, it
+	  is a lot easier to scale timeslices - for example, the
+	  timeslice given is scaled against the priority of the other
+	  tasks, so a lower priority process can still get larger
+	  timeslices if there aren't higher priority processes using
+	  the CPU.
+
+config SPA
+	bool "SPA-Zaphod"
+	default n
+	depends on EXPERIMENTAL && DISABLED
+	help
+	  SPA was written by Peter Williams.
+	
+	  SPA stands for Single Priority Array, which is the key
+	  architectural difference between SPA and the original scheduler.
+	  Traditionally, the original scheduler uses two priority arrays
+	  to manage running tasks: an active and an expired array.
+	  Tasks queue in the active array, and when they use their timeslice,
+	  they are queued into the expired array.  And when they are given
+	  a new slice, they are requeued back into the active array to be
+	  run again.
+
+	  While this kind of structure is good for scalability, unfortunately
+	  it suffers from a couple weaknesses on an interactivity-based desktop
+	  system.  Having two priority arrays is unneccessary overhead when
+	  only one is really needed.  Also, it is possible to run into queueing
+ 	  race conditions under certain circumstances.
+ 
+	  SPA changes the system to a single priority array, so the task just
+	  uses its timeslice and gets requeued back into the same array.  That
+	  way, we don't waste time doing extra queues into a different expired
+	  array, and then back again.
+
+	  This is the 'Zaphod' variation of SPA, which allows runtime switching
+	  between two bonus calculation methods: priority-based, and
+	  entitlement-based.  These can be echoed into the proc filesystem as
+	  'pb' and 'eb', respectively.
+
+	  Priority-based bonus calculation is the original calcuation that SPA
+	  has used since its inception.  Its system is still relatively similar
+	  to the default scheduler's: it will vary a task's priority according to
+	  two criteria: interactivity and throughput.  A task is interactive if it
+	  spends the majority of its time sleeping (that is, waiting for user input),
+	  and it is throughput-heavy if it is hogging the cpu most of the time.
+	  Interactive tasks are promoted, and cpu hogs are punished.  The system
+	  keeps track of various aspects of a task's execution time, and uses
+	  some miniature Kalman filters to estimate the actual cpu usage vs
+	  running time.
+
+	  Entitlement-based bonuses, on the other hand, are a more radical departure
+	  from the original priority calculation.  It is a throwback to the original
+	  Entitlement Based Scheduler (EBS), which Peter Williams also contributed to.
+	  With entitlement, a task has a certain amount of 'shares.'  This ranges from
+	  0-420.  The amount of shares a task has determines how much CPU time it will
+	  get, and the rate at which it will get that cpu.  While it doesn't sound
+	  much different from the traditional priority-based system, entitlement actually
+	  completely discards the idea of priority altogether and varies a task's shares
+	  according to quite a few circumstances, making it quite difficult to implement.
+
+config STAIRCASE
+	bool "Staircase"
+	default n
+	help
+	  Staircase was written by Con Kolivas.
+
+	  The staircase scheduler operates on a similar principle to the
+	  SPA scheduler.  Like SPA, it has only one priority array that
+	  tasks will remain in.  The difference is in how the "bonuses"
+	  are calculated.  Every task starts with a certain "deadline,"
+	  or "burst" as it's called in newer versions.  The deadline
+	  is used to calculate how large a timeslice the task will get.
+	  A task will be first activated with a relatively high deadline,
+	  and therefore get large timeslices.  However, each time it uses
+	  its timeslice and is requeued, its deadline is decreased.  So
+	  on the next run, it will get a smaller timeslice than before.
+	  And likewise, its timeslice will keep "stepping down" each
+	  requeue - like a staircase.  And, of course, there are other
+	  factors used in calculation.  For example, the actual timeslice
+	  size is scaled according to priority.  Also, the task has a
+	  maximum deadline based on its priority.  So a task with a certain
+	  priority will only be able to go so high on the staircase.
+	  Another task with a higher priority will also have a limit on the
+	  staircase, but its best deadline will be higher than the other
+	  task's.  Tasks will also regain deadline due to bonuses.
+
+config XSCHED
+	bool "Xsched"
+	default n
+	depends on EXPERIMENTAL
+	help
+	  This is a rework of the scheduler by xiphux.  At the moment, not
+	  very much of it is original code.  It's pretty much the prio-slot
+	  based structure from SPA by Peter Williams, with the priority
+	  bonus algorithms from Nicksched by Nick Piggin.  It's still
+	  extremely experimental.
+
+endmenu
 
 endmenu
Index: xx-sources/kernel/Makefile
===================================================================
--- xx-sources.orig/kernel/Makefile	2004-10-07 10:02:06.582444336 -0400
+++ xx-sources/kernel/Makefile	2004-10-07 10:02:27.625245344 -0400
@@ -2,11 +2,13 @@
 # Makefile for the linux kernel.
 #
 
-obj-$(CONFIG_SCHED_NONE)	= default-sched.o
-obj-$(CONFIG_NICKSCHED)		= nicksched-sched.o
-obj-$(CONFIG_STAIRCASE)		= staircase-sched.o
-obj-$(CONFIG_XSCHED)		= xsched-sched.o
-obj-$(CONFIG_SPA)		= spa-sched.o
+obj-y				 = sched.o null-sched.o
+
+obj-$(CONFIG_SCHED_NONE)	+= default-sched.o
+obj-$(CONFIG_NICKSCHED)		+= nicksched-sched.o
+obj-$(CONFIG_STAIRCASE)		+= staircase-sched.o
+obj-$(CONFIG_XSCHED)		+= xsched-sched.o
+obj-$(CONFIG_SPA)		+= spa-sched.o
 
 obj-y    += fork.o exec_domain.o panic.o printk.o profile.o \
 	    exit.o itimer.o time.o softirq.o resource.o \
Index: xx-sources/kernel/default-sched.c
===================================================================
--- xx-sources.orig/kernel/default-sched.c	2004-10-07 09:57:04.412381168 -0400
+++ xx-sources/kernel/default-sched.c	2004-10-07 10:08:53.580571216 -0400
@@ -51,8 +51,7 @@
 
 #include <asm/unistd.h>
 
-const char *scheduler_name = "Default scheduler";
-const char *scheduler_version = "NA";
+#define DEFAULT_SCHEDULER_VERSION "N/A"
 
 #ifdef CONFIG_NUMA
 #define cpu_to_node_mask(cpu) node_to_cpumask(cpu_to_node(cpu))
@@ -67,7 +66,7 @@
  */
 #define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
 #define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
-#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->default_data.static_prio)
 
 /*
  * 'User priority' is the nice value converted to something we
@@ -75,7 +74,7 @@
  * it's a [ 0 ... 39 ] range.
  */
 #define USER_PRIO(p)		((p)-MAX_RT_PRIO)
-#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
+#define TASK_USER_PRIO(p)	USER_PRIO((p)->default_data.static_prio)
 #define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
 
 /*
@@ -134,7 +133,7 @@
  */
 
 #define CURRENT_BONUS(p) \
-	(NS_TO_JIFFIES((p)->sleep_avg) * MAX_BONUS / \
+	(NS_TO_JIFFIES((p)->default_data.sleep_avg) * MAX_BONUS / \
 		MAX_SLEEP_AVG)
 
 #ifdef CONFIG_SMP
@@ -153,20 +152,20 @@
 	(SCALE(TASK_NICE(p), 40, MAX_BONUS) + INTERACTIVE_DELTA)
 
 #define TASK_INTERACTIVE(p) \
-	((p)->prio <= (p)->static_prio - DELTA(p))
+	((p)->default_data.prio <= (p)->default_data.static_prio - DELTA(p))
 
 #define INTERACTIVE_SLEEP(p) \
 	(JIFFIES_TO_NS(MAX_SLEEP_AVG * \
 		(MAX_BONUS / 2 + DELTA((p)) + 1) / MAX_BONUS - 1))
 
 #define HIGH_CREDIT(p) \
-	((p)->interactive_credit > CREDIT_LIMIT)
+	((p)->default_data.interactive_credit > CREDIT_LIMIT)
 
 #define LOW_CREDIT(p) \
-	((p)->interactive_credit < -CREDIT_LIMIT)
+	((p)->default_data.interactive_credit < -CREDIT_LIMIT)
 
 #define TASK_PREEMPTS_CURR(p, rq) \
-	((p)->prio < (rq)->curr->prio)
+	((p)->default_data.prio < (rq)->curr->default_data.prio)
 
 /*
  * task_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]
@@ -182,10 +181,10 @@
 
 static unsigned int task_timeslice(task_t *p)
 {
-	if (p->static_prio < NICE_TO_PRIO(0))
-		return SCALE_PRIO(DEF_TIMESLICE*4, p->static_prio);
+	if (p->default_data.static_prio < NICE_TO_PRIO(0))
+		return SCALE_PRIO(DEF_TIMESLICE*4, p->default_data.static_prio);
 	else
-		return SCALE_PRIO(DEF_TIMESLICE, p->static_prio);
+		return SCALE_PRIO(DEF_TIMESLICE, p->default_data.static_prio);
 }
 #define task_hot(p, now, sd) ((long long) ((now) - (p)->last_ran)	\
 				< (long long) (sd)->cache_hot_time)
@@ -196,8 +195,6 @@
 
 #define BITMAP_SIZE ((((MAX_PRIO+1+7)/8)+sizeof(long)-1)/sizeof(long))
 
-typedef struct runqueue runqueue_t;
-
 struct prio_array {
 	unsigned int nr_active;
 	unsigned long bitmap[BITMAP_SIZE];
@@ -273,7 +270,7 @@
 	unsigned long ttwu_attempts;
 	unsigned long ttwu_moved;
 
-	/* wake_up_new_task() stats */
+	/* default_wake_up_new_task() stats */
 	unsigned long wunt_cnt;
 	unsigned long wunt_moved;
 
@@ -455,7 +452,7 @@
  * queue is empty, without explicitly dequeuing and requeuing tasks in the
  * expired queue.  (Interactive tasks may be requeued directly to the
  * active queue, thus delaying tasks in the expired queue from running;
- * see scheduler_tick()).
+ * see default_scheduler_tick()).
  *
  * This function is only called from sched_info_arrive(), rather than
  * dequeue_task(). Even though a task may be queued and dequeued multiple
@@ -500,7 +497,7 @@
  * and requeuing any tasks, we are interested in queuing to either. It
  * is unusual but not impossible for tasks to be dequeued and immediately
  * requeued in the same or another array: this can happen in sched_yield(),
- * set_user_nice(), and even load_balance() as it moves tasks from runqueue
+ * default_set_user_nice(), and even load_balance() as it moves tasks from runqueue
  * to runqueue.
  *
  * This function is only called from enqueue_task(), but also only updates
@@ -560,17 +557,17 @@
 {
 	array->nr_active--;
 	list_del(&p->run_list);
-	if (list_empty(array->queue + p->prio))
-		__clear_bit(p->prio, array->bitmap);
+	if (list_empty(array->queue + p->default_data.prio))
+		__clear_bit(p->default_data.prio, array->bitmap);
 }
 
 static void enqueue_task(struct task_struct *p, prio_array_t *array)
 {
 	sched_info_queued(p);
-	list_add_tail(&p->run_list, array->queue + p->prio);
-	__set_bit(p->prio, array->bitmap);
+	list_add_tail(&p->run_list, array->queue + p->default_data.prio);
+	__set_bit(p->default_data.prio, array->bitmap);
 	array->nr_active++;
-	p->array = array;
+	p->default_data.array = array;
 }
 
 /*
@@ -580,10 +577,10 @@
  */
 static inline void enqueue_task_head(struct task_struct *p, prio_array_t *array)
 {
-	list_add(&p->run_list, array->queue + p->prio);
-	__set_bit(p->prio, array->bitmap);
+	list_add(&p->run_list, array->queue + p->default_data.prio);
+	__set_bit(p->default_data.prio, array->bitmap);
 	array->nr_active++;
-	p->array = array;
+	p->default_data.array = array;
 }
 
 /*
@@ -604,12 +601,12 @@
 {
 	int bonus, prio;
 
-	if (rt_task(p))
-		return p->prio;
+	if (default_rt_task(p))
+		return p->default_data.prio;
 
 	bonus = CURRENT_BONUS(p) - MAX_BONUS / 2;
 
-	prio = p->static_prio - bonus;
+	prio = p->default_data.static_prio - bonus;
 	if (prio < MAX_RT_PRIO)
 		prio = MAX_RT_PRIO;
 	if (prio > MAX_PRIO-1)
@@ -652,12 +649,12 @@
 		 * prevent them suddenly becoming cpu hogs and starving
 		 * other processes.
 		 */
-		if (p->mm && p->activated != -1 &&
+		if (p->mm && p->default_data.activated != -1 &&
 			sleep_time > INTERACTIVE_SLEEP(p)) {
-				p->sleep_avg = JIFFIES_TO_NS(MAX_SLEEP_AVG -
+				p->default_data.sleep_avg = JIFFIES_TO_NS(MAX_SLEEP_AVG -
 						DEF_TIMESLICE);
 				if (!HIGH_CREDIT(p))
-					p->interactive_credit++;
+					p->default_data.interactive_credit++;
 		} else {
 			/*
 			 * The lower the sleep avg a task has the more
@@ -678,12 +675,12 @@
 			 * sleep are limited in their sleep_avg rise as they
 			 * are likely to be cpu hogs waiting on I/O
 			 */
-			if (p->activated == -1 && !HIGH_CREDIT(p) && p->mm) {
-				if (p->sleep_avg >= INTERACTIVE_SLEEP(p))
+			if (p->default_data.activated == -1 && !HIGH_CREDIT(p) && p->mm) {
+				if (p->default_data.sleep_avg >= INTERACTIVE_SLEEP(p))
 					sleep_time = 0;
-				else if (p->sleep_avg + sleep_time >=
+				else if (p->default_data.sleep_avg + sleep_time >=
 						INTERACTIVE_SLEEP(p)) {
-					p->sleep_avg = INTERACTIVE_SLEEP(p);
+					p->default_data.sleep_avg = INTERACTIVE_SLEEP(p);
 					sleep_time = 0;
 				}
 			}
@@ -696,17 +693,17 @@
 			 * task spends sleeping, the higher the average gets -
 			 * and the higher the priority boost gets as well.
 			 */
-			p->sleep_avg += sleep_time;
+			p->default_data.sleep_avg += sleep_time;
 
-			if (p->sleep_avg > NS_MAX_SLEEP_AVG) {
-				p->sleep_avg = NS_MAX_SLEEP_AVG;
+			if (p->default_data.sleep_avg > NS_MAX_SLEEP_AVG) {
+				p->default_data.sleep_avg = NS_MAX_SLEEP_AVG;
 				if (!HIGH_CREDIT(p))
-					p->interactive_credit++;
+					p->default_data.interactive_credit++;
 			}
 		}
 	}
 
-	p->prio = effective_prio(p);
+	p->default_data.prio = effective_prio(p);
 }
 
 /*
@@ -735,7 +732,7 @@
 	 * This checks to make sure it's not an uninterruptible task
 	 * that is now waking up.
 	 */
-	if (!p->activated) {
+	if (!p->default_data.activated) {
 		/*
 		 * Tasks which were woken up by interrupts (ie. hw events)
 		 * are most likely of interactive nature. So we give them
@@ -744,13 +741,13 @@
 		 * on a CPU, first time around:
 		 */
 		if (in_interrupt())
-			p->activated = 2;
+			p->default_data.activated = 2;
 		else {
 			/*
 			 * Normal first-time wakeups get a credit too for
 			 * on-runqueue time, but it will be weighted down:
 			 */
-			p->activated = 1;
+			p->default_data.activated = 1;
 		}
 	}
 	p->timestamp = now;
@@ -766,8 +763,8 @@
 	rq->nr_running--;
 	if (p->state == TASK_UNINTERRUPTIBLE)
 		rq->nr_uninterruptible++;
-	dequeue_task(p, p->array);
-	p->array = NULL;
+	dequeue_task(p, p->default_data.array);
+	p->default_data.array = NULL;
 }
 
 /*
@@ -803,7 +800,7 @@
  * task_curr - is this task currently executing on a CPU?
  * @p: the task in question.
  */
-inline int task_curr(const task_t *p)
+inline int default_task_curr(const task_t *p)
 {
 	return cpu_curr(task_cpu(p)) == p;
 }
@@ -840,7 +837,7 @@
 	 * If the task is not on a runqueue (and not running), then
 	 * it is sufficient to simply update the task's cpu field.
 	 */
-	if (!p->array && !task_running(rq, p)) {
+	if (!p->default_data.array && !task_running(rq, p)) {
 		set_task_cpu(p, dest_cpu);
 		return 0;
 	}
@@ -854,7 +851,7 @@
 }
 
 /*
- * wait_task_inactive - wait for a thread to unschedule.
+ * default_wait_task_inactive - wait for a thread to unschedule.
  *
  * The caller must ensure that the task *will* unschedule sometime soon,
  * else this function might spin for a *long* time. This function can't
@@ -862,7 +859,7 @@
  * smp_call_function() if an IPI is sent by the same process we are
  * waiting to become inactive.
  */
-void wait_task_inactive(task_t * p)
+void default_wait_task_inactive(task_t * p)
 {
 	unsigned long flags;
 	runqueue_t *rq;
@@ -871,7 +868,7 @@
 repeat:
 	rq = task_rq_lock(p, &flags);
 	/* Must be off runqueue entirely, not preempted. */
-	if (unlikely(p->array)) {
+	if (unlikely(p->default_data.array)) {
 		/* If it's preempted, we yield.  It could be a while. */
 		preempted = !task_running(rq, p);
 		task_rq_unlock(rq, &flags);
@@ -884,13 +881,13 @@
 }
 
 /***
- * kick_process - kick a running thread to enter/exit the kernel
+ * default_kick_process - kick a running thread to enter/exit the kernel
  * @p: the to-be-kicked thread
  *
  * Cause a process which is running on another CPU to enter
  * kernel-mode, without any delay. (to get signals handled.)
  */
-void kick_process(task_t *p)
+void default_kick_process(task_t *p)
 {
 	int cpu;
 
@@ -901,8 +898,6 @@
 	preempt_enable();
 }
 
-EXPORT_SYMBOL_GPL(kick_process);
-
 /*
  * Return a low guess at the load of a migration-source cpu.
  *
@@ -1000,7 +995,7 @@
 	if (!(old_state & state))
 		goto out;
 
-	if (p->array)
+	if (p->default_data.array)
 		goto out_running;
 
 	cpu = task_cpu(p);
@@ -1079,7 +1074,7 @@
 		old_state = p->state;
 		if (!(old_state & state))
 			goto out;
-		if (p->array)
+		if (p->default_data.array)
 			goto out_running;
 
 		this_cpu = smp_processor_id();
@@ -1094,7 +1089,7 @@
 		 * Tasks on involuntary sleep don't earn
 		 * sleep_avg beyond just interactive state.
 		 */
-		p->activated = -1;
+		p->default_data.activated = -1;
 	}
 
 	/*
@@ -1120,15 +1115,13 @@
 	return success;
 }
 
-int fastcall wake_up_process(task_t * p)
+int fastcall default_wake_up_process(task_t * p)
 {
 	return try_to_wake_up(p, TASK_STOPPED | TASK_TRACED |
 		       		 TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE, 0);
 }
 
-EXPORT_SYMBOL(wake_up_process);
-
-int fastcall wake_up_state(task_t *p, unsigned int state)
+int fastcall default_wake_up_state(task_t *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
 }
@@ -1142,7 +1135,7 @@
  * Perform scheduler related setup for a newly forked process p.
  * p is forked by current.
  */
-void fastcall sched_fork(task_t *p)
+void fastcall default_sched_fork(task_t *p)
 {
 	/*
 	 * We mark the process as running here, but have not actually
@@ -1152,7 +1145,7 @@
 	 */
 	p->state = TASK_RUNNING;
 	INIT_LIST_HEAD(&p->run_list);
-	p->array = NULL;
+	p->default_data.array = NULL;
 	spin_lock_init(&p->switch_lock);
 #ifdef CONFIG_SCHEDSTATS
 	memset(&p->sched_info, 0, sizeof(p->sched_info));
@@ -1172,21 +1165,21 @@
 	 * resulting in more scheduling fairness.
 	 */
 	local_irq_disable();
-	p->time_slice = (current->time_slice + 1) >> 1;
+	p->default_data.time_slice = (current->default_data.time_slice + 1) >> 1;
 	/*
 	 * The remainder of the first timeslice might be recovered by
 	 * the parent if the child exits early enough.
 	 */
-	p->first_time_slice = 1;
-	current->time_slice >>= 1;
+	p->default_data.first_time_slice = 1;
+	current->default_data.time_slice >>= 1;
 	p->timestamp = sched_clock();
-	if (unlikely(!current->time_slice)) {
+	if (unlikely(!current->default_data.time_slice)) {
 		/*
 		 * This case is rare, it happens when the parent has only
 		 * a single jiffy left from its timeslice. Taking the
 		 * runqueue lock is not a problem.
 		 */
-		current->time_slice = 1;
+		current->default_data.time_slice = 1;
 		preempt_disable();
 		scheduler_tick(0, 0);
 		local_irq_enable();
@@ -1196,13 +1189,13 @@
 }
 
 /*
- * wake_up_new_task - wake up a newly created task for the first time.
+ * default_wake_up_new_task - wake up a newly created task for the first time.
  *
  * This function will do some initial scheduler statistics housekeeping
  * that must be done for every newly created context, then puts the task
  * on the runqueue and wakes it.
  */
-void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
+void fastcall default_wake_up_new_task(task_t * p, unsigned long clone_flags)
 {
 	unsigned long flags;
 	int this_cpu, cpu;
@@ -1221,12 +1214,12 @@
 	 * from forking tasks that are max-interactive. The parent
 	 * (current) is done further down, under its lock.
 	 */
-	p->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(p) *
+	p->default_data.sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(p) *
 		CHILD_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
 
-	p->interactive_credit = 0;
+	p->default_data.interactive_credit = 0;
 
-	p->prio = effective_prio(p);
+	p->default_data.prio = effective_prio(p);
 
 	if (likely(cpu == this_cpu)) {
 		if (!(clone_flags & CLONE_VM)) {
@@ -1235,13 +1228,13 @@
 			 * do child-runs-first in anticipation of an exec. This
 			 * usually avoids a lot of COW overhead.
 			 */
-			if (unlikely(!current->array))
+			if (unlikely(!current->default_data.array))
 				__activate_task(p, rq);
 			else {
-				p->prio = current->prio;
+				p->default_data.prio = current->default_data.prio;
 				list_add_tail(&p->run_list, &current->run_list);
-				p->array = current->array;
-				p->array->nr_active++;
+				p->default_data.array = current->default_data.array;
+				p->default_data.array->nr_active++;
 				rq->nr_running++;
 			}
 			set_need_resched();
@@ -1276,7 +1269,7 @@
 		task_rq_unlock(rq, &flags);
 		this_rq = task_rq_lock(current, &flags);
 	}
-	current->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(current) *
+	current->default_data.sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(current) *
 		PARENT_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
 	task_rq_unlock(this_rq, &flags);
 }
@@ -1290,7 +1283,7 @@
  * artificially, because any timeslice recovered here
  * was given away by the parent in the first place.)
  */
-void fastcall sched_exit(task_t * p)
+void fastcall default_sched_exit(task_t * p)
 {
 	unsigned long flags;
 	runqueue_t *rq;
@@ -1300,14 +1293,14 @@
 	 * the sleep_avg of the parent as well.
 	 */
 	rq = task_rq_lock(p->parent, &flags);
-	if (p->first_time_slice) {
-		p->parent->time_slice += p->time_slice;
-		if (unlikely(p->parent->time_slice > task_timeslice(p)))
-			p->parent->time_slice = task_timeslice(p);
-	}
-	if (p->sleep_avg < p->parent->sleep_avg)
-		p->parent->sleep_avg = p->parent->sleep_avg /
-		(EXIT_WEIGHT + 1) * EXIT_WEIGHT + p->sleep_avg /
+	if (p->default_data.first_time_slice) {
+		p->parent->default_data.time_slice += p->default_data.time_slice;
+		if (unlikely(p->parent->default_data.time_slice > task_timeslice(p)))
+			p->parent->default_data.time_slice = task_timeslice(p);
+	}
+	if (p->default_data.sleep_avg < p->parent->default_data.sleep_avg)
+		p->parent->default_data.sleep_avg = p->parent->default_data.sleep_avg /
+		(EXIT_WEIGHT + 1) * EXIT_WEIGHT + p->default_data.sleep_avg /
 		(EXIT_WEIGHT + 1);
 	task_rq_unlock(rq, &flags);
 }
@@ -1356,7 +1349,7 @@
  * schedule_tail - first thing a freshly forked thread must call.
  * @prev: the thread we just switched away from.
  */
-asmlinkage void schedule_tail(task_t *prev)
+asmlinkage void default_schedule_tail(task_t *prev)
 {
 	finish_task_switch(prev);
 
@@ -1400,7 +1393,7 @@
  * threads, current number of uninterruptible-sleeping threads, total
  * number of context switches performed since bootup.
  */
-unsigned long nr_running(void)
+unsigned long default_nr_running(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1410,7 +1403,7 @@
 	return sum;
 }
 
-unsigned long nr_uninterruptible(void)
+unsigned long default_nr_uninterruptible(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1420,7 +1413,7 @@
 	return sum;
 }
 
-unsigned long long nr_context_switches(void)
+unsigned long long default_nr_context_switches(void)
 {
 	unsigned long long i, sum = 0;
 
@@ -1430,7 +1423,7 @@
 	return sum;
 }
 
-unsigned long nr_iowait(void)
+unsigned long default_nr_iowait(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1576,7 +1569,7 @@
  * execve() is a valuable balancing opportunity, because at this point
  * the task has the smallest effective memory and cache footprint.
  */
-void sched_exec(void)
+void default_sched_exec(void)
 {
 	struct sched_domain *tmp, *sd = NULL;
 	int new_cpu, this_cpu = get_cpu();
@@ -2209,10 +2202,6 @@
 	return ret;
 }
 
-DEFINE_PER_CPU(struct kernel_stat, kstat);
-
-EXPORT_PER_CPU_SYMBOL(kstat);
-
 /*
  * We place interactive tasks back into the active array, if possible.
  *
@@ -2227,7 +2216,7 @@
 	((STARVATION_LIMIT && ((rq)->expired_timestamp && \
 		(jiffies - (rq)->expired_timestamp >= \
 			STARVATION_LIMIT * ((rq)->nr_running) + 1))) || \
-			((rq)->curr->static_prio > (rq)->best_expired_prio))
+			((rq)->curr->default_data.static_prio > (rq)->best_expired_prio))
 
 /*
  * This function gets called by the timer code, with HZ frequency.
@@ -2236,7 +2225,7 @@
  * It also gets called by the fork code, when changing the parent's
  * timeslices.
  */
-void scheduler_tick(int user_ticks, int sys_ticks)
+void default_scheduler_tick(int user_ticks, int sys_ticks)
 {
 	int cpu = smp_processor_id();
 	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
@@ -2274,7 +2263,7 @@
 	cpustat->system += sys_ticks;
 
 	/* Task might have expired already, but not scheduled off yet */
-	if (p->array != rq->active) {
+	if (p->default_data.array != rq->active) {
 		set_tsk_need_resched(p);
 		goto out;
 	}
@@ -2286,14 +2275,14 @@
 	 * timeslice. This makes it possible for interactive tasks
 	 * to use up their timeslices at their highest priority levels.
 	 */
-	if (rt_task(p)) {
+	if (default_rt_task(p)) {
 		/*
 		 * RR tasks need a special form of timeslice management.
 		 * FIFO tasks have no timeslices.
 		 */
-		if ((p->policy == SCHED_RR) && !--p->time_slice) {
-			p->time_slice = task_timeslice(p);
-			p->first_time_slice = 0;
+		if ((p->policy == SCHED_RR) && !--p->default_data.time_slice) {
+			p->default_data.time_slice = task_timeslice(p);
+			p->default_data.first_time_slice = 0;
 			set_tsk_need_resched(p);
 
 			/* put it at the end of the queue: */
@@ -2302,19 +2291,19 @@
 		}
 		goto out_unlock;
 	}
-	if (!--p->time_slice) {
+	if (!--p->default_data.time_slice) {
 		dequeue_task(p, rq->active);
 		set_tsk_need_resched(p);
-		p->prio = effective_prio(p);
-		p->time_slice = task_timeslice(p);
-		p->first_time_slice = 0;
+		p->default_data.prio = effective_prio(p);
+		p->default_data.time_slice = task_timeslice(p);
+		p->default_data.first_time_slice = 0;
 
 		if (!rq->expired_timestamp)
 			rq->expired_timestamp = jiffies;
 		if (!TASK_INTERACTIVE(p) || EXPIRED_STARVING(rq)) {
 			enqueue_task(p, rq->expired);
-			if (p->static_prio < rq->best_expired_prio)
-				rq->best_expired_prio = p->static_prio;
+			if (p->default_data.static_prio < rq->best_expired_prio)
+				rq->best_expired_prio = p->default_data.static_prio;
 		} else
 			enqueue_task(p, rq->active);
 	} else {
@@ -2335,13 +2324,13 @@
 		 * delta range with at least TIMESLICE_GRANULARITY to requeue.
 		 */
 		if (TASK_INTERACTIVE(p) && !((task_timeslice(p) -
-			p->time_slice) % TIMESLICE_GRANULARITY(p)) &&
-			(p->time_slice >= TIMESLICE_GRANULARITY(p)) &&
-			(p->array == rq->active)) {
+			p->default_data.time_slice) % TIMESLICE_GRANULARITY(p)) &&
+			(p->default_data.time_slice >= TIMESLICE_GRANULARITY(p)) &&
+			(p->default_data.array == rq->active)) {
 
 			dequeue_task(p, rq->active);
 			set_tsk_need_resched(p);
-			p->prio = effective_prio(p);
+			p->default_data.prio = effective_prio(p);
 			enqueue_task(p, rq->active);
 		}
 	}
@@ -2444,9 +2433,9 @@
 		 * task from using an unfair proportion of the
 		 * physical cpu's resources. -ck
 		 */
-		if (((smt_curr->time_slice * (100 - sd->per_cpu_gain) / 100) >
-			task_timeslice(p) || rt_task(smt_curr)) &&
-			p->mm && smt_curr->mm && !rt_task(p))
+		if (((smt_curr->default_data.time_slice * (100 - sd->per_cpu_gain) / 100) >
+			task_timeslice(p) || default_rt_task(smt_curr)) &&
+			p->mm && smt_curr->mm && !default_rt_task(p))
 				ret = 1;
 
 		/*
@@ -2454,9 +2443,9 @@
 		 * or wake it up if it has been put to sleep for priority
 		 * reasons.
 		 */
-		if ((((p->time_slice * (100 - sd->per_cpu_gain) / 100) >
-			task_timeslice(smt_curr) || rt_task(p)) &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
+		if ((((p->default_data.time_slice * (100 - sd->per_cpu_gain) / 100) >
+			task_timeslice(smt_curr) || default_rt_task(p)) &&
+			smt_curr->mm && p->mm && !default_rt_task(smt_curr)) ||
 			(smt_curr == smt_rq->idle && smt_rq->nr_running))
 				resched_task(smt_curr);
 	}
@@ -2476,90 +2465,6 @@
 }
 #endif
 
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
-
-void fastcall add_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(((int)preempt_count() < 0));
-	preempt_count() += val;
-	/*
-	 * Spinlock count overflowing soon?
-	 */
-	BUG_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
-}
-EXPORT_SYMBOL(add_preempt_count);
-
-void fastcall sub_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(val > preempt_count());
-	/*
-	 * Is the spinlock portion underflowing?
-	 */
-	BUG_ON((val < PREEMPT_MASK) && !(preempt_count() & PREEMPT_MASK));
-	preempt_count() -= val;
-}
-EXPORT_SYMBOL(sub_preempt_count);
-
-#ifdef __smp_processor_id
-/*
- * Debugging check.
- */
-unsigned int smp_processor_id(void)
-{
-	unsigned long preempt_count = preempt_count();
-	int this_cpu = __smp_processor_id();
-	cpumask_t this_mask;
-
-	if (likely(preempt_count))
-		goto out;
-
-	if (irqs_disabled())
-		goto out;
-
-	/*
-	 * Kernel threads bound to a single CPU can safely use
-	 * smp_processor_id():
-	 */
-	this_mask = cpumask_of_cpu(this_cpu);
-
-	if (cpus_equal(current->cpus_allowed, this_mask))
-		goto out;
-
-	/*
-	 * It is valid to assume CPU-locality during early bootup:
-	 */
-	if (system_state != SYSTEM_RUNNING)
-		goto out;
-
-	/*
-	 * Avoid recursion:
-	 */
-	preempt_disable();
-
-	if (!printk_ratelimit())
-		goto out_enable;
-
-	printk(KERN_ERR "using smp_processor_id() in preemptible code: %s/%d\n",
-		current->comm, current->pid);
-	dump_stack();
-
-out_enable:
-	preempt_enable_no_resched();
-out:
-	return this_cpu;
-}
-
-EXPORT_SYMBOL(smp_processor_id);
-
-#endif /* __smp_processor_id */
-
-#endif /* PREEMPT && DEBUG_PREEMPT */
 
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 
@@ -2568,7 +2473,7 @@
  * The 'big kernel semaphore'
  *
  * This mutex is taken and released recursively by lock_kernel()
- * and unlock_kernel().  It is transparently dropped and reaquired
+ * and default_unlock_kernel().  It is transparently dropped and reaquired
  * over schedule().  It is used to protect legacy code that hasn't
  * been migrated to a proper locking design yet.
  *
@@ -2580,13 +2485,11 @@
  */
 static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
 
-int kernel_locked(void)
+int default_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
-
 /*
  * Release global kernel semaphore:
  */
@@ -2625,7 +2528,7 @@
 /*
  * Getting the big kernel semaphore.
  */
-void lock_kernel(void)
+void default_lock_kernel(void)
 {
 	struct task_struct *task = current;
 	int depth = task->lock_depth + 1;
@@ -2639,9 +2542,7 @@
 	task->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
-
-void unlock_kernel(void)
+void default_unlock_kernel(void)
 {
 	struct task_struct *task = current;
 
@@ -2651,19 +2552,15 @@
 		up(&kernel_sem);
 }
 
-EXPORT_SYMBOL(unlock_kernel);
-
 #else
 
 static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
 
-int kernel_locked(void)
+int default_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
-
 #define get_kernel_lock()	spin_lock(&kernel_flag)
 #define put_kernel_lock()	spin_unlock(&kernel_flag)
 
@@ -2692,7 +2589,7 @@
  * so we only need to worry about other
  * CPU's.
  */
-void lock_kernel(void)
+void default_lock_kernel(void)
 {
 	int depth = current->lock_depth+1;
 	if (likely(!depth))
@@ -2700,17 +2597,13 @@
 	current->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
-
-void unlock_kernel(void)
+void default_unlock_kernel(void)
 {
 	BUG_ON(current->lock_depth < 0);
 	if (likely(--current->lock_depth < 0))
 		put_kernel_lock();
 }
 
-EXPORT_SYMBOL(unlock_kernel);
-
 #endif
 
 #else
@@ -2722,9 +2615,9 @@
 
 
 /*
- * schedule() is the main scheduler function.
+ * default_schedule() is the main scheduler function.
  */
-asmlinkage void __sched schedule(void)
+asmlinkage void __sched default_schedule(void)
 {
 	long *switch_count;
 	task_t *prev, *next;
@@ -2844,28 +2737,28 @@
 	queue = array->queue + idx;
 	next = list_entry(queue->next, task_t, run_list);
 
-	if (!rt_task(next) && next->activated > 0) {
+	if (!default_rt_task(next) && next->default_data.activated > 0) {
 		unsigned long long delta = now - next->timestamp;
 
-		if (next->activated == 1)
+		if (next->default_data.activated == 1)
 			delta = delta * (ON_RUNQUEUE_WEIGHT * 128 / 100) / 128;
 
-		array = next->array;
+		array = next->default_data.array;
 		dequeue_task(next, array);
 		recalc_task_prio(next, next->timestamp + delta);
 		enqueue_task(next, array);
 	}
-	next->activated = 0;
+	next->default_data.activated = 0;
 switch_tasks:
 	prefetch(next);
 	clear_tsk_need_resched(prev);
 	rcu_qsctr_inc(task_cpu(prev));
 
-	prev->sleep_avg -= run_time;
-	if ((long)prev->sleep_avg <= 0) {
-		prev->sleep_avg = 0;
+	prev->default_data.sleep_avg -= run_time;
+	if ((long)prev->default_data.sleep_avg <= 0) {
+		prev->default_data.sleep_avg = 0;
 		if (!(HIGH_CREDIT(prev) || LOW_CREDIT(prev)))
-			prev->interactive_credit--;
+			prev->default_data.interactive_credit--;
 	}
 	prev->timestamp = prev->last_ran = now;
 
@@ -2890,15 +2783,13 @@
 		goto need_resched;
 }
 
-EXPORT_SYMBOL(schedule);
-
 #ifdef CONFIG_PREEMPT
 /*
  * this is is the entry point to schedule() from in-kernel preemption
  * off of preempt_enable.  Kernel preemptions off return from interrupt
  * occur there and call schedule directly.
  */
-asmlinkage void __sched preempt_schedule(void)
+asmlinkage void __sched default_preempt_schedule(void)
 {
 	struct thread_info *ti = current_thread_info();
 #ifdef CONFIG_PREEMPT_BKL
@@ -2926,7 +2817,7 @@
 	saved_lock_depth = task->lock_depth;
 	task->lock_depth = -1;
 #endif
-	schedule();
+	default_schedule();
 #ifdef CONFIG_PREEMPT_BKL
 	task->lock_depth = saved_lock_depth;
 #endif
@@ -2938,17 +2829,14 @@
 		goto need_resched;
 }
 
-EXPORT_SYMBOL(preempt_schedule);
 #endif /* CONFIG_PREEMPT */
 
-int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
+int default_default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
 {
 	task_t *p = curr->task;
 	return try_to_wake_up(p, mode, sync);
 }
 
-EXPORT_SYMBOL(default_wake_function);
-
 /*
  * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
  * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve
@@ -2981,7 +2869,7 @@
  * @mode: which threads
  * @nr_exclusive: how many wake-one or wake-many threads to wake up
  */
-void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
+void fastcall default___wake_up(wait_queue_head_t *q, unsigned int mode,
 				int nr_exclusive, void *key)
 {
 	unsigned long flags;
@@ -2991,12 +2879,10 @@
 	spin_unlock_irqrestore(&q->lock, flags);
 }
 
-EXPORT_SYMBOL(__wake_up);
-
 /*
  * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
  */
-void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
+void fastcall default___wake_up_locked(wait_queue_head_t *q, unsigned int mode)
 {
 	__wake_up_common(q, mode, 1, 0, NULL);
 }
@@ -3014,7 +2900,7 @@
  *
  * On UP it can prevent extra preemption.
  */
-void fastcall __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
+void fastcall default___wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
 {
 	unsigned long flags;
 	int sync = 1;
@@ -3029,9 +2915,8 @@
 	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
-EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
 
-void fastcall complete(struct completion *x)
+void fastcall default_complete(struct completion *x)
 {
 	unsigned long flags;
 
@@ -3041,9 +2926,8 @@
 			 1, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete);
 
-void fastcall complete_all(struct completion *x)
+void fastcall default_complete_all(struct completion *x)
 {
 	unsigned long flags;
 
@@ -3053,9 +2937,8 @@
 			 0, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete_all);
 
-void fastcall __sched wait_for_completion(struct completion *x)
+void fastcall __sched default_wait_for_completion(struct completion *x)
 {
 	might_sleep();
 	spin_lock_irq(&x->wait.lock);
@@ -3067,7 +2950,7 @@
 		do {
 			__set_current_state(TASK_UNINTERRUPTIBLE);
 			spin_unlock_irq(&x->wait.lock);
-			schedule();
+			default_schedule();
 			spin_lock_irq(&x->wait.lock);
 		} while (!x->done);
 		__remove_wait_queue(&x->wait, &wait);
@@ -3075,7 +2958,6 @@
 	x->done--;
 	spin_unlock_irq(&x->wait.lock);
 }
-EXPORT_SYMBOL(wait_for_completion);
 
 #define	SLEEP_ON_VAR					\
 	unsigned long flags;				\
@@ -3092,20 +2974,18 @@
 	__remove_wait_queue(q, &wait);			\
 	spin_unlock_irqrestore(&q->lock, flags);
 
-void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+void fastcall __sched default_interruptible_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_INTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	default_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on);
-
-long fastcall __sched interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched default_interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -3118,22 +2998,18 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on_timeout);
-
-void fastcall __sched sleep_on(wait_queue_head_t *q)
+void fastcall __sched default_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_UNINTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	default_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(sleep_on);
-
-long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched default_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -3146,9 +3022,7 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(sleep_on_timeout);
-
-void set_user_nice(task_t *p, long nice)
+void default_set_user_nice(task_t *p, long nice)
 {
 	unsigned long flags;
 	prio_array_t *array;
@@ -3168,19 +3042,19 @@
 	 * it wont have any effect on scheduling until the task is
 	 * not SCHED_NORMAL:
 	 */
-	if (rt_task(p)) {
-		p->static_prio = NICE_TO_PRIO(nice);
+	if (default_rt_task(p)) {
+		p->default_data.static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
-	array = p->array;
+	array = p->default_data.array;
 	if (array)
 		dequeue_task(p, array);
 
-	old_prio = p->prio;
+	old_prio = p->default_data.prio;
 	new_prio = NICE_TO_PRIO(nice);
 	delta = new_prio - old_prio;
-	p->static_prio = NICE_TO_PRIO(nice);
-	p->prio += delta;
+	p->default_data.static_prio = NICE_TO_PRIO(nice);
+	p->default_data.prio += delta;
 
 	if (array) {
 		enqueue_task(p, array);
@@ -3195,8 +3069,6 @@
 	task_rq_unlock(rq, &flags);
 }
 
-EXPORT_SYMBOL(set_user_nice);
-
 #ifdef CONFIG_KGDB
 struct task_struct *kgdb_get_idle(int this_cpu)
 {
@@ -3207,13 +3079,13 @@
 #ifdef __ARCH_WANT_SYS_NICE
 
 /*
- * sys_nice - change the priority of the current process.
+ * default_sys_nice - change the priority of the current process.
  * @increment: priority increment
  *
  * sys_setpriority is a more generic, but much slower function that
  * does similar things.
  */
-asmlinkage long sys_nice(int increment)
+asmlinkage long default_sys_nice(int increment)
 {
 	int retval;
 	long nice;
@@ -3232,7 +3104,7 @@
 	if (increment > 40)
 		increment = 40;
 
-	nice = PRIO_TO_NICE(current->static_prio) + increment;
+	nice = PRIO_TO_NICE(current->default_data.static_prio) + increment;
 	if (nice < -20)
 		nice = -20;
 	if (nice > 19)
@@ -3256,33 +3128,29 @@
  * RT tasks are offset by -200. Normal tasks are centered
  * around 0, value goes from -16 to +15.
  */
-int task_prio(const task_t *p)
+int default_task_prio(const task_t *p)
 {
-	return p->prio - MAX_RT_PRIO;
+	return p->default_data.prio - MAX_RT_PRIO;
 }
 
 /**
  * task_nice - return the nice value of a given task.
  * @p: the task in question.
  */
-int task_nice(const task_t *p)
+int default_task_nice(const task_t *p)
 {
 	return TASK_NICE(p);
 }
 
-EXPORT_SYMBOL(task_nice);
-
 /**
  * idle_cpu - is a given cpu idle currently?
  * @cpu: the processor in question.
  */
-int idle_cpu(int cpu)
+int default_idle_cpu(int cpu)
 {
 	return cpu_curr(cpu) == cpu_rq(cpu)->idle;
 }
 
-EXPORT_SYMBOL_GPL(idle_cpu);
-
 /**
  * find_process_by_pid - find a process with a matching PID value.
  * @pid: the pid in question.
@@ -3295,13 +3163,13 @@
 /* Actually do priority change: must hold rq lock. */
 static void __setscheduler(struct task_struct *p, int policy, int prio)
 {
-	BUG_ON(p->array);
+	BUG_ON(p->default_data.array);
 	p->policy = policy;
 	p->rt_priority = prio;
 	if (policy != SCHED_NORMAL)
-		p->prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
+		p->default_data.prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
 	else
-		p->prio = p->static_prio;
+		p->default_data.prio = p->default_data.static_prio;
 }
 
 /*
@@ -3373,11 +3241,11 @@
 	if (retval)
 		goto out_unlock;
 
-	array = p->array;
+	array = p->default_data.array;
 	if (array)
 		deactivate_task(p, task_rq(p));
 	retval = 0;
-	oldprio = p->prio;
+	oldprio = p->default_data.prio;
 	__setscheduler(p, policy, lp.sched_priority);
 	if (array) {
 		__activate_task(p, task_rq(p));
@@ -3387,7 +3255,7 @@
 		 * this runqueue and our priority is higher than the current's
 		 */
 		if (task_running(rq, p)) {
-			if (p->prio > oldprio)
+			if (p->default_data.prio > oldprio)
 				resched_task(rq->curr);
 		} else if (TASK_PREEMPTS_CURR(p, rq))
 			resched_task(rq->curr);
@@ -3408,7 +3276,7 @@
  * @policy: new policy
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
+asmlinkage long default_sys_sched_setscheduler(pid_t pid, int policy,
 				       struct sched_param __user *param)
 {
 	return setscheduler(pid, policy, param);
@@ -3419,7 +3287,7 @@
  * @pid: the pid in question.
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long default_sys_sched_setparam(pid_t pid, struct sched_param __user *param)
 {
 	return setscheduler(pid, -1, param);
 }
@@ -3428,7 +3296,7 @@
  * sys_sched_getscheduler - get the policy (scheduling class) of a thread
  * @pid: the pid in question.
  */
-asmlinkage long sys_sched_getscheduler(pid_t pid)
+asmlinkage long default_sys_sched_getscheduler(pid_t pid)
 {
 	int retval = -EINVAL;
 	task_t *p;
@@ -3455,7 +3323,7 @@
  * @pid: the pid in question.
  * @param: structure containing the RT priority.
  */
-asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long default_sys_sched_getparam(pid_t pid, struct sched_param __user *param)
 {
 	struct sched_param lp;
 	int retval = -EINVAL;
@@ -3490,7 +3358,7 @@
 	return retval;
 }
 
-long sched_setaffinity(pid_t pid, cpumask_t new_mask)
+long default_sched_setaffinity(pid_t pid, cpumask_t new_mask)
 {
 	task_t *p;
 	int retval;
@@ -3529,52 +3397,7 @@
 	return retval;
 }
 
-static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
-			     cpumask_t *new_mask)
-{
-	if (len < sizeof(cpumask_t)) {
-		memset(new_mask, 0, sizeof(cpumask_t));
-	} else if (len > sizeof(cpumask_t)) {
-		len = sizeof(cpumask_t);
-	}
-	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
-}
-
-/**
- * sys_sched_setaffinity - set the cpu affinity of a process
- * @pid: pid of the process
- * @len: length in bytes of the bitmask pointed to by user_mask_ptr
- * @user_mask_ptr: user-space pointer to the new cpu mask
- */
-asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
-				      unsigned long __user *user_mask_ptr)
-{
-	cpumask_t new_mask;
-	int retval;
-
-	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
-	if (retval)
-		return retval;
-
-	return sched_setaffinity(pid, new_mask);
-}
-
-/*
- * Represents all cpu's present in the system
- * In systems capable of hotplug, this map could dynamically grow
- * as new cpu's are detected in the system via any platform specific
- * method, such as ACPI for e.g.
- */
-
-cpumask_t cpu_present_map;
-EXPORT_SYMBOL(cpu_present_map);
-
-#ifndef CONFIG_SMP
-cpumask_t cpu_online_map = CPU_MASK_ALL;
-cpumask_t cpu_possible_map = CPU_MASK_ALL;
-#endif
-
-long sched_getaffinity(pid_t pid, cpumask_t *mask)
+long default_sched_getaffinity(pid_t pid, cpumask_t *mask)
 {
 	int retval;
 	task_t *p;
@@ -3605,7 +3428,7 @@
  * @len: length in bytes of the bitmask pointed to by user_mask_ptr
  * @user_mask_ptr: user-space pointer to hold the current cpu mask
  */
-asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
+asmlinkage long default_sys_sched_getaffinity(pid_t pid, unsigned int len,
 				      unsigned long __user *user_mask_ptr)
 {
 	int ret;
@@ -3625,16 +3448,16 @@
 }
 
 /**
- * sys_sched_yield - yield the current processor to other threads.
+ * default_sys_sched_yield - yield the current processor to other threads.
  *
  * this function yields the current CPU by moving the calling thread
  * to the expired array. If there are no other threads running on this
  * CPU then this function will return.
  */
-asmlinkage long sys_sched_yield(void)
+asmlinkage long default_sys_sched_yield(void)
 {
 	runqueue_t *rq = this_rq_lock();
-	prio_array_t *array = current->array;
+	prio_array_t *array = current->default_data.array;
 	prio_array_t *target = rq->expired;
 
 	schedstat_inc(rq, yld_cnt);
@@ -3645,10 +3468,10 @@
 	 * (special rule: RT tasks will just roundrobin in the active
 	 *  array.)
 	 */
-	if (rt_task(current))
+	if (default_rt_task(current))
 		target = rq->active;
 
-	if (current->array->nr_active == 1) {
+	if (current->default_data.array->nr_active == 1) {
 		schedstat_inc(rq, yld_act_empty);
 		if (!rq->expired->nr_active)
 			schedstat_inc(rq, yld_both_empty);
@@ -3665,80 +3488,11 @@
 	_raw_spin_unlock(&rq->lock);
 	preempt_enable_no_resched();
 
-	schedule();
+	default_schedule();
 
 	return 0;
 }
 
-static inline void __cond_resched(void)
-{
-
-
-	if (preempt_count() & PREEMPT_ACTIVE)
-		return;
-	do {
-		add_preempt_count(PREEMPT_ACTIVE);
-		schedule();
-		sub_preempt_count(PREEMPT_ACTIVE);
-	} while (need_resched());
-}
-
-int __sched cond_resched(void)
-{
-	if (need_resched()) {
-		__cond_resched();
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched);
-
-/*
- * cond_resched_lock() - if a reschedule is pending, drop the given lock,
- * call schedule, and on return reacquire the lock.
- *
- * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
- * operations here to prevent schedule() from being called twice (once via
- * spin_unlock(), once by hand).
- */
-int cond_resched_lock(spinlock_t * lock)
-{
-#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT)
-	if (lock->break_lock) {
-		lock->break_lock = 0;
-		spin_unlock(lock);
-		cpu_relax();
-		spin_lock(lock);
-	}
-#endif
-	if (need_resched()) {
-		_raw_spin_unlock(lock);
-		preempt_enable_no_resched();
-		__cond_resched();
-		spin_lock(lock);
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched_lock);
-
-int __sched cond_resched_softirq(void)
-{
-	BUG_ON(!in_softirq());
-
-	if (need_resched()) {
-		__local_bh_enable();
-		__cond_resched();
-		local_bh_disable();
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched_softirq);
-
 
 /**
  * yield - yield the current processor to other threads.
@@ -3746,7 +3500,7 @@
  * this is a shortcut for kernel-space yielding - it marks the
  * thread runnable and calls sys_sched_yield().
  */
-void __sched yield(void)
+void __sched default_yield(void)
 {
 	set_current_state(TASK_RUNNING);
 	sys_sched_yield();
@@ -3761,18 +3515,16 @@
  * But don't do that if it is a deliberate, throttling IO wait (this task
  * has set its backing_dev_info: the queue against which it should throttle)
  */
-void __sched io_schedule(void)
+void __sched default_io_schedule(void)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 
 	atomic_inc(&rq->nr_iowait);
-	schedule();
+	default_schedule();
 	atomic_dec(&rq->nr_iowait);
 }
 
-EXPORT_SYMBOL(io_schedule);
-
-long __sched io_schedule_timeout(long timeout)
+long __sched default_io_schedule_timeout(long timeout)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 	long ret;
@@ -3784,52 +3536,7 @@
 }
 
 /**
- * sys_sched_get_priority_max - return maximum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the maximum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_max(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = MAX_USER_RT_PRIO-1;
-		break;
-	case SCHED_NORMAL:
-		ret = 0;
-		break;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_get_priority_min - return minimum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the minimum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_min(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = 1;
-		break;
-	case SCHED_NORMAL:
-		ret = 0;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_rr_get_interval - return the default timeslice of a process.
+ * default_sys_sched_rr_get_interval - return the default timeslice of a process.
  * @pid: pid of the process.
  * @interval: userspace pointer to the timeslice value.
  *
@@ -3837,7 +3544,7 @@
  * into the user-space timespec buffer. A value of '0' means infinity.
  */
 asmlinkage
-long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
+long default_sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
 {
 	int retval = -EINVAL;
 	struct timespec t;
@@ -3867,113 +3574,15 @@
 	return retval;
 }
 
-static inline struct task_struct *eldest_child(struct task_struct *p)
-{
-	if (list_empty(&p->children)) return NULL;
-	return list_entry(p->children.next,struct task_struct,sibling);
-}
-
-static inline struct task_struct *older_sibling(struct task_struct *p)
-{
-	if (p->sibling.prev==&p->parent->children) return NULL;
-	return list_entry(p->sibling.prev,struct task_struct,sibling);
-}
-
-static inline struct task_struct *younger_sibling(struct task_struct *p)
-{
-	if (p->sibling.next==&p->parent->children) return NULL;
-	return list_entry(p->sibling.next,struct task_struct,sibling);
-}
-
-static void show_task(task_t * p)
-{
-	task_t *relative;
-	unsigned state;
-	unsigned long free = 0;
-	static const char *stat_nam[] = { "R", "S", "D", "T", "t", "Z", "X" };
-
-	printk("%-13.13s ", p->comm);
-	state = p->state ? __ffs(p->state) + 1 : 0;
-	if (state < ARRAY_SIZE(stat_nam))
-		printk(stat_nam[state]);
-	else
-		printk("?");
-#if (BITS_PER_LONG == 32)
-	if (state == TASK_RUNNING)
-		printk(" running ");
-	else
-		printk(" %08lX ", thread_saved_pc(p));
-#else
-	if (state == TASK_RUNNING)
-		printk("  running task   ");
-	else
-		printk(" %016lx ", thread_saved_pc(p));
-#endif
-#ifdef CONFIG_DEBUG_STACK_USAGE
-	{
-		unsigned long * n = (unsigned long *) (p->thread_info+1);
-		while (!*n)
-			n++;
-		free = (unsigned long) n - (unsigned long)(p->thread_info+1);
-	}
-#endif
-	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
-	if ((relative = eldest_child(p)))
-		printk("%5d ", relative->pid);
-	else
-		printk("      ");
-	if ((relative = younger_sibling(p)))
-		printk("%7d", relative->pid);
-	else
-		printk("       ");
-	if ((relative = older_sibling(p)))
-		printk(" %5d", relative->pid);
-	else
-		printk("      ");
-	if (!p->mm)
-		printk(" (L-TLB)\n");
-	else
-		printk(" (NOTLB)\n");
-
-	if (state != TASK_RUNNING)
-		show_stack(p, NULL);
-}
-
-void show_state(void)
-{
-	task_t *g, *p;
-
-#if (BITS_PER_LONG == 32)
-	printk("\n"
-	       "                                               sibling\n");
-	printk("  task             PC      pid father child younger older\n");
-#else
-	printk("\n"
-	       "                                                       sibling\n");
-	printk("  task                 PC          pid father child younger older\n");
-#endif
-	read_lock(&tasklist_lock);
-	do_each_thread(g, p) {
-		/*
-		 * reset the NMI-timeout, listing all files on a slow
-		 * console might take alot of time:
-		 */
-		touch_nmi_watchdog();
-		show_task(p);
-	} while_each_thread(g, p);
-
-	read_unlock(&tasklist_lock);
-}
-
-void __devinit init_idle(task_t *idle, int cpu)
+void __devinit default_init_idle(task_t *idle, int cpu)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long flags;
 
-	idle->sleep_avg = 0;
-	idle->interactive_credit = 0;
-	idle->array = NULL;
-	idle->prio = MAX_PRIO;
+	idle->default_data.sleep_avg = 0;
+	idle->default_data.interactive_credit = 0;
+	idle->default_data.array = NULL;
+	idle->default_data.prio = MAX_PRIO;
 	idle->state = TASK_RUNNING;
 	set_task_cpu(idle, cpu);
 
@@ -3990,15 +3599,6 @@
 #endif
 }
 
-/*
- * In a system that switches off the HZ timer nohz_cpu_mask
- * indicates which cpus entered this state. This is used
- * in the rcu update to wait only for active cpus. For system
- * which do not switch off the HZ timer nohz_cpu_mask should
- * always be CPU_MASK_NONE.
- */
-cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
-
 #ifdef CONFIG_SMP
 /*
  * This is how migration works:
@@ -4025,7 +3625,7 @@
  * task must not exit() & deallocate itself prematurely.  The
  * call is not atomic; no spinlocks may be held.
  */
-int set_cpus_allowed(task_t *p, cpumask_t new_mask)
+int default_set_cpus_allowed(task_t *p, cpumask_t new_mask)
 {
 	unsigned long flags;
 	int ret = 0;
@@ -4058,11 +3658,9 @@
 	return ret;
 }
 
-EXPORT_SYMBOL_GPL(set_cpus_allowed);
-
 /*
  * Move (not current) task off this cpu, onto dest cpu.  We're doing
- * this because either it can't run here any more (set_cpus_allowed()
+ * this because either it can't run here any more (default_set_cpus_allowed()
  * away from this CPU, or CPU going down), or because we're
  * attempting to rebalance this task on exec (sched_exec).
  *
@@ -4088,7 +3686,7 @@
 		goto out;
 
 	set_task_cpu(p, dest_cpu);
-	if (p->array) {
+	if (p->default_data.array) {
 		/*
 		 * Sync timestamp with rq_dest's before activating.
 		 * The same thing could be achieved by doing this step
@@ -4144,7 +3742,7 @@
 
 		if (list_empty(head)) {
 			spin_unlock_irq(&rq->lock);
-			schedule();
+			default_schedule();
 			set_current_state(TASK_INTERRUPTIBLE);
 			continue;
 		}
@@ -4164,7 +3762,7 @@
 			WARN_ON(1);
 		}
 
-		complete(&req->done);
+		default_complete(&req->done);
 	}
 	__set_current_state(TASK_RUNNING);
 	return 0;
@@ -4173,7 +3771,7 @@
 	/* Wait for kthread_stop */
 	set_current_state(TASK_INTERRUPTIBLE);
 	while (!kthread_should_stop()) {
-		schedule();
+		default_schedule();
 		set_current_state(TASK_INTERRUPTIBLE);
 	}
 	__set_current_state(TASK_RUNNING);
@@ -4344,7 +3942,7 @@
 		/* Idle task back to normal (off runqueue, low prio) */
 		rq = task_rq_lock(rq->idle, &flags);
 		deactivate_task(rq->idle, rq);
-		rq->idle->static_prio = MAX_PRIO;
+		rq->idle->default_data.static_prio = MAX_PRIO;
 		__setscheduler(rq->idle, SCHED_NORMAL, 0);
 		migrate_dead_tasks(cpu);
 		task_rq_unlock(rq, &flags);
@@ -4360,7 +3958,7 @@
 					 migration_req_t, list);
 			BUG_ON(req->type != REQ_MOVE_TASK);
 			list_del_init(&req->list);
-			complete(&req->done);
+			default_complete(&req->done);
 		}
 		spin_unlock_irq(&rq->lock);
 		break;
@@ -4377,7 +3975,7 @@
 	.priority = 10
 };
 
-int __init migration_init(void)
+int __init default_migration_init(void)
 {
 	void *cpu = (void *)(long)smp_processor_id();
 	/* Start one for boot CPU. */
@@ -4393,7 +3991,7 @@
  * Attach the domain 'sd' to 'cpu' as its base domain.  Callers must
  * hold the hotplug lock.
  */
-void __devinit cpu_attach_domain(struct sched_domain *sd, int cpu)
+void __devinit default_cpu_attach_domain(struct sched_domain *sd, int cpu)
 {
 	migration_req_t req;
 	unsigned long flags;
@@ -4421,7 +4019,7 @@
 }
 
 /* cpus with isolated domains */
-cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
+cpumask_t __devinitdata default_cpu_isolated_map = CPU_MASK_NONE;
 
 /* Setup the mask of cpus configured for isolated domains */
 static int __init isolated_cpu_setup(char *str)
@@ -4429,9 +4027,9 @@
 	int ints[NR_CPUS], i;
 
 	str = get_options(str, ARRAY_SIZE(ints), ints);
-	cpus_clear(cpu_isolated_map);
+	cpus_clear(default_cpu_isolated_map);
 	for (i = 1; i <= ints[0]; i++)
-		cpu_set(ints[i], cpu_isolated_map);
+		cpu_set(ints[i], default_cpu_isolated_map);
 	return 1;
 }
 
@@ -4448,7 +4046,7 @@
  * covered by the given span, and will set each group's ->cpumask correctly,
  * and ->cpu_power to 0.
  */
-void __devinit init_sched_build_groups(struct sched_group groups[],
+void __devinit default_init_sched_build_groups(struct sched_group groups[],
 			cpumask_t span, int (*group_fn)(int cpu))
 {
 	struct sched_group *first = NULL, *last = NULL;
@@ -4530,7 +4128,7 @@
 	 * For now this just excludes isolated cpus, but could be used to
 	 * exclude other special cases in the future.
 	 */
-	cpus_complement(cpu_default_map, cpu_isolated_map);
+	cpus_complement(cpu_default_map, default_cpu_isolated_map);
 	cpus_and(cpu_default_map, cpu_default_map, cpu_online_map);
 
 	/*
@@ -4579,7 +4177,7 @@
 		if (i != first_cpu(this_sibling_map))
 			continue;
 
-		init_sched_build_groups(sched_group_cpus, this_sibling_map,
+		default_init_sched_build_groups(sched_group_cpus, this_sibling_map,
 						&cpu_to_cpu_group);
 	}
 #endif
@@ -4592,13 +4190,13 @@
 		if (cpus_empty(nodemask))
 			continue;
 
-		init_sched_build_groups(sched_group_phys, nodemask,
+		default_init_sched_build_groups(sched_group_phys, nodemask,
 						&cpu_to_phys_group);
 	}
 
 #ifdef CONFIG_NUMA
 	/* Set up node groups */
-	init_sched_build_groups(sched_group_nodes, cpu_default_map,
+	default_init_sched_build_groups(sched_group_nodes, cpu_default_map,
 					&cpu_to_node_group);
 #endif
 
@@ -4634,7 +4232,7 @@
 #else
 		sd = &per_cpu(phys_domains, i);
 #endif
-		cpu_attach_domain(sd, i);
+		default_cpu_attach_domain(sd, i);
 	}
 }
 
@@ -4761,7 +4359,7 @@
 	case CPU_UP_PREPARE:
 	case CPU_DOWN_PREPARE:
 		for_each_online_cpu(i)
-			cpu_attach_domain(&sched_domain_dummy, i);
+			default_cpu_attach_domain(&sched_domain_dummy, i);
 		arch_destroy_sched_domains();
 		return NOTIFY_OK;
 
@@ -4786,7 +4384,7 @@
 }
 #endif
 
-void __init sched_init_smp(void)
+void __init default_sched_init_smp(void)
 {
 	lock_cpu_hotplug();
 	arch_init_sched_domains();
@@ -4796,21 +4394,12 @@
 	hotcpu_notifier(update_sched_domains, 0);
 }
 #else
-void __init sched_init_smp(void)
+void __init default_sched_init_smp(void)
 {
 }
 #endif /* CONFIG_SMP */
 
-int in_sched_functions(unsigned long addr)
-{
-	/* Linker adds these: start and end of __sched functions */
-	extern char __sched_text_start[], __sched_text_end[];
-	return in_lock_functions(addr) ||
-		(addr >= (unsigned long)__sched_text_start
-		&& addr < (unsigned long)__sched_text_end);
-}
-
-void __init sched_init(void)
+void __init default_sched_init(void)
 {
 	runqueue_t *rq;
 	int i, j, k;
@@ -4845,6 +4434,8 @@
 		}
 	}
 
+	printk("Using %s scheduler\n", DEFAULT_SCHEDULER_NAME);
+
 	/*
 	 * The boot idle thread does lazy MMU switching as well:
 	 */
@@ -4857,7 +4448,7 @@
 	 * but because we are the idle thread, we just pick up running again
 	 * when this runqueue becomes "idle".
 	 */
-	init_idle(current, smp_processor_id());
+	default_init_idle(current, smp_processor_id());
 }
 
 #ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
@@ -4881,3 +4472,72 @@
 }
 EXPORT_SYMBOL(__might_sleep);
 #endif
+
+scheduler_t sched_default = {
+	.schedule_tail_fn = 			default_schedule_tail,
+	.schedule_fn =				default_schedule,
+	.scheduler_tick_fn = 			default_scheduler_tick,
+	.yield_fn = 				default_yield,
+	.wait_for_completion_fn	=		default_wait_for_completion,
+	.idle_cpu_fn = 				default_idle_cpu,
+	.default_wake_function_fn = 		default_default_wake_function,
+	.__wake_up_fn = 			default___wake_up,
+	.__wake_up_locked_fn = 			default___wake_up_locked,
+	.__wake_up_sync_fn = 			default___wake_up_sync,
+	.complete_fn =				default_complete,
+	.complete_all_fn =			default_complete_all,
+	.interruptible_sleep_on_fn = 		default_interruptible_sleep_on,
+	.interruptible_sleep_on_timeout_fn = 	default_interruptible_sleep_on_timeout,
+	.sleep_on_fn =				default_sleep_on,
+	.sleep_on_timeout_fn = 			default_sleep_on_timeout,
+	.set_user_nice_fn = 			default_set_user_nice,
+	.task_nice_fn = 			default_task_nice,
+	.io_schedule_fn = 			default_io_schedule,
+	.io_schedule_timeout_fn = 		default_io_schedule_timeout,
+	.task_curr_fn = 			default_task_curr,
+	.wake_up_process_fn = 			default_wake_up_process,
+	.wake_up_state_fn = 			default_wake_up_state,
+	.nr_running_fn = 			default_nr_running,
+	.nr_uninterruptible_fn = 		default_nr_uninterruptible,
+	.nr_iowait_fn = 			default_nr_iowait,
+	.nr_context_switches_fn = 		default_nr_context_switches,
+	.sched_exec_fn = 			default_sched_exec,
+	.sched_setaffinity_fn = 		default_sched_setaffinity,
+	.sched_getaffinity_fn =			default_sched_getaffinity,
+	.sys_nice_fn = 				default_sys_nice,
+	.sys_sched_setscheduler_fn = 		default_sys_sched_setscheduler,
+	.sys_sched_setparam_fn = 		default_sys_sched_setparam,
+	.sys_sched_getscheduler_fn = 		default_sys_sched_getscheduler,
+	.sys_sched_getparam_fn = 		default_sys_sched_getparam,
+	.sys_sched_getaffinity_fn = 		default_sys_sched_getaffinity,
+	.sys_sched_yield_fn = 			default_sys_sched_yield,
+	.sys_sched_rr_get_interval_fn = 	default_sys_sched_rr_get_interval,
+	.sched_init_fn = 			default_sched_init,
+	.sched_init_smp_fn = 			default_sched_init_smp,
+	.migration_init_fn = 			default_migration_init,
+	.sched_fork_fn = 			default_sched_fork,
+	.sched_exit_fn = 			default_sched_exit,
+	.init_idle_fn = 			default_init_idle,
+	.wake_up_new_task_fn = 			default_wake_up_new_task,
+	.task_prio_fn = 			default_task_prio,
+#if defined(CONFIG_SMP)
+	.init_sched_build_groups_fn = 		default_init_sched_build_groups,
+	.cpu_attach_domain_fn = 		default_cpu_attach_domain,
+	.set_cpus_allowed_fn = 			default_set_cpus_allowed,
+	.wait_task_inactive_fn = 		default_wait_task_inactive,
+	.kick_process_fn = 			default_kick_process,
+#endif
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+	.kernel_locked_fn = 			default_kernel_locked,
+	.lock_kernel_fn = 			default_lock_kernel,
+	.unlock_kernel_fn = 			default_unlock_kernel,
+#endif
+#if defined(CONFIG_PREEMPT)
+	.preempt_schedule_fn = 			default_preempt_schedule,
+#endif
+	.name = 				DEFAULT_SCHEDULER_NAME,
+	.version = 				DEFAULT_SCHEDULER_VERSION,
+	.type = 				SCHED_DEFAULT,
+};
+
+EXPORT_SYMBOL(sched_default);
Index: xx-sources/kernel/nicksched-sched.c
===================================================================
--- xx-sources.orig/kernel/nicksched-sched.c	2004-10-07 09:59:49.955214832 -0400
+++ xx-sources/kernel/nicksched-sched.c	2004-10-07 10:10:49.159000624 -0400
@@ -46,13 +46,13 @@
 #include <linux/seq_file.h>
 #include <linux/syscalls.h>
 #include <linux/times.h>
+#include <linux/sysctl.h>
 
 #include <asm/tlb.h>
 
 #include <asm/unistd.h>
 
-const char *scheduler_name = "Nicksched";
-const char *scheduler_version = "v31";
+#define NICKSCHED_SCHEDULER_VERSION "v31"
 
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
@@ -61,7 +61,7 @@
  */
 #define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 30)
 #define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 30)
-#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->nicksched_data.static_prio)
 
 /*
  * 'User priority' is the nice value converted to something we
@@ -69,7 +69,7 @@
  * it's a [ 0 ... 58 ] range.
  */
 #define USER_PRIO(p)		((p)-MAX_RT_PRIO)
-#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
+#define MAX_USER_PRIO		(USER_PRIO(NICK_MAX_PRIO))
 
 #define US_TO_JIFFIES(x)	((x) * HZ / 1000000)
 #define JIFFIES_TO_US(x)	((x) * 1000000 / HZ)
@@ -81,12 +81,12 @@
  * that the maximum priority process will get. Larger timeslices are attainable
  * by low priority processes however.
  */
-int sched_base_timeslice = 64;
-int sched_min_base = 1;
-int sched_max_base = 10000;
+int nicksched_sched_base_timeslice = 64;
+int nicksched_sched_min_base = 1;
+int nicksched_sched_max_base = 10000;
 
 #define RT_TIMESLICE		(50 * 1000 / HZ)     /* 50ms */
-#define BASE_TIMESLICE		(sched_base_timeslice)
+#define BASE_TIMESLICE		(nicksched_sched_base_timeslice)
 #define MIN_TIMESLICE		1
 
 /* Maximum amount of history that will be used to calculate priority */
@@ -120,7 +120,7 @@
 #define STIME_SLEEP		1	/* Sleeping */
 #define STIME_RUN		2	/* Using CPU */
 
-#define TASK_PREEMPTS_CURR(p, rq)	( (p)->prio < (rq)->curr->prio )
+#define TASK_PREEMPTS_CURR(p, rq)	( (p)->nicksched_data.prio < (rq)->curr->nicksched_data.prio )
 
 #define task_hot(p, now, sd) ((long long) ((now) - (p)->last_ran)	\
 				< (long long) (sd)->cache_hot_time)
@@ -129,15 +129,13 @@
  * These are the runqueue data structures:
  */
 
-#define BITMAP_SIZE ((((MAX_PRIO+1+7)/8)+sizeof(long)-1)/sizeof(long))
-
-typedef struct runqueue runqueue_t;
+#define BITMAP_SIZE ((((NICK_MAX_PRIO+1+7)/8)+sizeof(long)-1)/sizeof(long))
 
 struct prio_array {
 	int min_prio;
 	unsigned int nr_active;
 	unsigned long bitmap[BITMAP_SIZE];
-	struct list_head queue[MAX_PRIO];
+	struct list_head queue[NICK_MAX_PRIO];
 };
 
 /*
@@ -378,7 +376,7 @@
  * queue is empty, without explicitly dequeuing and requeuing tasks in the
  * expired queue.  (Interactive tasks may be requeued directly to the
  * active queue, thus delaying tasks in the expired queue from running;
- * see scheduler_tick()).
+ * see nicksched_scheduler_tick()).
  *
  * This function is only called from sched_info_arrive(), rather than
  * dequeue_task(). Even though a task may be queued and dequeued multiple
@@ -423,7 +421,7 @@
  * and requeuing any tasks, we are interested in queuing to either. It
  * is unusual but not impossible for tasks to be dequeued and immediately
  * requeued in the same or another array: this can happen in sched_yield(),
- * set_user_nice(), and even load_balance() as it moves tasks from runqueue
+ * nicksched_set_user_nice(), and even load_balance() as it moves tasks from runqueue
  * to runqueue.
  *
  * This function is only called from enqueue_task(), but also only updates
@@ -483,16 +481,16 @@
 {
 	array->nr_active--;
 	list_del(&p->run_list);
-	if (list_empty(array->queue + p->prio))
-		__clear_bit(p->prio, array->bitmap);
+	if (list_empty(array->queue + p->nicksched_data.prio))
+		__clear_bit(p->nicksched_data.prio, array->bitmap);
 }
 
 static void enqueue_task(struct task_struct *p, prio_array_t *array)
 {
-	struct list_head *entry = array->queue + p->prio;
+	struct list_head *entry = array->queue + p->nicksched_data.prio;
 	sched_info_queued(p);
 
-	if (!rt_task(p)) {
+	if (!nicksched_rt_task(p)) {
 		/*
 		 * Cycle tasks on the same priority level. This reduces their
 		 * timeslice fluctuations due to higher priority tasks expiring.
@@ -501,9 +499,9 @@
 			entry = entry->next;
 	}
 	list_add_tail(&p->run_list, entry);
-	__set_bit(p->prio, array->bitmap);
+	__set_bit(p->nicksched_data.prio, array->bitmap);
 	array->nr_active++;
-	p->array = array;
+	p->nicksched_data.array = array;
 }
 
 /*
@@ -513,10 +511,10 @@
  */
 static inline void enqueue_task_head(struct task_struct *p, prio_array_t *array)
 {
-	list_add(&p->run_list, array->queue + p->prio);
-	__set_bit(p->prio, array->bitmap);
+	list_add(&p->run_list, array->queue + p->nicksched_data.prio);
+	__set_bit(p->nicksched_data.prio, array->bitmap);
 	array->nr_active++;
-	p->array = array;
+	p->nicksched_data.array = array;
 }
 
 static inline unsigned long long clock_us(void)
@@ -539,7 +537,7 @@
 			time = MAX_SLEEP_AFFECT*4;
 		t = ((unsigned long)time + 3) / 4;
 	} else {
-		unsigned long div = 60 - USER_PRIO(p->static_prio);
+		unsigned long div = 60 - USER_PRIO(p->nicksched_data.static_prio);
 		t = (unsigned long)time * 30;
 		t = t / div;
 		t = t * 30;
@@ -547,22 +545,22 @@
 	}
 
 	ratio = MAX_SLEEP - t;
-	tmp = (unsigned long long)ratio*p->total_time + MAX_SLEEP/2;
+	tmp = (unsigned long long)ratio*p->nicksched_data.total_time + MAX_SLEEP/2;
 	tmp >>= MAX_SLEEP_SHIFT;
-	p->total_time = (unsigned long)tmp;
+	p->nicksched_data.total_time = (unsigned long)tmp;
 
-	tmp = (unsigned long long)ratio*p->sleep_time + MAX_SLEEP/2;
+	tmp = (unsigned long long)ratio*p->nicksched_data.sleep_time + MAX_SLEEP/2;
 	tmp >>= MAX_SLEEP_SHIFT;
-	p->sleep_time = (unsigned long)tmp;
+	p->nicksched_data.sleep_time = (unsigned long)tmp;
 
-	p->total_time += t;
+	p->nicksched_data.total_time += t;
 	if (type == STIME_SLEEP)
-		p->sleep_time += t;
+		p->nicksched_data.sleep_time += t;
 }
 
 static unsigned long task_sleep_avg(task_t *p)
 {
-	return (SLEEP_FACTOR * p->sleep_time) / (p->total_time + 1);
+	return (SLEEP_FACTOR * p->nicksched_data.sleep_time) / (p->nicksched_data.total_time + 1);
 }
 
 /*
@@ -578,11 +576,11 @@
 	int idx, base, delta;
 	int timeslice;
 
-	if (rt_task(p))
+	if (nicksched_rt_task(p))
 		return RT_TIMESLICE;
 
-	idx = min(p->prio, rq->expired->min_prio);
-	delta = p->prio - idx;
+	idx = min(p->nicksched_data.prio, rq->expired->min_prio);
+	delta = p->nicksched_data.prio - idx;
 	base = BASE_TIMESLICE * (MAX_USER_PRIO + 1) / (delta + 2);
 
 	base = base * 40 / (70 - USER_PRIO(idx));
@@ -606,20 +604,20 @@
 	unsigned long sleep_avg;
 	int bonus, prio;
 
-	if (rt_task(p))
-		return p->prio;
+	if (nicksched_rt_task(p))
+		return p->nicksched_data.prio;
 
 	sleep_avg = task_sleep_avg(p);
 
-	prio = USER_PRIO(p->static_prio) + 10;
+	prio = USER_PRIO(p->nicksched_data.static_prio) + 10;
 	bonus = (((MAX_USER_PRIO + 1) / 3) * sleep_avg + (SLEEP_FACTOR / 2))
 					/ SLEEP_FACTOR;
 	prio = MAX_RT_PRIO + prio - bonus;
 
 	if (prio < MAX_RT_PRIO)
 		return MAX_RT_PRIO;
-	if (prio > MAX_PRIO-1)
-		return MAX_PRIO-1;
+	if (prio > NICK_MAX_PRIO-1)
+		return NICK_MAX_PRIO-1;
 
 	return prio;
 }
@@ -631,9 +629,9 @@
 {
 	enqueue_task(p, array);
 	rq->nr_running++;
-	if (!rt_task(p)) {
-		if (p->prio < array->min_prio)
-			array->min_prio = p->prio;
+	if (!nicksched_rt_task(p)) {
+		if (p->nicksched_data.prio < array->min_prio)
+			array->min_prio = p->nicksched_data.prio;
 	}
 }
 
@@ -674,15 +672,15 @@
 	sleep = now - p->timestamp;
 	p->timestamp = now;
 	add_task_time(p, sleep, STIME_SLEEP);
-	p->prio = task_priority(p);
+	p->nicksched_data.prio = task_priority(p);
 
 	array = rq->active;
-	if (unlikely(p->used_slice == -1)) {
+	if (unlikely(p->nicksched_data.used_slice == -1)) {
 		/* This only applys to newly woken children */
 		array = rq->expired;
-		p->used_slice = 0;
-	} else if (rq->array_sequence != p->array_sequence)
-		p->used_slice = 0;
+		p->nicksched_data.used_slice = 0;
+	} else if (rq->array_sequence != p->nicksched_data.array_sequence)
+		p->nicksched_data.used_slice = 0;
 
 	__activate_task(p, rq, array);
 }
@@ -692,10 +690,10 @@
  */
 static inline void deactivate_task(struct task_struct *p, runqueue_t *rq)
 {
-	p->array_sequence = rq->array_sequence;
+	p->nicksched_data.array_sequence = rq->array_sequence;
 	rq->nr_running--;
-	dequeue_task(p, p->array);
-	p->array = NULL;
+	dequeue_task(p, p->nicksched_data.array);
+	p->nicksched_data.array = NULL;
 }
 
 /*
@@ -731,7 +729,7 @@
  * task_curr - is this task currently executing on a CPU?
  * @p: the task in question.
  */
-inline int task_curr(const task_t *p)
+inline int nicksched_task_curr(const task_t *p)
 {
 	return cpu_curr(task_cpu(p)) == p;
 }
@@ -768,7 +766,7 @@
 	 * If the task is not on a runqueue (and not running), then
 	 * it is sufficient to simply update the task's cpu field.
 	 */
-	if (!p->array && !task_running(rq, p)) {
+	if (!p->nicksched_data.array && !task_running(rq, p)) {
 		set_task_cpu(p, dest_cpu);
 		return 0;
 	}
@@ -782,7 +780,7 @@
 }
 
 /*
- * wait_task_inactive - wait for a thread to unschedule.
+ * nicksched_wait_task_inactive - wait for a thread to unschedule.
  *
  * The caller must ensure that the task *will* unschedule sometime soon,
  * else this function might spin for a *long* time. This function can't
@@ -790,7 +788,7 @@
  * smp_call_function() if an IPI is sent by the same process we are
  * waiting to become inactive.
  */
-void wait_task_inactive(task_t * p)
+void nicksched_wait_task_inactive(task_t * p)
 {
 	unsigned long flags;
 	runqueue_t *rq;
@@ -799,7 +797,7 @@
 repeat:
 	rq = task_rq_lock(p, &flags);
 	/* Must be off runqueue entirely, not preempted. */
-	if (unlikely(p->array)) {
+	if (unlikely(p->nicksched_data.array)) {
 		/* If it's preempted, we yield.  It could be a while. */
 		preempted = !task_running(rq, p);
 		task_rq_unlock(rq, &flags);
@@ -812,13 +810,13 @@
 }
 
 /***
- * kick_process - kick a running thread to enter/exit the kernel
+ * nicksched_kick_process - kick a running thread to enter/exit the kernel
  * @p: the to-be-kicked thread
  *
  * Cause a process which is running on another CPU to enter
  * kernel-mode, without any delay. (to get signals handled.)
  */
-void kick_process(task_t *p)
+void nicksched_kick_process(task_t *p)
 {
 	int cpu;
 
@@ -829,7 +827,7 @@
 	preempt_enable();
 }
 
-EXPORT_SYMBOL_GPL(kick_process);
+EXPORT_SYMBOL_GPL(nicksched_kick_process);
 
 /*
  * Return a low guess at the load of a migration-source cpu.
@@ -928,7 +926,7 @@
 	if (!(old_state & state))
 		goto out;
 
-	if (p->array)
+	if (p->nicksched_data.array)
 		goto out_running;
 
 	cpu = task_cpu(p);
@@ -1007,7 +1005,7 @@
 		old_state = p->state;
 		if (!(old_state & state))
 			goto out;
-		if (p->array)
+		if (p->nicksched_data.array)
 			goto out_running;
 
 		this_cpu = smp_processor_id();
@@ -1034,15 +1032,13 @@
 	return success;
 }
 
-int fastcall wake_up_process(task_t * p)
+int fastcall nicksched_wake_up_process(task_t * p)
 {
 	return try_to_wake_up(p, TASK_STOPPED | TASK_TRACED |
 				TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE, 0);
 }
 
-EXPORT_SYMBOL(wake_up_process);
-
-int fastcall wake_up_state(task_t *p, unsigned int state)
+int fastcall nicksched_wake_up_state(task_t *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
 }
@@ -1056,7 +1052,7 @@
  * Perform scheduler related setup for a newly forked process p.
  * p is forked by current.
  */
-void fastcall sched_fork(task_t *p)
+void fastcall nicksched_sched_fork(task_t *p)
 {
 	unsigned long sleep_avg;
 	runqueue_t *rq;
@@ -1069,7 +1065,7 @@
 	 */
 	p->state = TASK_RUNNING;
 	INIT_LIST_HEAD(&p->run_list);
-	p->array = NULL;
+	p->nicksched_data.array = NULL;
 	spin_lock_init(&p->switch_lock);
 #ifdef CONFIG_SCHEDSTATS
 	memset(&p->sched_info, 0, sizeof(p->sched_info));
@@ -1077,7 +1073,7 @@
 #ifdef CONFIG_PREEMPT
 	/*
 	 * During context-switch we hold precisely one spinlock, which
-	 * schedule_tail drops. (in the common case it's this_rq()->lock,
+	 * nicksched_schedule_tail drops. (in the common case it's this_rq()->lock,
 	 * but it also can be p->switch_lock.) So we compensate with a count
 	 * of 1. Also, we want to start with kernel preemption disabled.
 	 */
@@ -1094,26 +1090,26 @@
 			printk(KERN_INFO "Renicing %s for you\n", p->comm);
 			warned = 1;
 		}
-		p->static_prio = NICE_TO_PRIO(-10);
+		p->nicksched_data.static_prio = NICE_TO_PRIO(-10);
 	}
 
 	/* Get MIN_HISTORY of history with the same sleep_avg as parent. */
 	sleep_avg = task_sleep_avg(current);
-	p->total_time = MIN_HISTORY;
-	p->sleep_time = p->total_time * sleep_avg / SLEEP_FACTOR;
+	p->nicksched_data.total_time = MIN_HISTORY;
+	p->nicksched_data.sleep_time = p->nicksched_data.total_time * sleep_avg / SLEEP_FACTOR;
 
 	/* Parent loses 1/4 of sleep time for forking */
-	current->sleep_time = 3*current->sleep_time/4;
+	current->nicksched_data.sleep_time = 3*current->nicksched_data.sleep_time/4;
 
-	p->used_slice = 0;
+	p->nicksched_data.used_slice = 0;
 	local_irq_disable();
-	if (unlikely(current->used_slice == -1 || current == rq->idle))
-		p->used_slice = -1;
+	if (unlikely(current->nicksched_data.used_slice == -1 || current == rq->idle))
+		p->nicksched_data.used_slice = -1;
 	else {
 		int ts = task_timeslice(current, rq);
-		current->used_slice += (ts + 3) / 4;
-		if (current->used_slice >= ts) {
-			current->used_slice = -1;
+		current->nicksched_data.used_slice += (ts + 3) / 4;
+		if (current->nicksched_data.used_slice >= ts) {
+			current->nicksched_data.used_slice = -1;
 			set_need_resched();
 		}
 	}
@@ -1122,13 +1118,13 @@
 }
 
 /*
- * wake_up_new_task - wake up a newly created task for the first time.
+ * nicksched_wake_up_new_task - wake up a newly created task for the first time.
  *
  * This function will do some initial scheduler statistics housekeeping
  * that must be done for every newly created context, then puts the task
  * on the runqueue and wakes it.
  */
-void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
+void fastcall nicksched_wake_up_new_task(task_t * p, unsigned long clone_flags)
 {
 	unsigned long flags;
 	int this_cpu, cpu;
@@ -1137,7 +1133,7 @@
 
 	BUG_ON(p->state != TASK_RUNNING);
 
-	p->prio = task_priority(p);
+	p->nicksched_data.prio = task_priority(p);
 	p->timestamp = clock_us();
 
 	rq = task_rq_lock(p, &flags);
@@ -1147,14 +1143,14 @@
 	schedstat_inc(rq, wunt_cnt);
 
 	array = rq->active;
-	if (unlikely(p->used_slice == -1)) {
-		p->used_slice = 0;
+	if (unlikely(p->nicksched_data.used_slice == -1)) {
+		p->nicksched_data.used_slice = 0;
 		array = rq->expired;
 	} else {
 		int total = task_timeslice(p, rq);
 		int ts = max((total + 3) / 4, MIN_TIMESLICE);
 		ts = min(ts, (int)FORKED_TS_MAX);
-		p->used_slice = total - ts;
+		p->nicksched_data.used_slice = total - ts;
 	}
 
 	if (likely(cpu == this_cpu)) {
@@ -1164,11 +1160,11 @@
 			 * do child-runs-first in anticipation of an exec. This
 			 * usually avoids a lot of COW overhead.
 			 */
-			if (p->prio >= current->prio) {
-				p->prio = current->prio;
+			if (p->nicksched_data.prio >= current->nicksched_data.prio) {
+				p->nicksched_data.prio = current->nicksched_data.prio;
 				list_add_tail(&p->run_list, &current->run_list);
-				p->array = current->array;
-				p->array->nr_active++;
+				p->nicksched_data.array = current->nicksched_data.array;
+				p->nicksched_data.array->nr_active++;
 				rq->nr_running++;
 			} else
 				__activate_task(p, rq, array);
@@ -1198,7 +1194,7 @@
 	task_rq_unlock(rq, &flags);
 }
 
-void fastcall sched_exit(task_t * p)
+void fastcall nicksched_sched_exit(task_t * p)
 {
 }
 
@@ -1246,7 +1242,7 @@
  * schedule_tail - first thing a freshly forked thread must call.
  * @prev: the thread we just switched away from.
  */
-asmlinkage void schedule_tail(task_t *prev)
+asmlinkage void nicksched_schedule_tail(task_t *prev)
 {
 	finish_task_switch(prev);
 
@@ -1290,7 +1286,7 @@
  * threads, current number of uninterruptible-sleeping threads, total
  * number of context switches performed since bootup.
  */
-unsigned long nr_running(void)
+unsigned long nicksched_nr_running(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1300,7 +1296,7 @@
 	return sum;
 }
 
-unsigned long nr_uninterruptible(void)
+unsigned long nicksched_nr_uninterruptible(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1310,7 +1306,7 @@
 	return sum;
 }
 
-unsigned long long nr_context_switches(void)
+unsigned long long nicksched_nr_context_switches(void)
 {
 	unsigned long long i, sum = 0;
 
@@ -1320,7 +1316,7 @@
 	return sum;
 }
 
-unsigned long nr_iowait(void)
+unsigned long nicksched_nr_iowait(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1466,7 +1462,7 @@
  * execve() is a valuable balancing opportunity, because at this point
  * the task has the smallest effective memory and cache footprint.
  */
-void sched_exec(void)
+void nicksched_sched_exec(void)
 {
 	struct sched_domain *tmp, *sd = NULL;
 	int new_cpu, this_cpu = get_cpu();
@@ -1507,9 +1503,9 @@
 	set_task_cpu(p, this_cpu);
 	this_rq->nr_running++;
 	enqueue_task(p, this_array);
-	if (!rt_task(p)) {
-		if (p->prio < this_array->min_prio)
-			this_array->min_prio = p->prio;
+	if (!nicksched_rt_task(p)) {
+		if (p->nicksched_data.prio < this_array->min_prio)
+			this_array->min_prio = p->nicksched_data.prio;
 	}
 	p->timestamp = (p->timestamp - src_rq->timestamp_last_tick)
 				+ this_rq->timestamp_last_tick;
@@ -1589,8 +1585,8 @@
 	if (!idx)
 		idx = sched_find_first_bit(array->bitmap);
 	else
-		idx = find_next_bit(array->bitmap, MAX_PRIO, idx);
-	if (idx >= MAX_PRIO) {
+		idx = find_next_bit(array->bitmap, NICK_MAX_PRIO, idx);
+	if (idx >= NICK_MAX_PRIO) {
 		if (array == busiest->expired && busiest->active->nr_active) {
 			array = busiest->active;
 			dst_array = this_rq->active;
@@ -2096,10 +2092,6 @@
 	return 0;
 }
 
-DEFINE_PER_CPU(struct kernel_stat, kstat);
-
-EXPORT_PER_CPU_SYMBOL(kstat);
-
 /*
  * This function gets called by the timer code, with HZ frequency.
  * We call it with interrupts disabled.
@@ -2107,7 +2099,7 @@
  * It also gets called by the fork code, when changing the parent's
  * timeslices.
  */
-void scheduler_tick(int user_ticks, int sys_ticks)
+void nicksched_scheduler_tick(int user_ticks, int sys_ticks)
 {
 	enum idle_type cpu_status;
 	int cpu = smp_processor_id();
@@ -2151,17 +2143,17 @@
 	cpustat->system += sys_ticks;
 
 	/* Task might have expired already, but not scheduled off yet */
-	if (unlikely(p->used_slice == -1))
+	if (unlikely(p->nicksched_data.used_slice == -1))
 		goto out;
 
 	if (unlikely(p->policy == SCHED_FIFO))
 		goto out;
 
 	/* p was running during this tick. Update its time slice counter. */
-	p->used_slice++;
+	p->nicksched_data.used_slice++;
 	ts = task_timeslice(p, rq);
-	if (unlikely(p->used_slice >= ts)) {
-		p->used_slice = -1;
+	if (unlikely(p->nicksched_data.used_slice >= ts)) {
+		p->nicksched_data.used_slice = -1;
 		set_tsk_need_resched(p);
 	}
 
@@ -2262,8 +2254,8 @@
 		 * task from using an unfair proportion of the
 		 * physical cpu's resources. -ck
 		 */
-		if ((smt_curr->static_prio + 5 < p->static_prio) &&
-			p->mm && smt_curr->mm && !rt_task(p))
+		if ((smt_curr->nicksched_data.static_prio + 5 < p->nicksched_data.static_prio) &&
+			p->mm && smt_curr->mm && !nicksched_rt_task(p))
 				ret = 1;
 
 		/*
@@ -2271,8 +2263,8 @@
 		 * or wake it up if it has been put to sleep for priority
 		 * reasons.
 		 */
-		if ((p->static_prio + 5 < smt_curr->static_prio &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
+		if ((p->nicksched_data.static_prio + 5 < smt_curr->nicksched_data.static_prio &&
+			smt_curr->mm && p->mm && !nicksched_rt_task(smt_curr)) ||
 			(smt_curr == smt_rq->idle && smt_rq->nr_running))
 				resched_task(smt_curr);
 	}
@@ -2292,91 +2284,6 @@
 }
 #endif
 
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
-
-void fastcall add_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(((int)preempt_count() < 0));
-	preempt_count() += val;
-	/*
-	 * Spinlock count overflowing soon?
-	 */
-	BUG_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
-}
-EXPORT_SYMBOL(add_preempt_count);
-
-void fastcall sub_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(val > preempt_count());
-	/*
-	 * Is the spinlock portion underflowing?
-	 */
-	BUG_ON((val < PREEMPT_MASK) && !(preempt_count() & PREEMPT_MASK));
-	preempt_count() -= val;
-}
-EXPORT_SYMBOL(sub_preempt_count);
-
-#ifdef __smp_processor_id
-/*
- * Debugging check.
- */
-unsigned int smp_processor_id(void)
-{
-	unsigned long preempt_count = preempt_count();
-	int this_cpu = __smp_processor_id();
-	cpumask_t this_mask;
-
-	if (likely(preempt_count))
-		goto out;
-
-	if (irqs_disabled())
-		goto out;
-
-	/*
-	 * Kernel threads bound to a single CPU can safely use
-	 * smp_processor_id():
-	 */
-	this_mask = cpumask_of_cpu(this_cpu);
-
-	if (cpus_equal(current->cpus_allowed, this_mask))
-		goto out;
-
-	/*
-	 * It is valid to assume CPU-locality during early bootup:
-	 */
-	if (system_state != SYSTEM_RUNNING)
-		goto out;
-
-	/*
-	 * Avoid recursion:
-	 */
-	preempt_disable();
-
-	if (!printk_ratelimit())
-		goto out_enable;
-
-	printk(KERN_ERR "using smp_processor_id() in preemptible code: %s/%d\n",
-		current->comm, current->pid);
-	dump_stack();
-
-out_enable:
-	preempt_enable_no_resched();
-out:
-	return this_cpu;
-}
-
-EXPORT_SYMBOL(smp_processor_id);
-
-#endif /* __smp_processor_id */
-
-#endif /* PREEMPT && DEBUG_PREEMPT */
-
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 
 #ifdef CONFIG_PREEMPT_BKL
@@ -2384,7 +2291,7 @@
  * The 'big kernel semaphore'
  *
  * This mutex is taken and released recursively by lock_kernel()
- * and unlock_kernel().  It is transparently dropped and reaquired
+ * and nicksched_unlock_kernel().  It is transparently dropped and reaquired
  * over schedule().  It is used to protect legacy code that hasn't
  * been migrated to a proper locking design yet.
  *
@@ -2396,12 +2303,12 @@
  */
 static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
 
-int kernel_locked(void)
+int nicksched_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
+EXPORT_SYMBOL(nicksched_kernel_locked);
 
 /*
  * Release global kernel semaphore:
@@ -2441,7 +2348,7 @@
 /*
  * Getting the big kernel semaphore.
  */
-void lock_kernel(void)
+void nicksched_lock_kernel(void)
 {
 	struct task_struct *task = current;
 	int depth = task->lock_depth + 1;
@@ -2455,9 +2362,9 @@
 	task->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
+EXPORT_SYMBOL(nicksched_lock_kernel);
 
-void unlock_kernel(void)
+void nicksched_unlock_kernel(void)
 {
 	struct task_struct *task = current;
 
@@ -2467,18 +2374,18 @@
 		up(&kernel_sem);
 }
 
-EXPORT_SYMBOL(unlock_kernel);
+EXPORT_SYMBOL(nicksched_unlock_kernel);
 
 #else
 
 static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
 
-int kernel_locked(void)
+int nicksched_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
+EXPORT_SYMBOL(nicksched_kernel_locked);
 
 #define get_kernel_lock()	spin_lock(&kernel_flag)
 #define put_kernel_lock()	spin_unlock(&kernel_flag)
@@ -2508,7 +2415,7 @@
  * so we only need to worry about other
  * CPU's.
  */
-void lock_kernel(void)
+void nicksched_lock_kernel(void)
 {
 	int depth = current->lock_depth+1;
 	if (likely(!depth))
@@ -2516,16 +2423,16 @@
 	current->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
+EXPORT_SYMBOL(nicksched_lock_kernel);
 
-void unlock_kernel(void)
+void nicksched_unlock_kernel(void)
 {
 	BUG_ON(current->lock_depth < 0);
 	if (likely(--current->lock_depth < 0))
 		put_kernel_lock();
 }
 
-EXPORT_SYMBOL(unlock_kernel);
+EXPORT_SYMBOL(nicksched_unlock_kernel);
 
 #endif
 
@@ -2540,7 +2447,7 @@
 /*
  * schedule() is the main scheduler function.
  */
-asmlinkage void __sched schedule(void)
+asmlinkage void __sched nicksched_schedule(void)
 {
 	long *switch_count;
 	task_t *prev, *next;
@@ -2553,7 +2460,7 @@
 
 	/*
 	 * Test if we are atomic.  Since do_exit() needs to call into
-	 * schedule() atomically, we ignore that path for now.
+	 * nicksched_schedule() atomically, we ignore that path for now.
 	 * Otherwise, whine if we are scheduling when we should not be.
 	 */
 	if (unlikely(in_atomic()) &&
@@ -2570,7 +2477,7 @@
 	rq = this_rq();
 
 	/*
-	 * The idle thread is not allowed to schedule!
+	 * The idle thread is not allowed to nicksched_schedule!
 	 * Remove this check after it has been exercised a bit.
 	 */
 	if (unlikely(current == rq->idle) && current->state != TASK_RUNNING) {
@@ -2605,19 +2512,19 @@
 		}
 	}
 
-	if (unlikely(prev->used_slice == -1)) {
-		if (rt_task(prev)) {
+	if (unlikely(prev->nicksched_data.used_slice == -1)) {
+		if (nicksched_rt_task(prev)) {
 			/* SCHED_FIFO can come in here too, from sched_yield */
-			dequeue_task(prev, prev->array);
+			dequeue_task(prev, prev->nicksched_data.array);
 			enqueue_task(prev, rq->active);
 		} else {
-			dequeue_task(prev, prev->array);
-			prev->prio = task_priority(prev);
+			dequeue_task(prev, prev->nicksched_data.array);
+			prev->nicksched_data.prio = task_priority(prev);
 			enqueue_task(prev, rq->expired);
-			if (prev->prio < rq->expired->min_prio)
-				rq->expired->min_prio = prev->prio;
+			if (prev->nicksched_data.prio < rq->expired->min_prio)
+				rq->expired->min_prio = prev->nicksched_data.prio;
 		}
-		prev->used_slice = 0;
+		prev->nicksched_data.used_slice = 0;
 	}
 no_check_expired:
 
@@ -2627,8 +2534,8 @@
 		rq->array_sequence++;
 		idle_balance(cpu, rq);
 		if (!rq->nr_running) {
-			rq->arrays[0].min_prio = MAX_PRIO;
-			rq->arrays[1].min_prio = MAX_PRIO;
+			rq->arrays[0].min_prio = NICK_MAX_PRIO;
+			rq->arrays[1].min_prio = NICK_MAX_PRIO;
 			next = rq->idle;
 			wake_sleeping_dependent(cpu, rq);
 			/*
@@ -2663,7 +2570,7 @@
 		rq->array_sequence++;
 		rq->active = rq->expired;
 		rq->expired = array;
-		rq->expired->min_prio = MAX_PRIO;
+		rq->expired->min_prio = NICK_MAX_PRIO;
 		array = rq->active;
 	} else
 		schedstat_inc(rq, sched_noswitch);
@@ -2698,15 +2605,15 @@
 		goto need_resched;
 }
 
-EXPORT_SYMBOL(schedule);
+EXPORT_SYMBOL(nicksched_schedule);
 
 #ifdef CONFIG_PREEMPT
 /*
- * this is is the entry point to schedule() from in-kernel preemption
+ * this is is the entry point to nicksched_schedule() from in-kernel preemption
  * off of preempt_enable.  Kernel preemptions off return from interrupt
- * occur there and call schedule directly.
+ * occur there and call nicksched_schedule directly.
  */
-asmlinkage void __sched preempt_schedule(void)
+asmlinkage void __sched nicksched_preempt_schedule(void)
 {
 	struct thread_info *ti = current_thread_info();
 #ifdef CONFIG_PREEMPT_BKL
@@ -2727,14 +2634,14 @@
 	add_preempt_count(PREEMPT_ACTIVE);
 	/*
 	 * We keep the big kernel semaphore locked, but we
-	 * clear ->lock_depth so that schedule() doesnt
+	 * clear ->lock_depth so that nicksched_schedule() doesnt
 	 * auto-release the semaphore:
 	 */
 #ifdef CONFIG_PREEMPT_BKL
 	saved_lock_depth = task->lock_depth;
 	task->lock_depth = -1;
 #endif
-	schedule();
+	nicksched_schedule();
 #ifdef CONFIG_PREEMPT_BKL
 	task->lock_depth = saved_lock_depth;
 #endif
@@ -2746,16 +2653,16 @@
 		goto need_resched;
 }
 
-EXPORT_SYMBOL(preempt_schedule);
+EXPORT_SYMBOL(nicksched_preempt_schedule);
 #endif /* CONFIG_PREEMPT */
 
-int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
+int nicksched_default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
 {
 	task_t *p = curr->task;
 	return try_to_wake_up(p, mode, sync);
 }
 
-EXPORT_SYMBOL(default_wake_function);
+EXPORT_SYMBOL(nicksched_default_wake_function);
 
 /*
  * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
@@ -2789,7 +2696,7 @@
  * @mode: which threads
  * @nr_exclusive: how many wake-one or wake-many threads to wake up
  */
-void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
+void fastcall nicksched___wake_up(wait_queue_head_t *q, unsigned int mode,
 				int nr_exclusive, void *key)
 {
 	unsigned long flags;
@@ -2799,18 +2706,18 @@
 	spin_unlock_irqrestore(&q->lock, flags);
 }
 
-EXPORT_SYMBOL(__wake_up);
+EXPORT_SYMBOL(nicksched___wake_up);
 
 /*
- * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
+ * Same as nicksched___wake_up but called with the spinlock in wait_queue_head_t held.
  */
-void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
+void fastcall nicksched___wake_up_locked(wait_queue_head_t *q, unsigned int mode)
 {
 	__wake_up_common(q, mode, 1, 0, NULL);
 }
 
 /**
- * __wake_up - sync- wake up threads blocked on a waitqueue.
+ * nicksched___wake_up - sync- wake up threads blocked on a waitqueue.
  * @q: the waitqueue
  * @mode: which threads
  * @nr_exclusive: how many wake-one or wake-many threads to wake up
@@ -2822,7 +2729,7 @@
  *
  * On UP it can prevent extra preemption.
  */
-void fastcall __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
+void fastcall nicksched___wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
 {
 	unsigned long flags;
 	int sync = 1;
@@ -2837,9 +2744,9 @@
 	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
-EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
+EXPORT_SYMBOL_GPL(nicksched___wake_up_sync);	/* For internal use only */
 
-void fastcall complete(struct completion *x)
+void fastcall nicksched_complete(struct completion *x)
 {
 	unsigned long flags;
 
@@ -2849,9 +2756,9 @@
 			 1, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete);
+EXPORT_SYMBOL(nicksched_complete);
 
-void fastcall complete_all(struct completion *x)
+void fastcall nicksched_complete_all(struct completion *x)
 {
 	unsigned long flags;
 
@@ -2861,9 +2768,9 @@
 			 0, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete_all);
+EXPORT_SYMBOL(nicksched_complete_all);
 
-void fastcall __sched wait_for_completion(struct completion *x)
+void fastcall __sched nicksched_wait_for_completion(struct completion *x)
 {
 	might_sleep();
 	spin_lock_irq(&x->wait.lock);
@@ -2875,7 +2782,7 @@
 		do {
 			__set_current_state(TASK_UNINTERRUPTIBLE);
 			spin_unlock_irq(&x->wait.lock);
-			schedule();
+			nicksched_schedule();
 			spin_lock_irq(&x->wait.lock);
 		} while (!x->done);
 		__remove_wait_queue(&x->wait, &wait);
@@ -2883,7 +2790,6 @@
 	x->done--;
 	spin_unlock_irq(&x->wait.lock);
 }
-EXPORT_SYMBOL(wait_for_completion);
 
 #define	SLEEP_ON_VAR					\
 	unsigned long flags;				\
@@ -2900,20 +2806,20 @@
 	__remove_wait_queue(q, &wait);			\
 	spin_unlock_irqrestore(&q->lock, flags);
 
-void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+void fastcall __sched nicksched_interruptible_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_INTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	nicksched_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on);
+EXPORT_SYMBOL(nicksched_interruptible_sleep_on);
 
-long fastcall __sched interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched nicksched_interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -2926,22 +2832,22 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on_timeout);
+EXPORT_SYMBOL(nicksched_interruptible_sleep_on_timeout);
 
-void fastcall __sched sleep_on(wait_queue_head_t *q)
+void fastcall __sched nicksched_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_UNINTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	nicksched_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(sleep_on);
+EXPORT_SYMBOL(nicksched_sleep_on);
 
-long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched nicksched_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -2954,9 +2860,9 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(sleep_on_timeout);
+EXPORT_SYMBOL(nicksched_sleep_on_timeout);
 
-void set_user_nice(task_t *p, long nice)
+void nicksched_set_user_nice(task_t *p, long nice)
 {
 	unsigned long flags;
 	prio_array_t *array;
@@ -2976,19 +2882,19 @@
 	 * it wont have any effect on scheduling until the task is
 	 * not SCHED_NORMAL:
 	 */
-	if (rt_task(p)) {
-		p->static_prio = NICE_TO_PRIO(nice);
+	if (nicksched_rt_task(p)) {
+		p->nicksched_data.static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
-	array = p->array;
+	array = p->nicksched_data.array;
 	if (array)
 		dequeue_task(p, array);
 
-	old_prio = p->prio;
+	old_prio = p->nicksched_data.prio;
 	new_prio = NICE_TO_PRIO(nice);
 	delta = new_prio - old_prio;
-	p->static_prio = NICE_TO_PRIO(nice);
-	p->prio += delta;
+	p->nicksched_data.static_prio = NICE_TO_PRIO(nice);
+	p->nicksched_data.prio += delta;
 
 	if (array) {
 		enqueue_task(p, array);
@@ -3003,7 +2909,7 @@
 	task_rq_unlock(rq, &flags);
 }
 
-EXPORT_SYMBOL(set_user_nice);
+EXPORT_SYMBOL(nicksched_set_user_nice);
 
 #ifdef CONFIG_KGDB
 struct task_struct *kgdb_get_idle(int this_cpu)
@@ -3015,13 +2921,13 @@
 #ifdef __ARCH_WANT_SYS_NICE
 
 /*
- * sys_nice - change the priority of the current process.
+ * nicksched_sys_nice - change the priority of the current process.
  * @increment: priority increment
  *
  * sys_setpriority is a more generic, but much slower function that
  * does similar things.
  */
-asmlinkage long sys_nice(int increment)
+asmlinkage long nicksched_sys_nice(int increment)
 {
 	int retval;
 	long nice;
@@ -3040,7 +2946,7 @@
 	if (increment > 40)
 		increment = 40;
 
-	nice = PRIO_TO_NICE(current->static_prio) + increment;
+	nice = PRIO_TO_NICE(current->nicksched_data.static_prio) + increment;
 	if (nice < -20)
 		nice = -20;
 	if (nice > 19)
@@ -3050,47 +2956,45 @@
 	if (retval)
 		return retval;
 
-	set_user_nice(current, nice);
+	nicksched_set_user_nice(current, nice);
 	return 0;
 }
 
 #endif
 
 /**
- * task_prio - return the priority value of a given task.
+ * nicksched_task_prio - return the priority value of a given task.
  * @p: the task in question.
  *
  * This is the priority value as seen by users in /proc.
  * RT tasks are offset by -200. Normal tasks are centered
  * around 0, value goes from -16 to +15.
  */
-int task_prio(const task_t *p)
+int nicksched_task_prio(const task_t *p)
 {
-	return p->prio - MAX_RT_PRIO;
+	return p->nicksched_data.prio - MAX_RT_PRIO;
 }
 
 /**
- * task_nice - return the nice value of a given task.
+ * nicksched_task_nice - return the nice value of a given task.
  * @p: the task in question.
  */
-int task_nice(const task_t *p)
+int nicksched_task_nice(const task_t *p)
 {
 	return TASK_NICE(p);
 }
 
-EXPORT_SYMBOL(task_nice);
+EXPORT_SYMBOL(nicksched_task_nice);
 
 /**
- * idle_cpu - is a given cpu idle currently?
+ * nicksched_idle_cpu - is a given cpu idle currently?
  * @cpu: the processor in question.
  */
-int idle_cpu(int cpu)
+int nicksched_idle_cpu(int cpu)
 {
 	return cpu_curr(cpu) == cpu_rq(cpu)->idle;
 }
 
-EXPORT_SYMBOL_GPL(idle_cpu);
-
 /**
  * find_process_by_pid - find a process with a matching PID value.
  * @pid: the pid in question.
@@ -3103,13 +3007,13 @@
 /* Actually do priority change: must hold rq lock. */
 static void __setscheduler(struct task_struct *p, int policy, int prio)
 {
-	BUG_ON(p->array);
+	BUG_ON(p->nicksched_data.array);
 	p->policy = policy;
 	p->rt_priority = prio;
 	if (policy != SCHED_NORMAL)
-		p->prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
+		p->nicksched_data.prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
 	else
-		p->prio = p->static_prio;
+		p->nicksched_data.prio = p->nicksched_data.static_prio;
 }
 
 /*
@@ -3181,14 +3085,14 @@
 	if (retval)
 		goto out_unlock;
 
-	array = p->array;
+	array = p->nicksched_data.array;
 	if (array)
 		deactivate_task(p, rq);
 	retval = 0;
-	oldprio = p->prio;
+	oldprio = p->nicksched_data.prio;
 	__setscheduler(p, policy, lp.sched_priority);
 	if (policy == SCHED_FIFO || policy == SCHED_RR)
-		p->used_slice = 0;
+		p->nicksched_data.used_slice = 0;
 
 	if (array) {
 		__activate_task(p, rq, array);
@@ -3198,7 +3102,7 @@
 		 * this runqueue and our priority is higher than the current's
 		 */
 		if (task_running(rq, p)) {
-			if (p->prio > oldprio)
+			if (p->nicksched_data.prio > oldprio)
 				resched_task(rq->curr);
 		} else if (TASK_PREEMPTS_CURR(p, rq))
 			resched_task(rq->curr);
@@ -3219,7 +3123,7 @@
  * @policy: new policy
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
+asmlinkage long nicksched_sys_sched_setscheduler(pid_t pid, int policy,
 				       struct sched_param __user *param)
 {
 	return setscheduler(pid, policy, param);
@@ -3230,7 +3134,7 @@
  * @pid: the pid in question.
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long nicksched_sys_sched_setparam(pid_t pid, struct sched_param __user *param)
 {
 	return setscheduler(pid, -1, param);
 }
@@ -3239,7 +3143,7 @@
  * sys_sched_getscheduler - get the policy (scheduling class) of a thread
  * @pid: the pid in question.
  */
-asmlinkage long sys_sched_getscheduler(pid_t pid)
+asmlinkage long nicksched_sys_sched_getscheduler(pid_t pid)
 {
 	int retval = -EINVAL;
 	task_t *p;
@@ -3266,7 +3170,7 @@
  * @pid: the pid in question.
  * @param: structure containing the RT priority.
  */
-asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long nicksched_sys_sched_getparam(pid_t pid, struct sched_param __user *param)
 {
 	struct sched_param lp;
 	int retval = -EINVAL;
@@ -3301,7 +3205,7 @@
 	return retval;
 }
 
-long sched_setaffinity(pid_t pid, cpumask_t new_mask)
+long nicksched_sched_setaffinity(pid_t pid, cpumask_t new_mask)
 {
 	task_t *p;
 	int retval;
@@ -3318,7 +3222,7 @@
 	}
 
 	/*
-	 * It is not safe to call set_cpus_allowed with the
+	 * It is not safe to call nicksched_set_cpus_allowed with the
 	 * tasklist_lock held.  We will bump the task_struct's
 	 * usage count and then drop tasklist_lock.
 	 */
@@ -3340,52 +3244,7 @@
 	return retval;
 }
 
-static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
-			     cpumask_t *new_mask)
-{
-	if (len < sizeof(cpumask_t)) {
-		memset(new_mask, 0, sizeof(cpumask_t));
-	} else if (len > sizeof(cpumask_t)) {
-		len = sizeof(cpumask_t);
-	}
-	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
-}
-
-/**
- * sys_sched_setaffinity - set the cpu affinity of a process
- * @pid: pid of the process
- * @len: length in bytes of the bitmask pointed to by user_mask_ptr
- * @user_mask_ptr: user-space pointer to the new cpu mask
- */
-asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
-				      unsigned long __user *user_mask_ptr)
-{
-	cpumask_t new_mask;
-	int retval;
-
-	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
-	if (retval)
-		return retval;
-
-	return sched_setaffinity(pid, new_mask);
-}
-
-/*
- * Represents all cpu's present in the system
- * In systems capable of hotplug, this map could dynamically grow
- * as new cpu's are detected in the system via any platform specific
- * method, such as ACPI for e.g.
- */
-
-cpumask_t cpu_present_map;
-EXPORT_SYMBOL(cpu_present_map);
-
-#ifndef CONFIG_SMP
-cpumask_t cpu_online_map = CPU_MASK_ALL;
-cpumask_t cpu_possible_map = CPU_MASK_ALL;
-#endif
-
-long sched_getaffinity(pid_t pid, cpumask_t *mask)
+long nicksched_sched_getaffinity(pid_t pid, cpumask_t *mask)
 {
 	int retval;
 	task_t *p;
@@ -3416,7 +3275,7 @@
  * @len: length in bytes of the bitmask pointed to by user_mask_ptr
  * @user_mask_ptr: user-space pointer to hold the current cpu mask
  */
-asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
+asmlinkage long nicksched_sys_sched_getaffinity(pid_t pid, unsigned int len,
 				      unsigned long __user *user_mask_ptr)
 {
 	int ret;
@@ -3436,13 +3295,13 @@
 }
 
 /**
- * sys_sched_yield - yield the current processor to other threads.
+ * nicksched_sys_sched_yield - yield the current processor to other threads.
  *
  * this function yields the current CPU by moving the calling thread
  * to the expired array. If there are no other threads running on this
  * CPU then this function will return.
  */
-asmlinkage long sys_sched_yield(void)
+asmlinkage long nicksched_sys_sched_yield(void)
 {
 #ifdef CONFIG_SCHEDSTATS
 	runqueue_t *rq;
@@ -3454,111 +3313,39 @@
 
 	schedstat_inc(rq, yld_cnt);
 	spin_lock(&rq->lock);
-	if (current->array->nr_active == 1) {
+	if (current->nicksched_data.array->nr_active == 1) {
 		schedstat_inc(rq, yld_act_empty);
 		if (!rq->expired->nr_active)
 			schedstat_inc(rq, yld_both_empty);
 	} else if (!rq->expired->nr_active)
 		schedstat_inc(rq, yld_exp_empty);
 	/*
-	 * Since we are going to call schedule() anyway, there's
+	 * Since we are going to call nicksched_schedule() anyway, there's
 	 * no need to preempt or enable interrupts:
 	 */
 	_raw_spin_unlock(&rq->lock);
 	preempt_enable_no_resched();
 #endif
-	current->used_slice = -1;
+	current->nicksched_data.used_slice = -1;
 	local_irq_enable();
 
-	schedule();
-
-	return 0;
-}
-
-static inline void __cond_resched(void)
-{
-
-
-	if (preempt_count() & PREEMPT_ACTIVE)
-		return;
-	do {
-		add_preempt_count(PREEMPT_ACTIVE);
-		schedule();
-		sub_preempt_count(PREEMPT_ACTIVE);
-	} while (need_resched());
-}
-
-int __sched cond_resched(void)
-{
-	if (need_resched()) {
-		__cond_resched();
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched);
-
-/*
- * cond_resched_lock() - if a reschedule is pending, drop the given lock,
- * call schedule, and on return reacquire the lock.
- *
- * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
- * operations here to prevent schedule() from being called twice (once via
- * spin_unlock(), once by hand).
- */
-int cond_resched_lock(spinlock_t * lock)
-{
-#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT)
-	if (lock->break_lock) {
-		lock->break_lock = 0;
-		spin_unlock(lock);
-		cpu_relax();
-		spin_lock(lock);
-	}
-#endif
-	if (need_resched()) {
-		_raw_spin_unlock(lock);
-		preempt_enable_no_resched();
-		__cond_resched();
-		spin_lock(lock);
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched_lock);
+	nicksched_schedule();
 
-int __sched cond_resched_softirq(void)
-{
-	BUG_ON(!in_softirq());
-
-	if (need_resched()) {
-		__local_bh_enable();
-		__cond_resched();
-		local_bh_disable();
-		return 1;
-	}
 	return 0;
 }
 
-EXPORT_SYMBOL(cond_resched_softirq);
-
-
 /**
  * yield - yield the current processor to other threads.
  *
  * this is a shortcut for kernel-space yielding - it marks the
  * thread runnable and calls sys_sched_yield().
  */
-void __sched yield(void)
+void __sched nicksched_yield(void)
 {
 	set_current_state(TASK_RUNNING);
 	sys_sched_yield();
 }
 
-EXPORT_SYMBOL(yield);
-
 /*
  * This task is about to go to sleep on IO.  Increment rq->nr_iowait so
  * that process accounting knows that this is a task in IO wait state.
@@ -3566,18 +3353,18 @@
  * But don't do that if it is a deliberate, throttling IO wait (this task
  * has set its backing_dev_info: the queue against which it should throttle)
  */
-void __sched io_schedule(void)
+void __sched nicksched_io_schedule(void)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 
 	atomic_inc(&rq->nr_iowait);
-	schedule();
+	nicksched_schedule();
 	atomic_dec(&rq->nr_iowait);
 }
 
-EXPORT_SYMBOL(io_schedule);
+EXPORT_SYMBOL(nicksched_io_schedule);
 
-long __sched io_schedule_timeout(long timeout)
+long __sched nicksched_io_schedule_timeout(long timeout)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 	long ret;
@@ -3589,52 +3376,7 @@
 }
 
 /**
- * sys_sched_get_priority_max - return maximum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the maximum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_max(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = MAX_USER_RT_PRIO-1;
-		break;
-	case SCHED_NORMAL:
-		ret = 0;
-		break;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_get_priority_min - return minimum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the minimum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_min(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = 1;
-		break;
-	case SCHED_NORMAL:
-		ret = 0;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_rr_get_interval - return the default timeslice of a process.
+ * nicksched_sys_sched_rr_get_interval - return the default timeslice of a process.
  * @pid: pid of the process.
  * @interval: userspace pointer to the timeslice value.
  *
@@ -3642,7 +3384,7 @@
  * into the user-space timespec buffer. A value of '0' means infinity.
  */
 asmlinkage
-long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
+long nicksched_sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
 {
 	int retval = -EINVAL;
 	struct timespec t;
@@ -3675,113 +3417,15 @@
 	return retval;
 }
 
-static inline struct task_struct *eldest_child(struct task_struct *p)
-{
-	if (list_empty(&p->children)) return NULL;
-	return list_entry(p->children.next,struct task_struct,sibling);
-}
-
-static inline struct task_struct *older_sibling(struct task_struct *p)
-{
-	if (p->sibling.prev==&p->parent->children) return NULL;
-	return list_entry(p->sibling.prev,struct task_struct,sibling);
-}
-
-static inline struct task_struct *younger_sibling(struct task_struct *p)
-{
-	if (p->sibling.next==&p->parent->children) return NULL;
-	return list_entry(p->sibling.next,struct task_struct,sibling);
-}
-
-static void show_task(task_t * p)
-{
-	task_t *relative;
-	unsigned state;
-	unsigned long free = 0;
-	static const char *stat_nam[] = { "R", "S", "D", "T", "t", "Z", "X" };
-
-	printk("%-13.13s ", p->comm);
-	state = p->state ? __ffs(p->state) + 1 : 0;
-	if (state < ARRAY_SIZE(stat_nam))
-		printk(stat_nam[state]);
-	else
-		printk("?");
-#if (BITS_PER_LONG == 32)
-	if (state == TASK_RUNNING)
-		printk(" running ");
-	else
-		printk(" %08lX ", thread_saved_pc(p));
-#else
-	if (state == TASK_RUNNING)
-		printk("  running task   ");
-	else
-		printk(" %016lx ", thread_saved_pc(p));
-#endif
-#ifdef CONFIG_DEBUG_STACK_USAGE
-	{
-		unsigned long * n = (unsigned long *) (p->thread_info+1);
-		while (!*n)
-			n++;
-		free = (unsigned long) n - (unsigned long)(p->thread_info+1);
-	}
-#endif
-	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
-	if ((relative = eldest_child(p)))
-		printk("%5d ", relative->pid);
-	else
-		printk("      ");
-	if ((relative = younger_sibling(p)))
-		printk("%7d", relative->pid);
-	else
-		printk("       ");
-	if ((relative = older_sibling(p)))
-		printk(" %5d", relative->pid);
-	else
-		printk("      ");
-	if (!p->mm)
-		printk(" (L-TLB)\n");
-	else
-		printk(" (NOTLB)\n");
-
-	if (state != TASK_RUNNING)
-		show_stack(p, NULL);
-}
-
-void show_state(void)
-{
-	task_t *g, *p;
-
-#if (BITS_PER_LONG == 32)
-	printk("\n"
-	       "                                               sibling\n");
-	printk("  task             PC      pid father child younger older\n");
-#else
-	printk("\n"
-	       "                                                       sibling\n");
-	printk("  task                 PC          pid father child younger older\n");
-#endif
-	read_lock(&tasklist_lock);
-	do_each_thread(g, p) {
-		/*
-		 * reset the NMI-timeout, listing all files on a slow
-		 * console might take alot of time:
-		 */
-		touch_nmi_watchdog();
-		show_task(p);
-	} while_each_thread(g, p);
-
-	read_unlock(&tasklist_lock);
-}
-
-void __devinit init_idle(task_t *idle, int cpu)
+void __devinit nicksched_init_idle(task_t *idle, int cpu)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long flags;
 
-	idle->array = NULL;
-	idle->prio = MAX_PRIO;
+	idle->nicksched_data.array = NULL;
+	idle->nicksched_data.prio = NICK_MAX_PRIO;
 	idle->state = TASK_RUNNING;
-	idle->used_slice = 0;
+	idle->nicksched_data.used_slice = 0;
 	set_task_cpu(idle, cpu);
 
 	spin_lock_irqsave(&rq->lock, flags);
@@ -3797,15 +3441,6 @@
 #endif
 }
 
-/*
- * In a system that switches off the HZ timer nohz_cpu_mask
- * indicates which cpus entered this state. This is used
- * in the rcu update to wait only for active cpus. For system
- * which do not switch off the HZ timer nohz_cpu_mask should
- * always be CPU_MASK_NONE.
- */
-cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
-
 #ifdef CONFIG_SMP
 /*
  * This is how migration works:
@@ -3832,7 +3467,7 @@
  * task must not exit() & deallocate itself prematurely.  The
  * call is not atomic; no spinlocks may be held.
  */
-int set_cpus_allowed(task_t *p, cpumask_t new_mask)
+int nicksched_set_cpus_allowed(task_t *p, cpumask_t new_mask)
 {
 	unsigned long flags;
 	int ret = 0;
@@ -3865,13 +3500,11 @@
 	return ret;
 }
 
-EXPORT_SYMBOL_GPL(set_cpus_allowed);
-
 /*
  * Move (not current) task off this cpu, onto dest cpu.  We're doing
- * this because either it can't run here any more (set_cpus_allowed()
+ * this because either it can't run here any more (nicksched_set_cpus_allowed()
  * away from this CPU, or CPU going down), or because we're
- * attempting to rebalance this task on exec (sched_exec).
+ * attempting to rebalance this task on exec (nicksched_sched_exec).
  *
  * So we race with normal scheduler movements, but that's OK, as long
  * as the task is no longer on this CPU.
@@ -3895,7 +3528,7 @@
 		goto out;
 
 	set_task_cpu(p, dest_cpu);
-	if (p->array) {
+	if (p->nicksched_data.array) {
 		/*
 		 * Sync timestamp with rq_dest's before activating.
 		 * The same thing could be achieved by doing this step
@@ -3951,7 +3584,7 @@
 
 		if (list_empty(head)) {
 			spin_unlock_irq(&rq->lock);
-			schedule();
+			nicksched_schedule();
 			set_current_state(TASK_INTERRUPTIBLE);
 			continue;
 		}
@@ -3971,7 +3604,7 @@
 			WARN_ON(1);
 		}
 
-		complete(&req->done);
+		nicksched_complete(&req->done);
 	}
 	__set_current_state(TASK_RUNNING);
 	return 0;
@@ -3980,7 +3613,7 @@
 	/* Wait for kthread_stop */
 	set_current_state(TASK_INTERRUPTIBLE);
 	while (!kthread_should_stop()) {
-		schedule();
+		nicksched_schedule();
 		set_current_state(TASK_INTERRUPTIBLE);
 	}
 	__set_current_state(TASK_RUNNING);
@@ -4096,7 +3729,7 @@
 	struct runqueue *rq = cpu_rq(dead_cpu);
 
 	for (arr = 0; arr < 2; arr++) {
-		for (i = 0; i < MAX_PRIO; i++) {
+		for (i = 0; i < NICK_MAX_PRIO; i++) {
 			struct list_head *list = &rq->arrays[arr].queue[i];
 			while (!list_empty(list))
 				migrate_dead(dead_cpu,
@@ -4151,7 +3784,7 @@
 		/* Idle task back to normal (off runqueue, low prio) */
 		rq = task_rq_lock(rq->idle, &flags);
 		deactivate_task(rq->idle, rq);
-		rq->idle->static_prio = MAX_PRIO;
+		rq->idle->nicksched_data.static_prio = NICK_MAX_PRIO;
 		__setscheduler(rq->idle, SCHED_NORMAL, 0);
 		migrate_dead_tasks(cpu);
 		task_rq_unlock(rq, &flags);
@@ -4167,7 +3800,7 @@
 					 migration_req_t, list);
 			BUG_ON(req->type != REQ_MOVE_TASK);
 			list_del_init(&req->list);
-			complete(&req->done);
+			nicksched_complete(&req->done);
 		}
 		spin_unlock_irq(&rq->lock);
 		break;
@@ -4184,7 +3817,7 @@
 	.priority = 10
 };
 
-int __init migration_init(void)
+int __init nicksched_migration_init(void)
 {
 	void *cpu = (void *)(long)smp_processor_id();
 	/* Start one for boot CPU. */
@@ -4200,7 +3833,7 @@
  * Attach the domain 'sd' to 'cpu' as its base domain.  Callers must
  * hold the hotplug lock.
  */
-void __devinit cpu_attach_domain(struct sched_domain *sd, int cpu)
+void __devinit nicksched_cpu_attach_domain(struct sched_domain *sd, int cpu)
 {
 	migration_req_t req;
 	unsigned long flags;
@@ -4228,7 +3861,7 @@
 }
 
 /* cpus with isolated domains */
-cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
+cpumask_t __devinitdata nicksched_cpu_isolated_map = CPU_MASK_NONE;
 
 /* Setup the mask of cpus configured for isolated domains */
 static int __init isolated_cpu_setup(char *str)
@@ -4236,26 +3869,26 @@
 	int ints[NR_CPUS], i;
 
 	str = get_options(str, ARRAY_SIZE(ints), ints);
-	cpus_clear(cpu_isolated_map);
+	cpus_clear(nicksched_cpu_isolated_map);
 	for (i = 1; i <= ints[0]; i++)
-		cpu_set(ints[i], cpu_isolated_map);
+		cpu_set(ints[i], nicksched_cpu_isolated_map);
 	return 1;
 }
 
 __setup ("isolcpus=", isolated_cpu_setup);
 
 /*
- * init_sched_build_groups takes an array of groups, the cpumask we wish
+ * nicksched_init_sched_build_groups takes an array of groups, the cpumask we wish
  * to span, and a pointer to a function which identifies what group a CPU
  * belongs to. The return value of group_fn must be a valid index into the
  * groups[] array, and must be >= 0 and < NR_CPUS (due to the fact that we
  * keep track of groups covered with a cpumask_t).
  *
- * init_sched_build_groups will build a circular linked list of the groups
+ * nicksched_init_sched_build_groups will build a circular linked list of the groups
  * covered by the given span, and will set each group's ->cpumask correctly,
  * and ->cpu_power to 0.
  */
-void __devinit init_sched_build_groups(struct sched_group groups[],
+void __devinit nicksched_init_sched_build_groups(struct sched_group groups[],
 			cpumask_t span, int (*group_fn)(int cpu))
 {
 	struct sched_group *first = NULL, *last = NULL;
@@ -4337,7 +3970,7 @@
 	 * For now this just excludes isolated cpus, but could be used to
 	 * exclude other special cases in the future.
 	 */
-	cpus_complement(cpu_default_map, cpu_isolated_map);
+	cpus_complement(cpu_default_map, nicksched_cpu_isolated_map);
 	cpus_and(cpu_default_map, cpu_default_map, cpu_online_map);
 
 	/*
@@ -4386,7 +4019,7 @@
 		if (i != first_cpu(this_sibling_map))
 			continue;
 
-		init_sched_build_groups(sched_group_cpus, this_sibling_map,
+		nicksched_init_sched_build_groups(sched_group_cpus, this_sibling_map,
 						&cpu_to_cpu_group);
 	}
 #endif
@@ -4399,13 +4032,13 @@
 		if (cpus_empty(nodemask))
 			continue;
 
-		init_sched_build_groups(sched_group_phys, nodemask,
+		nicksched_init_sched_build_groups(sched_group_phys, nodemask,
 						&cpu_to_phys_group);
 	}
 
 #ifdef CONFIG_NUMA
 	/* Set up node groups */
-	init_sched_build_groups(sched_group_nodes, cpu_default_map,
+	nicksched_init_sched_build_groups(sched_group_nodes, cpu_default_map,
 					&cpu_to_node_group);
 #endif
 
@@ -4441,7 +4074,7 @@
 #else
 		sd = &per_cpu(phys_domains, i);
 #endif
-		cpu_attach_domain(sd, i);
+		nicksched_cpu_attach_domain(sd, i);
 	}
 }
 
@@ -4568,7 +4201,7 @@
 	case CPU_UP_PREPARE:
 	case CPU_DOWN_PREPARE:
 		for_each_online_cpu(i)
-			cpu_attach_domain(&sched_domain_dummy, i);
+			nicksched_cpu_attach_domain(&sched_domain_dummy, i);
 		arch_destroy_sched_domains();
 		return NOTIFY_OK;
 
@@ -4593,7 +4226,7 @@
 }
 #endif
 
-void __init sched_init_smp(void)
+void __init nicksched_sched_init_smp(void)
 {
 	lock_cpu_hotplug();
 	arch_init_sched_domains();
@@ -4603,21 +4236,48 @@
 	hotcpu_notifier(update_sched_domains, 0);
 }
 #else
-void __init sched_init_smp(void)
+void __init nicksched_sched_init_smp(void)
 {
 }
 #endif /* CONFIG_SMP */
 
-int in_sched_functions(unsigned long addr)
+#if defined(CONFIG_SYSCTL)
+static struct ctl_table_header *nicksched_table_header;
+
+enum
 {
-	/* Linker adds these: start and end of __sched functions */
-	extern char __sched_text_start[], __sched_text_end[];
-	return in_lock_functions(addr) ||
-		(addr >= (unsigned long)__sched_text_start
-		&& addr < (unsigned long)__sched_text_end);
-}
+	NICKSCHED_CPU_SCHED_END_OF_LIST=0,
+	NICKSCHED_CPU_BASE_TIMESLICE=1,
+};
+
+static ctl_table nicksched_cpu_sched_table[] = {
+	{
+		.ctl_name	= NICKSCHED_CPU_BASE_TIMESLICE,
+		.procname	= "base_timeslice",
+		.data		= &nicksched_sched_base_timeslice,
+		.maxlen		= sizeof (int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.strategy	= &sysctl_intvec,
+		.extra1		= &nicksched_sched_min_base,
+		.extra2		= &nicksched_sched_max_base,
+	},
+	{ .ctl_name = NICKSCHED_CPU_SCHED_END_OF_LIST }
+};
 
-void __init sched_init(void)
+static ctl_table nicksched_root_table[] = {
+	{
+		.ctl_name	= CTL_SCHED,
+		.procname	= "sched",
+		.maxlen		= 0,
+		.mode		= 0555,
+		.child		= nicksched_cpu_sched_table,
+	},
+	{ .ctl_name = 0 }
+};
+#endif
+
+void __init nicksched_sched_init(void)
 {
 	runqueue_t *rq;
 	int i, j, k;
@@ -4642,16 +4302,21 @@
 
 		for (j = 0; j < 2; j++) {
 			array = rq->arrays + j;
-			array->min_prio = MAX_PRIO;
-			for (k = 0; k < MAX_PRIO; k++) {
+			array->min_prio = NICK_MAX_PRIO;
+			for (k = 0; k < NICK_MAX_PRIO; k++) {
 				INIT_LIST_HEAD(array->queue + k);
 				__clear_bit(k, array->bitmap);
 			}
 			/* delimiter for bitsearch */
-			__set_bit(MAX_PRIO, array->bitmap);
+			__set_bit(NICK_MAX_PRIO, array->bitmap);
 		}
 	}
 
+	printk("Using %s version %s\n", NICKSCHED_SCHEDULER_NAME, NICKSCHED_SCHEDULER_VERSION);
+#if defined(CONFIG_SYSCTL)
+	nicksched_table_header = register_sysctl_table(nicksched_root_table, 1);
+#endif
+
 	/*
 	 * The boot idle thread does lazy MMU switching as well:
 	 */
@@ -4659,12 +4324,12 @@
 	enter_lazy_tlb(&init_mm, current);
 
 	/*
-	 * Make us the idle thread. Technically, schedule() should not be
+	 * Make us the idle thread. Technically, nicksched_schedule() should not be
 	 * called from this thread, however somewhere below it might be,
 	 * but because we are the idle thread, we just pick up running again
 	 * when this runqueue becomes "idle".
 	 */
-	init_idle(current, smp_processor_id());
+	nicksched_init_idle(current, smp_processor_id());
 }
 
 #ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
@@ -4688,3 +4353,73 @@
 }
 EXPORT_SYMBOL(__might_sleep);
 #endif
+
+scheduler_t sched_nicksched = {
+	.schedule_tail_fn = 			nicksched_schedule_tail,
+	.schedule_fn =				nicksched_schedule,
+	.scheduler_tick_fn = 			nicksched_scheduler_tick,
+	.yield_fn = 				nicksched_yield,
+	.wait_for_completion_fn	=		nicksched_wait_for_completion,
+	.idle_cpu_fn = 				nicksched_idle_cpu,
+	.default_wake_function_fn = 		nicksched_default_wake_function,
+	.__wake_up_fn = 			nicksched___wake_up,
+	.__wake_up_locked_fn = 			nicksched___wake_up_locked,
+	.__wake_up_sync_fn = 			nicksched___wake_up_sync,
+	.complete_fn =				nicksched_complete,
+	.complete_all_fn =			nicksched_complete_all,
+	.interruptible_sleep_on_fn = 		nicksched_interruptible_sleep_on,
+	.interruptible_sleep_on_timeout_fn = 	nicksched_interruptible_sleep_on_timeout,
+	.sleep_on_fn =				nicksched_sleep_on,
+	.sleep_on_timeout_fn = 			nicksched_sleep_on_timeout,
+	.set_user_nice_fn = 			nicksched_set_user_nice,
+	.task_nice_fn = 			nicksched_task_nice,
+	.io_schedule_fn = 			nicksched_io_schedule,
+	.io_schedule_timeout_fn = 		nicksched_io_schedule_timeout,
+	.task_curr_fn = 			nicksched_task_curr,
+	.wake_up_process_fn = 			nicksched_wake_up_process,
+	.wake_up_state_fn = 			nicksched_wake_up_state,
+	.nr_running_fn = 			nicksched_nr_running,
+	.nr_uninterruptible_fn = 		nicksched_nr_uninterruptible,
+	.nr_iowait_fn = 			nicksched_nr_iowait,
+	.nr_context_switches_fn = 		nicksched_nr_context_switches,
+	.sched_exec_fn = 			nicksched_sched_exec,
+	.sched_setaffinity_fn = 		nicksched_sched_setaffinity,
+	.sched_getaffinity_fn = 		nicksched_sched_getaffinity,
+	.sys_nice_fn = 				nicksched_sys_nice,
+	.sys_sched_setscheduler_fn = 		nicksched_sys_sched_setscheduler,
+	.sys_sched_setparam_fn = 		nicksched_sys_sched_setparam,
+	.sys_sched_getscheduler_fn = 		nicksched_sys_sched_getscheduler,
+	.sys_sched_getparam_fn = 		nicksched_sys_sched_getparam,
+	.sys_sched_getaffinity_fn = 		nicksched_sys_sched_getaffinity,
+	.sys_sched_yield_fn = 			nicksched_sys_sched_yield,
+	.sys_sched_rr_get_interval_fn = 	nicksched_sys_sched_rr_get_interval,
+	.sched_init_fn = 			nicksched_sched_init,
+	.sched_init_smp_fn = 			nicksched_sched_init_smp,
+	.migration_init_fn = 			nicksched_migration_init,
+	.sched_fork_fn = 			nicksched_sched_fork,
+	.sched_exit_fn = 			nicksched_sched_exit,
+	.init_idle_fn = 			nicksched_init_idle,
+	.wake_up_new_task_fn = 			nicksched_wake_up_new_task,
+	.task_prio_fn = 			nicksched_task_prio,
+#if defined(CONFIG_SMP)
+	.init_sched_build_groups_fn = 		nicksched_init_sched_build_groups,
+	.cpu_attach_domain_fn = 		nicksched_cpu_attach_domain,
+	.set_cpus_allowed_fn = 			nicksched_set_cpus_allowed,
+	.wait_task_inactive_fn = 		nicksched_wait_task_inactive,
+	.kick_process_fn = 			nicksched_kick_process,
+#endif
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+	.kernel_locked_fn = 			nicksched_kernel_locked,
+	.lock_kernel_fn = 			nicksched_lock_kernel,
+	.unlock_kernel_fn = 			nicksched_unlock_kernel,
+#endif
+#if defined(CONFIG_PREEMPT)
+	.preempt_schedule_fn = 			nicksched_preempt_schedule,
+#endif
+	
+	.name = 				NICKSCHED_SCHEDULER_NAME,
+	.version = 				NICKSCHED_SCHEDULER_VERSION,
+	.type = 				SCHED_NICKSCHED,
+};
+
+EXPORT_SYMBOL(sched_nicksched);
Index: xx-sources/kernel/null-sched.c
===================================================================
--- xx-sources.orig/kernel/null-sched.c	2004-05-31 17:36:38.000000000 -0400
+++ xx-sources/kernel/null-sched.c	2004-10-07 10:02:27.691235312 -0400
@@ -0,0 +1,137 @@
+#include <linux/init.h>
+#include <asm/uaccess.h>
+#include <linux/sched.h>
+
+struct runqueue {
+	spinlock_t lock;
+	unsigned long nr_running;
+	unsigned long nr_switches;
+	unsigned long nr_interruptible;
+	unsigned long long timestamp_last_tick;
+	task_t *curr, *idle;
+	struct mm_struct *prev_mm;
+	unsigned long bitmap[BITS_TO_LONGS(MAX_PRIO+1)];
+	struct list_head queue[MAX_PRIO + 1];
+	atomic_t nr_iowait;
+};
+
+static DEFINE_PER_CPU(struct runqueue, runqueues);
+
+#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
+#define this_rq()		(&__get_cpu_var(runqueues))
+#define task_rq(p)		cpu_rq(task_cpu(p))
+#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
+
+#ifndef prepare_arch_switch
+# define prepare_arch_switch(rq, next)	do { } while (0)
+# define finish_arch_switch(rq, next)	spin_unlock_irq(&(rq)->lock)
+# define task_running(rq, p)		((rq)->curr == (p))
+#endif
+
+static void finish_task_switch(task_t *prev)
+{
+	runqueue_t *rq = this_rq();
+	struct mm_struct *mm = rq->prev_mm;
+	unsigned long prev_task_flags;
+
+	rq->prev_mm = NULL;
+
+	prev_task_flags = prev->flags;
+	finish_arch_switch(rq, prev);
+	if (mm)
+		mmdrop(mm);
+	if (unlikely(prev_task_flags & PF_DEAD))
+		put_task_struct(prev);
+}
+
+asmlinkage void null_schedule_tail(task_t *prev)
+{
+	finish_task_switch(prev);
+	if (current->set_child_tid)
+		put_user(current->pid, current->set_child_tid);
+}
+
+void null_scheduler_tick(int user_ticks, int sys_ticks)
+{
+	runqueue_t *rq = this_rq();
+	rq->timestamp_last_tick = sched_clock();
+}
+
+asmlinkage void __sched null_schedule(void)
+{
+	task_t *prev, *next;
+	runqueue_t *rq;
+	struct list_head *queue;
+	unsigned long long now;
+	int cpu, idx;
+
+need_resched:
+	preempt_disable();
+	prev = current;
+	rq = this_rq();
+
+	now = sched_clock();
+	prev->timestamp = now;
+	spin_lock_irq(&rq->lock);
+
+	cpu = smp_processor_id();
+	if (unlikely(!rq->nr_running)) {
+go_idle:
+		if (!rq->nr_running) {
+			next = rq->idle;
+			if (!rq->nr_running)
+				goto switch_tasks;
+		}
+	} else {
+		if (unlikely(!rq->nr_running))
+			goto go_idle;
+	}
+
+	idx = sched_find_first_bit(rq->bitmap);
+	queue = rq->queue + idx;
+	next = list_entry(queue->next, task_t, run_list);
+
+switch_tasks:
+	prefetch(next);
+	clear_tsk_need_resched(prev);
+
+	prev->timestamp = now;
+	if (likely(prev != next)) {
+		next->timestamp = now;
+		rq->nr_switches++;
+		rq->curr = next;
+	} else
+		spin_unlock_irq(&rq->lock);
+
+	preempt_enable_no_resched();
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+		goto need_resched;
+}
+
+void __devinit null_init_idle(task_t *idle, int cpu)
+{
+	runqueue_t *rq = this_rq();
+	unsigned long flags;
+	idle->staircase_data.prio = MAX_PRIO;
+	idle->state = TASK_RUNNING;
+	set_task_cpu(idle, cpu);
+	spin_lock_irqsave(&rq->lock, flags);
+	rq->curr = rq->idle = idle;
+	set_tsk_need_resched(idle);
+	spin_unlock_irqrestore(&rq->lock, flags);
+}
+
+void __init null_sched_init(void)
+{
+	runqueue_t *rq;
+	int i;
+	rq = this_rq();
+	spin_lock_init(&rq->lock);
+	for (i = 0; i <= MAX_PRIO; i++)
+		INIT_LIST_HEAD(&rq->queue[i]);
+	memset(rq->bitmap, 0, BITS_TO_LONGS(MAX_PRIO+1)*sizeof(long));
+	__set_bit(MAX_PRIO, rq->bitmap);
+	atomic_inc(&init_mm.mm_count);
+	null_init_idle(current, smp_processor_id());
+}
+
Index: xx-sources/kernel/sched.c
===================================================================
--- xx-sources.orig/kernel/sched.c	2004-05-31 17:36:38.000000000 -0400
+++ xx-sources/kernel/sched.c	2004-10-07 10:08:55.261315704 -0400
@@ -0,0 +1,855 @@
+/*
+ *  kernel/sched.c
+ *
+ *  Kernel scheduler and related syscalls
+ *
+ *  Copyright (C) 1991-2002  Linus Torvalds
+ *
+ *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and
+ *		make semaphores SMP safe
+ *  1998-11-19	Implemented schedule_timeout() and related stuff
+ *		by Andrea Arcangeli
+ *  2002-01-04	New ultra-scalable O(1) scheduler by Ingo Molnar:
+ *		hybrid priority-list and round-robin design with
+ *		an array-switch method of distributing timeslices
+ *		and per-CPU runqueues.  Cleanups and useful suggestions
+ *		by Davide Libenzi, preemptible kernel bits by Robert Love.
+ *  2003-09-03	Interactivity tuning by Con Kolivas.
+ *  2004-04-02	Scheduler domains code by Nick Piggin
+ */
+
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/nmi.h>
+#include <linux/init.h>
+#include <asm/uaccess.h>
+#include <linux/highmem.h>
+#include <linux/smp_lock.h>
+#include <asm/mmu_context.h>
+#include <linux/interrupt.h>
+#include <linux/completion.h>
+#include <linux/kernel_stat.h>
+#include <linux/security.h>
+#include <linux/notifier.h>
+#include <linux/profile.h>
+#include <linux/suspend.h>
+#include <linux/blkdev.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/timer.h>
+#include <linux/rcupdate.h>
+#include <linux/cpu.h>
+#include <linux/cpuset.h>
+#include <linux/percpu.h>
+#include <linux/perfctr.h>
+#include <linux/kthread.h>
+#include <linux/seq_file.h>
+#include <linux/syscalls.h>
+#include <linux/times.h>
+
+#include <asm/tlb.h>
+
+#include <asm/unistd.h>
+
+scheduler_t *current_scheduler =
+#if defined(CONFIG_NICKSCHED)
+	&sched_nicksched;
+#elif defined(CONFIG_STAIRCASE)
+	&sched_staircase;
+#elif defined(CONFIG_XSCHED)
+	&sched_xsched;
+#elif defined(CONFIG_SCHED_NONE)
+	&sched_default;
+#else
+	NULL;
+#error "You must have at least 1 process scheduler selected"
+#endif
+
+static int __init scheduler_setup(char *str)
+{
+#if defined(CONFIG_NICKSCHED)
+	if (!strcmp(str, "nicksched")) {
+		init_idle(current, smp_processor_id());
+		current_scheduler = &sched_nicksched;
+		sched_init();
+	}
+#endif
+#if defined(CONFIG_STAIRCASE)
+	if (!strcmp(str, "staircase")) {
+		init_idle(current, smp_processor_id());
+		current_scheduler = &sched_staircase;
+		sched_init();
+	}
+#endif
+#if defined(CONFIG_STAIRCASE)
+	if (!strcmp(str, "xsched")) {
+		init_idle(current, smp_processor_id());
+		current_scheduler = &sched_xsched;
+		sched_init();
+	}
+#endif
+#if defined(CONFIG_SCHED_NONE)
+	if (!strcmp(str, "default")) {
+		init_idle(current, smp_processor_id());
+		current_scheduler = &sched_default;
+		sched_init();
+	}
+#endif
+	return 1;
+}
+
+__setup("scheduler=", scheduler_setup);
+
+
+inline int task_curr(const task_t *p)
+{
+	if (current_scheduler->task_curr_fn)
+		return current_scheduler->task_curr_fn(p);
+	return 0;
+}
+
+inline void wait_task_inactive(task_t * p)
+{
+	if (current_scheduler->wait_task_inactive_fn)
+		current_scheduler->wait_task_inactive_fn(p);
+}
+
+#if defined(CONFIG_SMP)
+inline void kick_process(task_t *p)
+{
+	if (current_scheduler->kick_process_fn)
+		current_scheduler->kick_process_fn(p);
+}
+#endif
+
+int fastcall wake_up_process(task_t * p)
+{
+	if (current_scheduler->wake_up_process_fn)
+		return current_scheduler->wake_up_process_fn(p);
+	return 0;
+}
+
+inline int fastcall wake_up_state(task_t *p, unsigned int state)
+{
+	if (current_scheduler->wake_up_state_fn)
+		return current_scheduler->wake_up_state_fn(p, state);
+	return 0;
+}
+
+inline void fastcall sched_fork(task_t *p)
+{
+	if (current_scheduler->sched_fork_fn)
+		current_scheduler->sched_fork_fn(p);
+}
+
+inline void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
+{
+	if (current_scheduler->wake_up_new_task_fn)
+		current_scheduler->wake_up_new_task_fn(p, clone_flags);
+}
+
+inline void fastcall sched_exit(task_t * p)
+{
+	if (current_scheduler->sched_exit_fn)
+		current_scheduler->sched_exit_fn(p);
+}
+
+void __devinit init_idle(task_t *idle, int cpu)
+{
+	if (current_scheduler->init_idle_fn)
+		current_scheduler->init_idle_fn(idle, cpu);
+}
+
+inline asmlinkage void schedule_tail(task_t *prev)
+{
+	if (current_scheduler->schedule_tail_fn)
+		current_scheduler->schedule_tail_fn(prev);
+}
+
+inline unsigned long nr_running(void)
+{
+	if (current_scheduler->nr_running_fn)
+		return current_scheduler->nr_running_fn();
+	return 0;
+}
+
+inline unsigned long nr_uninterruptible(void)
+{
+	if (current_scheduler->nr_uninterruptible_fn)
+		return current_scheduler->nr_uninterruptible_fn();
+	return 0;
+}
+
+inline unsigned long long nr_context_switches(void)
+{
+	if (current_scheduler->nr_context_switches_fn)
+		return current_scheduler->nr_context_switches_fn();
+	return 0;
+}
+
+inline unsigned long nr_iowait(void)
+{
+	if (current_scheduler->nr_iowait_fn)
+		return current_scheduler->nr_iowait_fn();
+	return 0;
+}
+
+#if defined(CONFIG_SMP)
+inline void sched_exec(void)
+{
+	if (current_scheduler->sched_exec_fn)
+		current_scheduler->sched_exec_fn();
+}
+#endif
+
+inline void scheduler_tick(int user_ticks, int sys_ticks)
+{
+	if (current_scheduler->scheduler_tick_fn)
+		current_scheduler->scheduler_tick_fn(user_ticks, sys_ticks);
+}
+
+
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+
+inline int kernel_locked(void)
+{
+	if (current_scheduler->kernel_locked_fn)
+		return current_scheduler->kernel_locked_fn();
+	return 0;
+}
+
+inline void lock_kernel(void)
+{
+	if (current_scheduler->lock_kernel_fn)
+		current_scheduler->lock_kernel_fn();
+}
+
+inline void unlock_kernel(void)
+{
+	if (current_scheduler->unlock_kernel_fn)
+		current_scheduler->unlock_kernel_fn();
+}
+
+#endif
+
+inline asmlinkage void __sched schedule(void)
+{
+	if (current_scheduler->schedule_fn)
+		current_scheduler->schedule_fn();
+}
+
+#if defined(CONFIG_PREEMPT)
+asmlinkage void __sched preempt_schedule(void)
+{
+	if (current_scheduler->preempt_schedule_fn)
+		current_scheduler->preempt_schedule_fn();
+}
+#endif
+
+int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
+{
+	if (current_scheduler->default_wake_function_fn)
+		return current_scheduler->default_wake_function_fn(curr, mode, sync, key);
+	return 0;
+}
+
+inline void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, void *key)
+{
+	if (current_scheduler->__wake_up_fn)
+		current_scheduler->__wake_up_fn(q, mode, nr_exclusive, key);
+}
+
+inline void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
+{
+	if (current_scheduler->__wake_up_locked_fn)
+		current_scheduler->__wake_up_locked_fn(q, mode);
+}
+
+inline void fastcall __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
+{
+	if (current_scheduler->__wake_up_sync_fn)
+		current_scheduler->__wake_up_sync_fn(q, mode, nr_exclusive);
+}
+
+inline void fastcall complete(struct completion *x)
+{
+	if (current_scheduler->complete_fn)
+		current_scheduler->complete_fn(x);
+}
+
+inline void fastcall complete_all(struct completion *x)
+{
+	if (current_scheduler->complete_all_fn)
+		current_scheduler->complete_all_fn(x);
+}
+
+inline void fastcall wait_for_completion(struct completion *x)
+{
+	if (current_scheduler->wait_for_completion_fn)
+		current_scheduler->wait_for_completion_fn(x);
+}
+
+inline void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+{
+	if (current_scheduler->interruptible_sleep_on_fn)
+		current_scheduler->interruptible_sleep_on_fn(q);
+}
+
+inline long fastcall __sched interruptible_sleep_on_timeout(wait_queue_head_t *q, signed long timeout)
+{
+	if (current_scheduler->interruptible_sleep_on_timeout_fn)
+		return current_scheduler->interruptible_sleep_on_timeout_fn(q, timeout);
+	return 0;
+}
+
+inline void fastcall __sched sleep_on(wait_queue_head_t *q)
+{
+	if (current_scheduler->sleep_on_fn)
+		current_scheduler->sleep_on_fn(q);
+}
+
+inline long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, signed long timeout)
+{
+	if (current_scheduler->sleep_on_timeout_fn)
+		return current_scheduler->sleep_on_timeout_fn(q, timeout);
+	return 0;
+}
+
+inline void set_user_nice(task_t *p, long nice)
+{
+	if (current_scheduler->set_user_nice_fn)
+		current_scheduler->set_user_nice_fn(p, nice);
+}
+
+inline long sched_setaffinity(pid_t pid, cpumask_t new_mask)
+{
+	if (current_scheduler->sched_setaffinity_fn)
+		return current_scheduler->sched_setaffinity_fn(pid, new_mask);
+	return 0;
+}
+
+inline long sched_getaffinity(pid_t pid, cpumask_t *mask)
+{
+	if (current_scheduler->sched_getaffinity_fn)
+		return current_scheduler->sched_getaffinity_fn(pid, mask);
+	return 0;
+}
+
+inline asmlinkage long sys_nice(int increment)
+{
+	if (current_scheduler->sys_nice_fn)
+		return current_scheduler->sys_nice_fn(increment);
+	return 0;
+}
+
+inline int task_prio(const task_t *p)
+{
+	if (current_scheduler->task_prio_fn)
+		return current_scheduler->task_prio_fn(p);
+	return 0;
+}
+
+inline int task_nice(const task_t *p)
+{
+	if (current_scheduler->task_nice_fn)
+		return current_scheduler->task_nice_fn(p);
+	return 0;
+}
+
+inline int idle_cpu(int cpu)
+{
+	if (current_scheduler->idle_cpu_fn)
+		return current_scheduler->idle_cpu_fn(cpu);
+	return 0;
+}
+
+inline asmlinkage long sys_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
+{
+	if (current_scheduler->sys_sched_setscheduler_fn)
+		return current_scheduler->sys_sched_setscheduler_fn(pid, policy, param);
+	return 0;
+}
+
+inline asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
+{
+	if (current_scheduler->sys_sched_setparam_fn)
+		return current_scheduler->sys_sched_setparam_fn(pid, param);
+	return 0;
+}
+
+inline asmlinkage long sys_sched_getscheduler(pid_t pid)
+{
+	if (current_scheduler->sys_sched_getscheduler_fn)
+		return current_scheduler->sys_sched_getscheduler_fn(pid);
+	return 0;
+}
+
+inline asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
+{
+	if (current_scheduler->sys_sched_getparam_fn)
+		return current_scheduler->sys_sched_getparam_fn(pid, param);
+	return 0;
+}
+
+inline asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len, unsigned long __user *user_mask_ptr)
+{
+	if (current_scheduler->sys_sched_getaffinity_fn)
+		return current_scheduler->sys_sched_getaffinity_fn(pid, len, user_mask_ptr);
+	return 0;
+}
+
+inline asmlinkage long sys_sched_yield(void)
+{
+	if (current_scheduler->sys_sched_yield_fn)
+		return current_scheduler->sys_sched_yield_fn();
+	return 0;
+}
+
+inline void __sched yield(void)
+{
+	if (current_scheduler->yield_fn)
+		current_scheduler->yield_fn();
+}
+
+inline void __sched io_schedule(void)
+{
+	if (current_scheduler->io_schedule_fn)
+		current_scheduler->io_schedule_fn();
+}
+
+inline long __sched io_schedule_timeout(long timeout)
+{
+	if (current_scheduler->io_schedule_timeout_fn)
+		return current_scheduler->io_schedule_timeout_fn(timeout);
+	return 0;
+}
+
+inline asmlinkage long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
+{
+	if (current_scheduler->sys_sched_rr_get_interval_fn)
+		return current_scheduler->sys_sched_rr_get_interval_fn(pid, interval);
+	return 0;
+}
+
+#if defined(CONFIG_SMP)
+inline void __devinit cpu_attach_domain(struct sched_domain *sd, int cpu)
+{
+	if (current_scheduler->cpu_attach_domain_fn)
+		current_scheduler->cpu_attach_domain_fn(sd, cpu);
+}
+
+inline int set_cpus_allowed(task_t *p, cpumask_t new_mask)
+{
+	if (current_scheduler->set_cpus_allowed_fn)
+		return current_scheduler->set_cpus_allowed_fn(p, new_mask);
+	return 0;
+}
+#endif
+
+inline int __init migration_init(void)
+{
+	if (current_scheduler->migration_init_fn)
+		return current_scheduler->migration_init_fn();
+	return 0;
+}
+
+void __init sched_init(void)
+{
+	if (current_scheduler->sched_init_fn)
+		current_scheduler->sched_init_fn();
+}
+
+void __init sched_init_smp(void)
+{
+	if (current_scheduler->sched_init_smp_fn)
+		current_scheduler->sched_init_smp_fn();
+}
+
+
+#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
+
+void fastcall add_preempt_count(int val)
+{
+	/*
+	 * Underflow?
+	 */
+	BUG_ON(((int)preempt_count() < 0));
+	preempt_count() += val;
+	/*
+	 * Spinlock count overflowing soon?
+	 */
+	BUG_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
+}
+EXPORT_SYMBOL(add_preempt_count);
+
+void fastcall sub_preempt_count(int val)
+{
+	/*
+	 * Underflow?
+	 */
+	BUG_ON(val > preempt_count());
+	/*
+	 * Is the spinlock portion underflowing?
+	 */
+	BUG_ON((val < PREEMPT_MASK) && !(preempt_count() & PREEMPT_MASK));
+	preempt_count() -= val;
+}
+EXPORT_SYMBOL(sub_preempt_count);
+
+#ifdef __smp_processor_id
+/*
+ * Debugging check.
+ */
+unsigned int smp_processor_id(void)
+{
+	unsigned long preempt_count = preempt_count();
+	int this_cpu = __smp_processor_id();
+	cpumask_t this_mask;
+
+	if (likely(preempt_count))
+		goto out;
+
+	if (irqs_disabled())
+		goto out;
+
+	/*
+	 * Kernel threads bound to a single CPU can safely use
+	 * smp_processor_id():
+	 */
+	this_mask = cpumask_of_cpu(this_cpu);
+
+	if (cpus_equal(current->cpus_allowed, this_mask))
+		goto out;
+
+	/*
+	 * It is valid to assume CPU-locality during early bootup:
+	 */
+	if (system_state != SYSTEM_RUNNING)
+		goto out;
+
+	/*
+	 * Avoid recursion:
+	 */
+	preempt_disable();
+
+	if (!printk_ratelimit())
+		goto out_enable;
+
+	printk(KERN_ERR "using smp_processor_id() in preemptible code: %s/%d\n",
+		current->comm, current->pid);
+	dump_stack();
+
+out_enable:
+	preempt_enable_no_resched();
+out:
+	return this_cpu;
+}
+
+EXPORT_SYMBOL(smp_processor_id);
+
+#endif /* __smp_processor_id */
+
+#endif /* PREEMPT && DEBUG_PREEMPT */
+
+DEFINE_PER_CPU(struct kernel_stat, kstat);
+
+EXPORT_PER_CPU_SYMBOL(kstat);
+
+static inline void __cond_resched(void)
+{
+
+
+	if (preempt_count() & PREEMPT_ACTIVE)
+		return;
+	do {
+		add_preempt_count(PREEMPT_ACTIVE);
+		schedule();
+		sub_preempt_count(PREEMPT_ACTIVE);
+	} while (need_resched());
+}
+
+int __sched cond_resched(void)
+{
+	if (need_resched()) {
+		__cond_resched();
+		return 1;
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL(cond_resched);
+
+/*
+ * cond_resched_lock() - if a reschedule is pending, drop the given lock,
+ * call schedule, and on return reacquire the lock.
+ *
+ * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
+ * operations here to prevent schedule() from being called twice (once via
+ * spin_unlock(), once by hand).
+ */
+int cond_resched_lock(spinlock_t * lock)
+{
+#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT)
+	if (lock->break_lock) {
+		lock->break_lock = 0;
+		spin_unlock(lock);
+		cpu_relax();
+		spin_lock(lock);
+	}
+#endif
+	if (need_resched()) {
+		_raw_spin_unlock(lock);
+		preempt_enable_no_resched();
+		__cond_resched();
+		spin_lock(lock);
+		return 1;
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL(cond_resched_lock);
+
+int __sched cond_resched_softirq(void)
+{
+	BUG_ON(!in_softirq());
+
+	if (need_resched()) {
+		__local_bh_enable();
+		__cond_resched();
+		local_bh_disable();
+		return 1;
+	}
+	return 0;
+}
+
+EXPORT_SYMBOL(cond_resched_softirq);
+
+/**
+ * sys_sched_get_priority_max - return maximum RT priority.
+ * @policy: scheduling class.
+ *
+ * this syscall returns the maximum rt_priority that can be used
+ * by a given scheduling class.
+ */
+asmlinkage long sys_sched_get_priority_max(int policy)
+{
+	int ret = -EINVAL;
+
+	switch (policy) {
+	case SCHED_FIFO:
+	case SCHED_RR:
+		ret = MAX_USER_RT_PRIO-1;
+		break;
+	case SCHED_NORMAL:
+		ret = 0;
+		break;
+	}
+	return ret;
+}
+
+/**
+ * sys_sched_get_priority_min - return minimum RT priority.
+ * @policy: scheduling class.
+ *
+ * this syscall returns the minimum rt_priority that can be used
+ * by a given scheduling class.
+ */
+asmlinkage long sys_sched_get_priority_min(int policy)
+{
+	int ret = -EINVAL;
+
+	switch (policy) {
+	case SCHED_FIFO:
+	case SCHED_RR:
+		ret = 1;
+		break;
+	case SCHED_NORMAL:
+		ret = 0;
+	}
+	return ret;
+}
+
+static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
+			     cpumask_t *new_mask)
+{
+	if (len < sizeof(cpumask_t)) {
+		memset(new_mask, 0, sizeof(cpumask_t));
+	} else if (len > sizeof(cpumask_t)) {
+		len = sizeof(cpumask_t);
+	}
+	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
+}
+
+/**
+ * sys_sched_setaffinity - set the cpu affinity of a process
+ * @pid: pid of the process
+ * @len: length in bytes of the bitmask pointed to by user_mask_ptr
+ * @user_mask_ptr: user-space pointer to the new cpu mask
+ */
+asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
+				      unsigned long __user *user_mask_ptr)
+{
+	cpumask_t new_mask;
+	int retval;
+
+	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
+	if (retval)
+		return retval;
+
+	return sched_setaffinity(pid, new_mask);
+}
+
+static inline struct task_struct *eldest_child(struct task_struct *p)
+{
+	if (list_empty(&p->children)) return NULL;
+	return list_entry(p->children.next,struct task_struct,sibling);
+}
+
+static inline struct task_struct *older_sibling(struct task_struct *p)
+{
+	if (p->sibling.prev==&p->parent->children) return NULL;
+	return list_entry(p->sibling.prev,struct task_struct,sibling);
+}
+
+static inline struct task_struct *younger_sibling(struct task_struct *p)
+{
+	if (p->sibling.next==&p->parent->children) return NULL;
+	return list_entry(p->sibling.next,struct task_struct,sibling);
+}
+
+static void show_task(task_t * p)
+{
+	task_t *relative;
+	unsigned state;
+	unsigned long free = 0;
+	static const char *stat_nam[] = { "R", "S", "D", "T", "t", "Z", "X" };
+
+	printk("%-13.13s ", p->comm);
+	state = p->state ? __ffs(p->state) + 1 : 0;
+	if (state < ARRAY_SIZE(stat_nam))
+		printk(stat_nam[state]);
+	else
+		printk("?");
+#if (BITS_PER_LONG == 32)
+	if (state == TASK_RUNNING)
+		printk(" running ");
+	else
+		printk(" %08lX ", thread_saved_pc(p));
+#else
+	if (state == TASK_RUNNING)
+		printk("  running task   ");
+	else
+		printk(" %016lx ", thread_saved_pc(p));
+#endif
+#ifdef CONFIG_DEBUG_STACK_USAGE
+	{
+		unsigned long * n = (unsigned long *) (p->thread_info+1);
+		while (!*n)
+			n++;
+		free = (unsigned long) n - (unsigned long)(p->thread_info+1);
+	}
+#endif
+	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
+	if ((relative = eldest_child(p)))
+		printk("%5d ", relative->pid);
+	else
+		printk("      ");
+	if ((relative = younger_sibling(p)))
+		printk("%7d", relative->pid);
+	else
+		printk("       ");
+	if ((relative = older_sibling(p)))
+		printk(" %5d", relative->pid);
+	else
+		printk("      ");
+	if (!p->mm)
+		printk(" (L-TLB)\n");
+	else
+		printk(" (NOTLB)\n");
+
+	if (state != TASK_RUNNING)
+		show_stack(p, NULL);
+}
+
+void show_state(void)
+{
+	task_t *g, *p;
+
+#if (BITS_PER_LONG == 32)
+	printk("\n"
+	       "                                               sibling\n");
+	printk("  task             PC      pid father child younger older\n");
+#else
+	printk("\n"
+	       "                                                       sibling\n");
+	printk("  task                 PC          pid father child younger older\n");
+#endif
+	read_lock(&tasklist_lock);
+	do_each_thread(g, p) {
+		/*
+		 * reset the NMI-timeout, listing all files on a slow
+		 * console might take alot of time:
+		 */
+		touch_nmi_watchdog();
+		show_task(p);
+	} while_each_thread(g, p);
+
+	read_unlock(&tasklist_lock);
+}
+
+/*
+ * Represents all cpu's present in the system
+ * In systems capable of hotplug, this map could dynamically grow
+ * as new cpu's are detected in the system via any platform specific
+ * method, such as ACPI for e.g.
+ */
+
+cpumask_t cpu_present_map;
+EXPORT_SYMBOL(cpu_present_map);
+
+#ifndef CONFIG_SMP
+cpumask_t cpu_online_map = CPU_MASK_ALL;
+cpumask_t cpu_possible_map = CPU_MASK_ALL;
+#endif
+
+/*
+ * In a system that switches off the HZ timer nohz_cpu_mask
+ * indicates which cpus entered this state. This is used
+ * in the rcu update to wait only for active cpus. For system
+ * which do not switch off the HZ timer nohz_cpu_mask should
+ * always be CPU_MASK_NONE.
+ */
+cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
+
+int in_sched_functions(unsigned long addr)
+{
+	/* Linker adds these: start and end of __sched functions */
+	extern char __sched_text_start[], __sched_text_end[];
+	return in_lock_functions(addr) ||
+		(addr >= (unsigned long)__sched_text_start
+		&& addr < (unsigned long)__sched_text_end);
+}
+
+
+EXPORT_SYMBOL(default_wake_function);
+EXPORT_SYMBOL(wait_for_completion);
+EXPORT_SYMBOL(set_user_nice);
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+EXPORT_SYMBOL(lock_kernel);
+EXPORT_SYMBOL(unlock_kernel);
+#endif
+EXPORT_SYMBOL(schedule);
+#if defined(CONFIG_PREEMPT)
+EXPORT_SYMBOL(preempt_schedule);
+#endif
+EXPORT_SYMBOL(wake_up_process);
+EXPORT_SYMBOL(__wake_up);
+EXPORT_SYMBOL_GPL(__wake_up_sync);
+EXPORT_SYMBOL(interruptible_sleep_on);
+EXPORT_SYMBOL(interruptible_sleep_on_timeout);
+EXPORT_SYMBOL(complete);
+EXPORT_SYMBOL_GPL(idle_cpu);
+#if defined(CONFIG_SMP)
+EXPORT_SYMBOL_GPL(set_cpus_allowed);
+EXPORT_SYMBOL_GPL(kick_process);
+#endif
Index: xx-sources/kernel/sched/Kconfig
===================================================================
--- xx-sources.orig/kernel/sched/Kconfig	2004-10-07 10:02:06.583444184 -0400
+++ xx-sources/kernel/sched/Kconfig	2004-05-31 17:36:38.000000000 -0400
@@ -1,129 +0,0 @@
-choice
-	prompt "Process scheduling policy"
-	default SCHED_NONE
-	depends on EXPERIMENTAL
-	help
-	  Different people have written alternate implementations of
-	  the kernel's scheduler code.  Each implementation is different
-	  and has its own pros and cons.  This will allow you to choose
-	  between different scheduling policies.
-
-config SCHED_NONE
-	bool "Default"
-	help
-	  This is the default scheduler as is included in the -mm kernels.
-	  It contains the sched domains code by Nick Piggin and some tweaks
-	  to the scheduling code, but no significant changes.
-
-config NICKSCHED
-	bool "Nicksched"
-	help
-	  This is a scheduler written by Nick Piggin.  It is quite fast,
-	  responsive, and does well under heavy loads.
-
-	  In this scheduler, the architecture is still pretty similar
-	  to the original scheduler - there are still two priority
-	  arrays, for example.  The differences are in how the bonuses
- 	  are given.
-
-	  In Nicksched, a task's priority and bonuses are based on its
-	  "sleep time."  The scheduler keeps track of a task's history -
-	  how long the task was running.  Each task is also assigned one
-	  of three running modes: sleeping (not active), running (using
-	  the CPU), and waiting (waiting for CPU time).  Tasks are given
-	  priority bonuses based on these two factors.  This is a lot
-	  simpler than the original interactivity/credit model, since
-	  it mostly uses linear functions and bit shifts to keep
-	  calculations simple and quick.  And with this method, it
-	  is a lot easier to scale timeslices - for example, the
-	  timeslice given is scaled against the priority of the other
-	  tasks, so a lower priority process can still get larger
-	  timeslices if there aren't higher priority processes using
-	  the CPU.
-
-config SPA
-	bool "SPA-Zaphod"
-	help
-	  SPA was written by Peter Williams.
-	
-	  SPA stands for Single Priority Array, which is the key
-	  architectural difference between SPA and the original scheduler.
-	  Traditionally, the original scheduler uses two priority arrays
-	  to manage running tasks: an active and an expired array.
-	  Tasks queue in the active array, and when they use their timeslice,
-	  they are queued into the expired array.  And when they are given
-	  a new slice, they are requeued back into the active array to be
-	  run again.
-
-	  While this kind of structure is good for scalability, unfortunately
-	  it suffers from a couple weaknesses on an interactivity-based desktop
-	  system.  Having two priority arrays is unneccessary overhead when
-	  only one is really needed.  Also, it is possible to run into queueing
- 	  race conditions under certain circumstances.
- 
-	  SPA changes the system to a single priority array, so the task just
-	  uses its timeslice and gets requeued back into the same array.  That
-	  way, we don't waste time doing extra queues into a different expired
-	  array, and then back again.
-
-	  This is the 'Zaphod' variation of SPA, which allows runtime switching
-	  between two bonus calculation methods: priority-based, and
-	  entitlement-based.  These can be echoed into the proc filesystem as
-	  'pb' and 'eb', respectively.
-
-	  Priority-based bonus calculation is the original calcuation that SPA
-	  has used since its inception.  Its system is still relatively similar
-	  to the default scheduler's: it will vary a task's priority according to
-	  two criteria: interactivity and throughput.  A task is interactive if it
-	  spends the majority of its time sleeping (that is, waiting for user input),
-	  and it is throughput-heavy if it is hogging the cpu most of the time.
-	  Interactive tasks are promoted, and cpu hogs are punished.  The system
-	  keeps track of various aspects of a task's execution time, and uses
-	  some miniature Kalman filters to estimate the actual cpu usage vs
-	  running time.
-
-	  Entitlement-based bonuses, on the other hand, are a more radical departure
-	  from the original priority calculation.  It is a throwback to the original
-	  Entitlement Based Scheduler (EBS), which Peter Williams also contributed to.
-	  With entitlement, a task has a certain amount of 'shares.'  This ranges from
-	  0-420.  The amount of shares a task has determines how much CPU time it will
-	  get, and the rate at which it will get that cpu.  While it doesn't sound
-	  much different from the traditional priority-based system, entitlement actually
-	  completely discards the idea of priority altogether and varies a task's shares
-	  according to quite a few circumstances, making it quite difficult to implement.
-
-config STAIRCASE
-	bool "Staircase"
-	help
-	  Staircase was written by Con Kolivas.
-
-	  The staircase scheduler operates on a similar principle to the
-	  SPA scheduler.  Like SPA, it has only one priority array that
-	  tasks will remain in.  The difference is in how the "bonuses"
-	  are calculated.  Every task starts with a certain "deadline,"
-	  or "burst" as it's called in newer versions.  The deadline
-	  is used to calculate how large a timeslice the task will get.
-	  A task will be first activated with a relatively high deadline,
-	  and therefore get large timeslices.  However, each time it uses
-	  its timeslice and is requeued, its deadline is decreased.  So
-	  on the next run, it will get a smaller timeslice than before.
-	  And likewise, its timeslice will keep "stepping down" each
-	  requeue - like a staircase.  And, of course, there are other
-	  factors used in calculation.  For example, the actual timeslice
-	  size is scaled according to priority.  Also, the task has a
-	  maximum deadline based on its priority.  So a task with a certain
-	  priority will only be able to go so high on the staircase.
-	  Another task with a higher priority will also have a limit on the
-	  staircase, but its best deadline will be higher than the other
-	  task's.  Tasks will also regain deadline due to bonuses.
-
-config XSCHED
-	bool "Xsched"
-	help
-	  This is a rework of the scheduler by xiphux.  At the moment, not
-	  very much of it is original code.  It's pretty much the prio-slot
-	  based structure from SPA by Peter Williams, with the priority
-	  bonus algorithms from Nicksched by Nick Piggin.  It's still
-	  extremely experimental.
-
-endchoice
Index: xx-sources/kernel/spa-sched.c
===================================================================
--- xx-sources.orig/kernel/spa-sched.c	2004-10-07 10:02:06.593442664 -0400
+++ xx-sources/kernel/spa-sched.c	2004-10-07 10:02:27.703233488 -0400
@@ -77,7 +77,7 @@
  */
 #define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
 #define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
-#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->spa_data.static_prio)
 
 /*
  * 'User priority' is the nice value converted to something we
@@ -85,7 +85,7 @@
  * it's a [ 0 ... 39 ] range.
  */
 #define USER_PRIO(p)		((p)-MAX_RT_PRIO)
-#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
+#define TASK_USER_PRIO(p)	USER_PRIO((p)->spa_data.static_prio)
 #define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
 
 /*
@@ -310,7 +310,7 @@
 
 static inline int is_bgnd_task(const task_t *p)
 {
-	return p->cpu_rate_cap == 0;
+	return p->spa_data.cpu_rate_cap == 0;
 }
 
 static inline unsigned int task_timeslice(const task_t *p)
@@ -324,7 +324,7 @@
 	return time_slice_ticks;
 }
 
-#define task_hot(p, sd) ((p)->rq->timestamp_last_tick - (p)->timestamp < (sd)->cache_hot_time)
+#define task_hot(p, sd) ((p)->spa_data.rq->timestamp_last_tick - (p)->timestamp < (sd)->cache_hot_time)
 
 /*
  * These are the runqueue data structures:
@@ -450,7 +450,7 @@
 #define this_rq()		(&__get_cpu_var(runqueues))
 #define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
 
-#define is_idle_task(p) ((p) == (p)->rq->idle)
+#define is_idle_task(p) ((p) == (p)->spa_data.rq->idle)
 
 #ifdef CONFIG_SMP
 void set_task_cpu(struct task_struct *p, unsigned int cpu)
@@ -458,7 +458,7 @@
 	BUG_ON(!list_empty(&p->run_list));
 
 	p->thread_info->cpu = cpu;
-	p->rq = cpu_rq(cpu);
+	p->spa_data.rq = cpu_rq(cpu);
 }
 
 /*
@@ -466,7 +466,7 @@
  */
 static inline void adjust_timestamp(task_t *p, const runqueue_t *oldrq)
 {
-	p->timestamp += (p->rq->timestamp_last_tick - oldrq->timestamp_last_tick);
+	p->timestamp += (p->spa_data.rq->timestamp_last_tick - oldrq->timestamp_last_tick);
 }
 
 /* 
@@ -486,7 +486,7 @@
 		while (unlikely(oldrq_tlt != oldrq->timestamp_last_tick))
 			oldrq_tlt = oldrq->timestamp_last_tick;
 
-	p->sched_timestamp += p->rq->timestamp_last_tick - oldrq_tlt;
+	p->spa_data.sched_timestamp += p->spa_data.rq->timestamp_last_tick - oldrq_tlt;
 }
 
 /*
@@ -501,7 +501,7 @@
 {
 	runqueue_t *trq = this_rq();
 
-	return sched_clock() + (p->rq->timestamp_last_tick - trq->timestamp_last_tick);
+	return sched_clock() + (p->spa_data.rq->timestamp_last_tick - trq->timestamp_last_tick);
 }
 
 #else
@@ -516,9 +516,9 @@
 #ifndef prepare_arch_switch
 # define prepare_arch_switch(rq, next)	do { } while (0)
 # define finish_arch_switch(rq, next)	spin_unlock_irq(&(rq)->lock)
-# define task_is_running(p)		((p)->rq->curr == (p))
+# define task_is_running(p)		((p)->spa_data.rq->curr == (p))
 #else
-# define task_is_running(p) task_running((p)->rq, p)
+# define task_is_running(p) task_running((p)->spa_data.rq, p)
 #endif
 #define task_is_exiting(p) (unlikely(((p)->flags & PF_EXITING) != 0))
 #define task_is_sinbinned(p) (unlikely(((p)->flags & PF_SINBINNED) != 0))
@@ -542,12 +542,12 @@
 
 	rq->eb_yardstick = map_proportion(decay_per_interval, rq->eb_yardstick);
 	rq->eb_ticks_to_decay = time_slice_ticks;
-	if (unlikely(rt_task(rq->curr) || is_bgnd_task(rq->curr)))
+	if (unlikely(spa_rt_task(rq->curr) || is_bgnd_task(rq->curr)))
 		return;
-	if (rq->curr->cpu_usage_rate < rq->curr->cpu_rate_cap)
-		pny = sched_div_64(rq->curr->cpu_usage_rate, rq->curr->eb_shares);
+	if (rq->curr->spa_data.cpu_usage_rate < rq->curr->spa_data.cpu_rate_cap)
+		pny = sched_div_64(rq->curr->spa_data.cpu_usage_rate, rq->curr->spa_data.eb_shares);
 	else
-		pny = sched_div_64(rq->curr->cpu_rate_cap, rq->curr->eb_shares);
+		pny = sched_div_64(rq->curr->spa_data.cpu_rate_cap, rq->curr->spa_data.eb_shares);
 	if (pny > rq->eb_yardstick)
 		rq->eb_yardstick = pny;
 }
@@ -563,9 +563,9 @@
 
 repeat_lock_task:
 	local_irq_save(*flags);
-	rql = &p->rq->lock;
+	rql = &p->spa_data.rq->lock;
 	spin_lock(rql);
-	if (unlikely(rql != &p->rq->lock)) {
+	if (unlikely(rql != &p->spa_data.rq->lock)) {
 		spin_unlock_irqrestore(rql, *flags);
 		goto repeat_lock_task;
 	}
@@ -727,11 +727,11 @@
 	t->sched_info.last_arrival = now;
 	t->sched_info.pcnt++;
 
-	if (!t->rq)
+	if (!t->spa_data.rq)
 		return;
 
-	t->rq->rq_sched_info.run_delay += diff;
-	t->rq->rq_sched_info.pcnt++;
+	t->spa_data.rq->rq_sched_info.run_delay += diff;
+	t->spa_data.rq->rq_sched_info.pcnt++;
 }
 
 /*
@@ -765,8 +765,8 @@
 
 	t->sched_info.cpu_time += diff;
 
-	if (t->rq)
-		t->rq->rq_sched_info.cpu_time += diff;
+	if (t->spa_data.rq)
+		t->spa_data.rq->rq_sched_info.cpu_time += diff;
 }
 
 /*
@@ -776,7 +776,7 @@
  */
 static inline void sched_info_switch(task_t *prev, task_t *next)
 {
-	struct runqueue *rq = prev->rq;
+	struct runqueue *rq = prev->spa_data.rq;
 
 	/*
 	 * prev now departs the cpu.  It's not interesting to record
@@ -796,7 +796,7 @@
 
 static inline int task_preempts_curr(const struct task_struct *p)
 {
-	return (p->prio < p->rq->curr->prio) && !task_is_exiting(p->rq->curr);
+	return (p->spa_data.prio < p->spa_data.rq->curr->spa_data.prio) && !task_is_exiting(p->spa_data.rq->curr);
 }
 
 static inline int task_queued(const task_t *task)
@@ -823,14 +823,14 @@
 	 */
 	list_del_init(&p->run_list);
 	if (list_empty(slotp))
-		__clear_bit(list_entry(slotp, struct prio_slot, queue)->prio, p->rq->bitmap);
+		__clear_bit(list_entry(slotp, struct prio_slot, queue)->prio, p->spa_data.rq->bitmap);
 }
 
 static void enqueue_task(struct task_struct *p)
 {
 	sched_info_queued(p);
-	list_add_tail(&p->run_list, &p->rq->queues[p->prio].queue);
-	__set_bit(p->prio, p->rq->bitmap);
+	list_add_tail(&p->run_list, &p->spa_data.rq->queues[p->spa_data.prio].queue);
+	__set_bit(p->spa_data.prio, p->spa_data.rq->bitmap);
 }
 
 /*
@@ -840,8 +840,8 @@
  */
 static inline void enqueue_task_head(struct task_struct *p)
 {
-	list_add(&p->run_list, &p->rq->queues[p->prio].queue);
-	__set_bit(p->prio, p->rq->bitmap);
+	list_add(&p->run_list, &p->spa_data.rq->queues[p->spa_data.prio].queue);
+	__set_bit(p->spa_data.prio, p->spa_data.rq->bitmap);
 }
 
 /*
@@ -852,13 +852,13 @@
 {
 	unsigned int bonus_factor = 0;
 
-	if (rt_task(p))
-		return p->prio;
+	if (spa_rt_task(p))
+		return p->spa_data.prio;
 
 	if (unlikely(is_bgnd_task(p) && !(p->flags & PF_UISLEEP)))
 		return BGND_PRIO;
 
-	if (task_is_unpriv_rt(p) && (p->cpu_usage_rate < unpriv_rt_threshold))
+	if (task_is_unpriv_rt(p) && (p->spa_data.cpu_usage_rate < unpriv_rt_threshold))
 		return MAX_RT_PRIO;
 
 	/*
@@ -867,12 +867,12 @@
 	 */
 	if (p->mm == NULL)
 		bonus_factor = MAX_TOTAL_BONUS;
-	else if (p->cpu_usage_rate < p->cpu_rate_cap) {
-		bonus_factor = SCHED_IA_BONUS_RND(p->interactive_bonus);
-		bonus_factor += p->throughput_bonus;
+	else if (p->spa_data.cpu_usage_rate < p->spa_data.cpu_rate_cap) {
+		bonus_factor = SCHED_IA_BONUS_RND(p->spa_data.interactive_bonus);
+		bonus_factor += p->spa_data.throughput_bonus;
 	}
 
-	return p->pre_bonus_priority - bonus_factor;
+	return p->spa_data.pre_bonus_priority - bonus_factor;
 }
 
 /*
@@ -881,9 +881,9 @@
 static inline void __activate_task(task_t *p)
 {
 	enqueue_task(p);
-	p->rq->nr_running++;
-	if (p->rq->nr_running == 2)
-		restart_promotions(p->rq);
+	p->spa_data.rq->nr_running++;
+	if (p->spa_data.rq->nr_running == 2)
+		restart_promotions(p->spa_data.rq);
 }
 
 /*
@@ -892,9 +892,9 @@
 static inline void __activate_task_head(task_t *p)
 {
 	enqueue_task_head(p);
-	p->rq->nr_running++;
-	if (p->rq->nr_running == 2)
-		restart_promotions(p->rq);
+	p->spa_data.rq->nr_running++;
+	if (p->spa_data.rq->nr_running == 2)
+		restart_promotions(p->spa_data.rq);
 }
 
 /*
@@ -905,20 +905,20 @@
 {
 	unsigned long long bl;
 
-	apply_sched_avg_decay(&p->avg_sleep_per_cycle);
-	apply_sched_avg_decay(&p->avg_delay_per_cycle);
-	apply_sched_avg_decay(&p->avg_cpu_per_cycle);
-	bl  = p->avg_sleep_per_cycle + p->avg_cpu_per_cycle;
+	apply_sched_avg_decay(&p->spa_data.avg_sleep_per_cycle);
+	apply_sched_avg_decay(&p->spa_data.avg_delay_per_cycle);
+	apply_sched_avg_decay(&p->spa_data.avg_cpu_per_cycle);
+	bl  = p->spa_data.avg_sleep_per_cycle + p->spa_data.avg_cpu_per_cycle;
 	/*
 	 * Take a shortcut and avoid possible divide by zero later
 	 */
 	if (unlikely(bl == 0)) {
-		p->sleepiness = PROPORTION_ONE;
-		p->cpu_usage_rate = 0;
+		p->spa_data.sleepiness = PROPORTION_ONE;
+		p->spa_data.cpu_usage_rate = 0;
 	} else {
-		p->sleepiness = calc_proportion(p->avg_sleep_per_cycle, bl);
-		bl += p->avg_delay_per_cycle;
-		p->cpu_usage_rate = calc_proportion(p->avg_cpu_per_cycle, bl);
+		p->spa_data.sleepiness = calc_proportion(p->spa_data.avg_sleep_per_cycle, bl);
+		bl += p->spa_data.avg_delay_per_cycle;
+		p->spa_data.cpu_usage_rate = calc_proportion(p->spa_data.avg_cpu_per_cycle, bl);
 	}
 }
 
@@ -928,16 +928,16 @@
  */
 static inline void calculate_pb_pre_bonus_priority(task_t *p)
 {
-	if (unlikely(p->cpu_usage_rate > p->cpu_rate_cap)) {
-		p->pre_bonus_priority = BGND_PRIO - 1;
-		if (p->cpu_rate_cap != 0) {
+	if (unlikely(p->spa_data.cpu_usage_rate > p->spa_data.cpu_rate_cap)) {
+		p->spa_data.pre_bonus_priority = BGND_PRIO - 1;
+		if (p->spa_data.cpu_rate_cap != 0) {
 			unsigned long long prop = PROPORTION_ONE;
 
-			prop -= calc_proportion(p->cpu_rate_cap, p->cpu_usage_rate);
-			p->pre_bonus_priority -= map_proportion(prop, MAX_PRIO - p->static_prio);
+			prop -= calc_proportion(p->spa_data.cpu_rate_cap, p->spa_data.cpu_usage_rate);
+			p->spa_data.pre_bonus_priority -= map_proportion(prop, MAX_PRIO - p->spa_data.static_prio);
 		}	
 	} else
-		p->pre_bonus_priority = p->static_prio + MAX_TOTAL_BONUS;
+		p->spa_data.pre_bonus_priority = p->spa_data.static_prio + MAX_TOTAL_BONUS;
 }
 
 /*
@@ -950,28 +950,28 @@
 	/*
 	 * Prevent possible divide by zero and take shortcut
 	 */
-	if (unlikely(p->cpu_rate_cap == 0)) {
-		p->pre_bonus_priority = BGND_PRIO - 1;
-	} else if (p->cpu_usage_rate > p->cpu_rate_cap) {
-		unsigned long long cap_per_share = sched_div_64(p->cpu_rate_cap, p->eb_shares);
-		unsigned long long prop = calc_proportion(p->cpu_rate_cap, p->cpu_usage_rate);
-
-		p->pre_bonus_priority = (BGND_PRIO - 1);
-		p->pre_bonus_priority -= map_proportion_rnd(prop, EB_PAR + 1);
-		if (cap_per_share > p->rq->eb_yardstick)
-			p->rq->eb_yardstick = cap_per_share;
+	if (unlikely(p->spa_data.cpu_rate_cap == 0)) {
+		p->spa_data.pre_bonus_priority = BGND_PRIO - 1;
+	} else if (p->spa_data.cpu_usage_rate > p->spa_data.cpu_rate_cap) {
+		unsigned long long cap_per_share = sched_div_64(p->spa_data.cpu_rate_cap, p->spa_data.eb_shares);
+		unsigned long long prop = calc_proportion(p->spa_data.cpu_rate_cap, p->spa_data.cpu_usage_rate);
+
+		p->spa_data.pre_bonus_priority = (BGND_PRIO - 1);
+		p->spa_data.pre_bonus_priority -= map_proportion_rnd(prop, EB_PAR + 1);
+		if (cap_per_share > p->spa_data.rq->eb_yardstick)
+			p->spa_data.rq->eb_yardstick = cap_per_share;
 	} else {
-		unsigned long long usage_per_share = sched_div_64(p->cpu_usage_rate, p->eb_shares);
+		unsigned long long usage_per_share = sched_div_64(p->spa_data.cpu_usage_rate, p->spa_data.eb_shares);
 
-		if (usage_per_share > p->rq->eb_yardstick) {
-			p->rq->eb_yardstick = usage_per_share;
-			p->pre_bonus_priority = MAX_RT_PRIO + MAX_TOTAL_BONUS + EB_PAR;
+		if (usage_per_share > p->spa_data.rq->eb_yardstick) {
+			p->spa_data.rq->eb_yardstick = usage_per_share;
+			p->spa_data.pre_bonus_priority = MAX_RT_PRIO + MAX_TOTAL_BONUS + EB_PAR;
 		} else {
 			unsigned long long prop;
 
-			prop = calc_proportion(usage_per_share, p->rq->eb_yardstick);
-			p->pre_bonus_priority = MAX_RT_PRIO + MAX_TOTAL_BONUS;
-			p->pre_bonus_priority += map_proportion_rnd(prop, EB_PAR);
+			prop = calc_proportion(usage_per_share, p->spa_data.rq->eb_yardstick);
+			p->spa_data.pre_bonus_priority = MAX_RT_PRIO + MAX_TOTAL_BONUS;
+			p->spa_data.pre_bonus_priority += map_proportion_rnd(prop, EB_PAR);
 		}
 	}
 }
@@ -989,16 +989,16 @@
  */
 static inline void initialize_stats(task_t *p)
 {
-	p->avg_sleep_per_cycle = 0;
-	p->avg_delay_per_cycle = 0;
-	p->avg_cpu_per_cycle = 0;
-	p->total_sleep = 0;
-	p->total_delay = 0;
-	p->total_cpu = 0;
-	p->total_sinbin = 0;
-	p->cycle_count = 0;
-	p->intr_wake_ups = 0;
-	p->sched_timestamp = sched_clock();
+	p->spa_data.avg_sleep_per_cycle = 0;
+	p->spa_data.avg_delay_per_cycle = 0;
+	p->spa_data.avg_cpu_per_cycle = 0;
+	p->spa_data.total_sleep = 0;
+	p->spa_data.total_delay = 0;
+	p->spa_data.total_cpu = 0;
+	p->spa_data.total_sinbin = 0;
+	p->spa_data.cycle_count = 0;
+	p->spa_data.intr_wake_ups = 0;
+	p->spa_data.sched_timestamp = sched_clock();
 }
 
 /*
@@ -1014,15 +1014,15 @@
 	unsigned long long delta;
 
 	/* sched_clock() is not guaranteed monotonic */
-	if (now <= p->sched_timestamp) {
-		p->sched_timestamp = now;
+	if (now <= p->spa_data.sched_timestamp) {
+		p->spa_data.sched_timestamp = now;
 		return;
 	}
 
-	delta = now - p->sched_timestamp;
-	p->sched_timestamp = now;
-	p->avg_sleep_per_cycle += delta;
-	p->total_sleep += delta;
+	delta = now - p->spa_data.sched_timestamp;
+	p->spa_data.sched_timestamp = now;
+	p->spa_data.avg_sleep_per_cycle += delta;
+	p->spa_data.total_sleep += delta;
 }
 
 static inline void delta_cpu_stats(task_t *p, unsigned long long now)
@@ -1030,15 +1030,15 @@
 	unsigned long long delta;
 
 	/* sched_clock() is not guaranteed monotonic */
-	if (now <= p->sched_timestamp) {
-		p->sched_timestamp = now;
+	if (now <= p->spa_data.sched_timestamp) {
+		p->spa_data.sched_timestamp = now;
 		return;
 	}
 
-	delta = now - p->sched_timestamp;
-	p->sched_timestamp = now;
-	p->avg_cpu_per_cycle += delta;
-	p->total_cpu += delta;
+	delta = now - p->spa_data.sched_timestamp;
+	p->spa_data.sched_timestamp = now;
+	p->spa_data.avg_cpu_per_cycle += delta;
+	p->spa_data.total_cpu += delta;
 }
 
 static inline void delta_delay_stats(task_t *p, unsigned long long now)
@@ -1046,19 +1046,19 @@
 	unsigned long long delta;
 
 	/* sched_clock() is not guaranteed monotonic */
-	if (now <= p->sched_timestamp) {
-		p->sched_timestamp = now;
+	if (now <= p->spa_data.sched_timestamp) {
+		p->spa_data.sched_timestamp = now;
 		return;
 	}
 
-	delta = now - p->sched_timestamp;
-	p->sched_timestamp = now;
-	p->avg_delay_per_cycle += delta;
-	p->total_delay += delta;
-	p->rq->total_delay += delta;
+	delta = now - p->spa_data.sched_timestamp;
+	p->spa_data.sched_timestamp = now;
+	p->spa_data.avg_delay_per_cycle += delta;
+	p->spa_data.total_delay += delta;
+	p->spa_data.rq->total_delay += delta;
 	if (task_is_sinbinned(p)) {
-		p->total_sinbin += delta;
-		p->rq->total_sinbin += delta;
+		p->spa_data.total_sinbin += delta;
+		p->spa_data.rq->total_sinbin += delta;
 	}
 }
 
@@ -1074,16 +1074,16 @@
 
 	delta_sleep_stats(p, now);
 	if (in_interrupt())
-		p->intr_wake_ups++;
-	p->cycle_count++;
-	if (!rt_task(p))
+		p->spa_data.intr_wake_ups++;
+	p->spa_data.cycle_count++;
+	if (!spa_rt_task(p))
 		decay_avgs_and_calculate_rates(p);
 }
 
 static inline void decay_sched_ia_bonus(struct task_struct *p)
 {
-	p->interactive_bonus *= SCHED_IA_BONUS_ALPHA;
-	p->interactive_bonus >>= SCHED_IA_BONUS_OFFSET;
+	p->spa_data.interactive_bonus *= SCHED_IA_BONUS_ALPHA;
+	p->spa_data.interactive_bonus >>= SCHED_IA_BONUS_OFFSET;
 }
 
 /*
@@ -1094,16 +1094,16 @@
 static void reassess_cpu_boundness(task_t *p)
 {
 	if (max_ia_bonus == 0) {
-		p->interactive_bonus = 0;
+		p->spa_data.interactive_bonus = 0;
 		return;
 	}
 	/*
 	 * No point going any further if there's no bonus to lose
 	 */
-	if (p->interactive_bonus == 0)
+	if (p->spa_data.interactive_bonus == 0)
 		return;
 
-	if (p->cpu_usage_rate > cpu_hog_threshold)
+	if (p->spa_data.cpu_usage_rate > cpu_hog_threshold)
 		decay_sched_ia_bonus(p);
 }
 
@@ -1115,23 +1115,23 @@
 static void reassess_interactiveness(task_t *p)
 {
 	if (max_ia_bonus == 0) {
-		p->interactive_bonus = 0;
+		p->spa_data.interactive_bonus = 0;
 		return;
 	}
 	/*
 	 * No sleep means not interactive (in most cases), but
 	 */
-	if (unlikely(p->avg_sleep_per_cycle > LOWER_MAX_IA_SLEEP)) {
+	if (unlikely(p->spa_data.avg_sleep_per_cycle > LOWER_MAX_IA_SLEEP)) {
 		/*
 		 * Really long sleeps mean it's probably not interactive
 		 */
-		if (unlikely(p->avg_sleep_per_cycle > UPPER_MAX_IA_SLEEP))
+		if (unlikely(p->spa_data.avg_sleep_per_cycle > UPPER_MAX_IA_SLEEP))
 			decay_sched_ia_bonus(p);
 		return;
 	}
-	if (p->sleepiness > ia_threshold) {
+	if (p->spa_data.sleepiness > ia_threshold) {
 		decay_sched_ia_bonus(p);
-		p->interactive_bonus += map_proportion_rnd(p->sleepiness, max_ia_bonus);
+		p->spa_data.interactive_bonus += map_proportion_rnd(p->spa_data.sleepiness, max_ia_bonus);
 	}
 }
 
@@ -1145,29 +1145,29 @@
 	unsigned long long ratio;
 	unsigned long long expected_delay;
 	unsigned long long adjusted_delay;
-	unsigned long long load = p->rq->avg_nr_running;
+	unsigned long long load = p->spa_data.rq->avg_nr_running;
 
-	p->throughput_bonus = 0;
+	p->spa_data.throughput_bonus = 0;
 	if (max_tpt_bonus == 0)
 		return;
 
 	if (load <= SCHED_AVG_ONE)
 		expected_delay = 0;
 	else
-		expected_delay = SCHED_AVG_MUL(p->avg_cpu_per_cycle, (load - SCHED_AVG_ONE));
+		expected_delay = SCHED_AVG_MUL(p->spa_data.avg_cpu_per_cycle, (load - SCHED_AVG_ONE));
 
 	/*
 	 * No unexpected delay means no bonus, but
 	 * NB this test also avoids a possible divide by zero error if
 	 * cpu is also zero and negative bonuses
 	 */
-	if (p->avg_delay_per_cycle <= expected_delay)
+	if (p->spa_data.avg_delay_per_cycle <= expected_delay)
 		return;
 
-	adjusted_delay  = p->avg_delay_per_cycle - expected_delay;
-	ratio = calc_proportion(adjusted_delay, adjusted_delay + p->avg_cpu_per_cycle);
+	adjusted_delay  = p->spa_data.avg_delay_per_cycle - expected_delay;
+	ratio = calc_proportion(adjusted_delay, adjusted_delay + p->spa_data.avg_cpu_per_cycle);
 	ratio = proportion_sqrt(ratio);
-	p->throughput_bonus = map_proportion_rnd(ratio, max_tpt_bonus);
+	p->spa_data.throughput_bonus = map_proportion_rnd(ratio, max_tpt_bonus);
 }
 
 static void recalc_task_prio(task_t *p, unsigned long long now)
@@ -1177,11 +1177,11 @@
 	 * catch any CPU changes
 	 * Interactive bonus is updated in the wake up function.
 	 */
-	if (!rt_task(p)) {
+	if (!spa_rt_task(p)) {
 		recalc_throughput_bonus(p);
 		calculate_pre_bonus_priority(p);
 	}
-	p->prio = effective_prio(p);
+	p->spa_data.prio = effective_prio(p);
 }
 
 /*
@@ -1194,7 +1194,7 @@
 
 	recalc_task_prio(p, now);
 	p->timestamp = now;
-	p->time_slice = task_timeslice(p);
+	p->spa_data.time_slice = task_timeslice(p);
 	p->flags &= ~PF_UISLEEP;
 
 	__activate_task(p);
@@ -1205,14 +1205,14 @@
  */
 static void deactivate_task(struct task_struct *p)
 {
-	p->rq->nr_running--;
+	p->spa_data.rq->nr_running--;
 	if (p->state == TASK_UNINTERRUPTIBLE) {
 		p->flags |= PF_UISLEEP;
-		p->rq->nr_uninterruptible++;
+		p->spa_data.rq->nr_uninterruptible++;
 	}
 	dequeue_task(p);
-	if (p->rq->nr_running == 1)
-		stop_promotions(p->rq);
+	if (p->spa_data.rq->nr_running == 1)
+		stop_promotions(p->spa_data.rq);
 }
 
 /*
@@ -1222,15 +1222,15 @@
 {
 	unsigned long long acpc_jiffies, bl, tl;
 
-	if (p->cpu_rate_hard_cap == 0)
+	if (p->spa_data.cpu_rate_hard_cap == 0)
 		return ULONG_MAX;
 
-	acpc_jiffies = sched_div_64(SCHED_AVG_RND(p->avg_cpu_per_cycle) * HZ, 1000000000);
+	acpc_jiffies = sched_div_64(SCHED_AVG_RND(p->spa_data.avg_cpu_per_cycle) * HZ, 1000000000);
 	/*
 	 * we have to be careful about overflow and/or underflow
 	 */
-	bl = p->cpu_usage_rate * p->cpu_rate_hard_cap;
-	tl = acpc_jiffies * (p->cpu_usage_rate - p->cpu_rate_hard_cap);
+	bl = p->spa_data.cpu_usage_rate * p->spa_data.cpu_rate_hard_cap;
+	tl = acpc_jiffies * (p->spa_data.cpu_usage_rate - p->spa_data.cpu_rate_hard_cap);
 	while (tl > PROPORTION_OVERFLOW) {
 		tl >>= 1;
 		if (unlikely((bl >>= 1) == 0))
@@ -1242,8 +1242,8 @@
 
 static inline int task_needs_sinbinning(const struct task_struct *p)
 {
-	return (p->cpu_usage_rate > p->cpu_rate_hard_cap) &&
-		(p->state == TASK_RUNNING) && !rt_task(p) && !task_is_exiting(p);
+	return (p->spa_data.cpu_usage_rate > p->spa_data.cpu_rate_hard_cap) &&
+		(p->state == TASK_RUNNING) && !spa_rt_task(p) && !task_is_exiting(p);
 }
 
 static inline void put_task_in_sinbin(struct task_struct *p)
@@ -1254,8 +1254,8 @@
 		return;
 	deactivate_task(p);
 	p->flags |= PF_SINBINNED;
-	p->sinbin_timer.expires = jiffies + durn;
-	add_timer(&p->sinbin_timer);
+	p->spa_data.sinbin_timer.expires = jiffies + durn;
+	add_timer(&p->spa_data.sinbin_timer);
 }
 
 /*
@@ -1272,9 +1272,9 @@
 	 */
 	delta_delay_stats(p, adjusted_sched_clock(p));
 	p->flags &= ~PF_SINBINNED;
-	if (!rt_task(p)) {
+	if (!spa_rt_task(p)) {
 		calculate_pre_bonus_priority(p);
-		p->prio = effective_prio(p);
+		p->spa_data.prio = effective_prio(p);
 	}
 	__activate_task(p);
 
@@ -1293,7 +1293,7 @@
 {
 	int need_resched, nrpolling;
 
-	BUG_ON(!spin_is_locked(&p->rq->lock));
+	BUG_ON(!spin_is_locked(&p->spa_data.rq->lock));
 
 	/* minimise the chance of sending an interrupt to poll_idle() */
 	nrpolling = test_tsk_thread_flag(p,TIF_POLLING_NRFLAG);
@@ -1313,7 +1313,7 @@
 static inline void preempt_curr_if_warranted(struct task_struct *p)
 {
 	if (task_preempts_curr(p))
-		resched_task(p->rq->curr);
+		resched_task(p->spa_data.rq->curr);
 }
 
 /**
@@ -1362,7 +1362,7 @@
 			delta_sleep_stats(p, adjusted_sched_clock(p));
 		set_task_cpu(p, dest_cpu);
 		/* time stamp was set for old queue above so fix it */
-		p->sched_timestamp = adjusted_sched_clock(p);
+		p->spa_data.sched_timestamp = adjusted_sched_clock(p);
 		return 0;
 	}
 
@@ -1370,7 +1370,7 @@
 	req->type = REQ_MOVE_TASK;
 	req->task = p;
 	req->dest_cpu = dest_cpu;
-	list_add(&req->list, &p->rq->migration_queue);
+	list_add(&req->list, &p->spa_data.rq->migration_queue);
 	return 1;
 }
 
@@ -1517,8 +1517,8 @@
 #endif
 
 	rql = task_rq_lock(p, &flags);
-	old_rq = p->rq;
-	schedstat_inc(p->rq, ttwu_cnt);
+	old_rq = p->spa_data.rq;
+	schedstat_inc(p->spa_data.rq, ttwu_cnt);
 	old_state = p->state;
 	if (!(old_state & state))
 		goto out;
@@ -1596,10 +1596,10 @@
 
 	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
 out_set_cpu:
-	schedstat_inc(p->rq, ttwu_attempts);
+	schedstat_inc(p->spa_data.rq, ttwu_attempts);
 	new_cpu = wake_idle(new_cpu, p);
 	if (new_cpu != cpu && cpu_isset(new_cpu, p->cpus_allowed)) {
-		schedstat_inc(p->rq, ttwu_moved);
+		schedstat_inc(p->spa_data.rq, ttwu_moved);
 		set_task_cpu(p, new_cpu);
 		task_rq_unlock(rql, &flags);
 		/* might preempt at this point */
@@ -1625,7 +1625,7 @@
 	 * called at times when thes calculations are unnecessary e.g. for a
 	 * change of CPU
 	 */
-	if (!rt_task(p))
+	if (!spa_rt_task(p))
 		reassess_interactiveness(p);
 	/*
 	 * Sync wakeups (i.e. those types of wakeups where the waker
@@ -1671,9 +1671,9 @@
  */
 static inline void initialize_bonuses(task_t *p)
 {
-	p->interactive_bonus = (max_ia_bonus >= initial_ia_bonus) ?
+	p->spa_data.interactive_bonus = (max_ia_bonus >= initial_ia_bonus) ?
 				initial_ia_bonus : max_ia_bonus;
-	p->throughput_bonus =  0;
+	p->spa_data.throughput_bonus =  0;
 }
 
 /*
@@ -1691,8 +1691,8 @@
 	p->state = TASK_RUNNING;
 	INIT_LIST_HEAD(&p->run_list);
 	spin_lock_init(&p->switch_lock);
-	init_timer(&p->sinbin_timer);
-	p->sinbin_timer.data = (unsigned long) p;
+	init_timer(&p->spa_data.sinbin_timer);
+	p->spa_data.sinbin_timer.data = (unsigned long) p;
 #ifdef CONFIG_SCHEDSTATS
 	memset(&p->sched_info, 0, sizeof(p->sched_info));
 #endif
@@ -1708,7 +1708,7 @@
 	/*
 	 * Give the child a new timeslice
 	 */
-	p->time_slice = task_timeslice(p);
+	p->spa_data.time_slice = task_timeslice(p);
 	p->timestamp = sched_clock();
 	/*
 	 * Initialize the scheduling statistics and bonus counters
@@ -1736,7 +1736,7 @@
 
 	BUG_ON(p->state != TASK_RUNNING);
 
-	schedstat_inc(p->rq, wunt_cnt);
+	schedstat_inc(p->spa_data.rq, wunt_cnt);
 
 	if (likely(cpu == this_cpu)) {
 		if (!(clone_flags & CLONE_VM)) {
@@ -1749,22 +1749,22 @@
 			 * only fork() doesn't end up in the idle priority slot. 
 			 * Just testing for empty run list is no longer adequate.
 			 */
-			if (unlikely(!task_queued(current) || RUNQUEUE_IDLE(current->rq))) {
-				p->prio = effective_prio(p);
+			if (unlikely(!task_queued(current) || RUNQUEUE_IDLE(current->spa_data.rq))) {
+				p->spa_data.prio = effective_prio(p);
 				__activate_task(p);
 			} else {
 				/*
 				 * Put the child on the same list(s) as (but
 				 *  ahead of) the parent
 				 */
-				p->prio = current->prio;
+				p->spa_data.prio = current->spa_data.prio;
 				list_add_tail(&p->run_list, &current->run_list);
-				current->rq->nr_running++;
+				current->spa_data.rq->nr_running++;
 			}
 			set_need_resched();
 		} else {
 			/* Run child last */
-			p->prio = effective_prio(p);
+			p->spa_data.prio = effective_prio(p);
 			__activate_task(p);
 		}
 	} else {
@@ -1774,10 +1774,10 @@
 		 */
 		adjust_timestamp(p, this_rq());
 		adjust_sched_timestamp(p, this_rq());
-		p->prio = effective_prio(p);
+		p->spa_data.prio = effective_prio(p);
 		__activate_task(p);
 		preempt_curr_if_warranted(p);
-		schedstat_inc(p->rq, wunt_moved);
+		schedstat_inc(p->spa_data.rq, wunt_moved);
 	}
 	task_rq_unlock(rql, &flags);
 }
@@ -2042,11 +2042,11 @@
 	    || unlikely(cpu_is_offline(dest_cpu)))
 		goto out;
 
-	schedstat_inc(p->rq, smt_cnt);
+	schedstat_inc(p->spa_data.rq, smt_cnt);
 	/* force the process onto the specified CPU */
 	if (migrate_task(p, dest_cpu, &req)) {
 		/* Need to wait for migration thread (might exit: take ref). */
-		struct task_struct *mt = p->rq->migration_thread;
+		struct task_struct *mt = p->spa_data.rq->migration_thread;
 		get_task_struct(mt);
 		task_rq_unlock(rql, &flags);
 		wake_up_process(mt);
@@ -2100,13 +2100,13 @@
 static inline
 void pull_task(task_t *p,  int this_cpu)
 {
-	runqueue_t *src_rq = p->rq;
+	runqueue_t *src_rq = p->spa_data.rq;
 
 	dequeue_task(p);
 	src_rq->nr_running--;
 	delta_delay_stats(p, adjusted_sched_clock(p));
 	set_task_cpu(p, this_cpu);
-	p->rq->nr_running++;
+	p->spa_data.rq->nr_running++;
 	enqueue_task(p);
 	adjust_timestamp(p, src_rq);
 	adjust_sched_timestamp(p, src_rq);
@@ -2718,8 +2718,8 @@
 		__set_bit(new_prio, rq->bitmap);
 	}
 	/* The only prio field that might need updating is the current task's */
-	if (likely((rq->curr->prio > MIN_NORMAL_PRIO) && (rq->curr->prio < BGND_PRIO)))
-		rq->curr->prio--;
+	if (likely((rq->curr->spa_data.prio > MIN_NORMAL_PRIO) && (rq->curr->spa_data.prio < BGND_PRIO)))
+		rq->curr->spa_data.prio--;
 	restart_promotions(rq);
 out_unlock:
 	spin_unlock(&rq->lock);
@@ -2744,7 +2744,7 @@
 	unsigned long decayed_avg_nr_running;
 	unsigned long long now;
 
-	now = p->rq->timestamp_last_tick = sched_clock();
+	now = p->spa_data.rq->timestamp_last_tick = sched_clock();
 
 	if (rcu_pending(cpu))
 		rcu_check_callbacks(cpu, user_ticks);
@@ -2761,21 +2761,21 @@
 	/* this has to be done regardless of task type but hold lock for the
 	 * minimum possible time
 	 */
-	decayed_avg_nr_running = SCHED_AVG_MUL(p->rq->avg_nr_running, SCHED_AVG_ALPHA);
-	spin_lock(&p->rq->lock);
-	p->rq->avg_nr_running = decayed_avg_nr_running + p->rq->nr_running;
-	if ((sched_mode == SCHED_MODE_ENTITLEMENT_BASED) && (!--p->rq->eb_ticks_to_decay))
-		decay_eb_yardstick(p->rq);
-	spin_unlock(&p->rq->lock);
+	decayed_avg_nr_running = SCHED_AVG_MUL(p->spa_data.rq->avg_nr_running, SCHED_AVG_ALPHA);
+	spin_lock(&p->spa_data.rq->lock);
+	p->spa_data.rq->avg_nr_running = decayed_avg_nr_running + p->spa_data.rq->nr_running;
+	if ((sched_mode == SCHED_MODE_ENTITLEMENT_BASED) && (!--p->spa_data.rq->eb_ticks_to_decay))
+		decay_eb_yardstick(p->spa_data.rq);
+	spin_unlock(&p->spa_data.rq->lock);
 
 	if (is_idle_task(p)) {
-		if (atomic_read(&p->rq->nr_iowait) > 0)
+		if (atomic_read(&p->spa_data.rq->nr_iowait) > 0)
 			cpustat->iowait += sys_ticks;
 		else
 			cpustat->idle += sys_ticks;
-		if (wake_priority_sleeper(p->rq))
+		if (wake_priority_sleeper(p->spa_data.rq))
 			goto out;
-		rebalance_tick(cpu, p->rq, SCHED_IDLE);
+		rebalance_tick(cpu, p->spa_data.rq, SCHED_IDLE);
 		return;
 	}
 	if (TASK_NICE(p) > 0)
@@ -2790,8 +2790,8 @@
 	if (unlikely(p->policy == SCHED_FIFO))
 		goto out;
 
-	spin_lock(&p->rq->lock);
-	if (!--p->time_slice) {
+	spin_lock(&p->spa_data.rq->lock);
+	if (!--p->spa_data.time_slice) {
 		dequeue_task(p);
 		set_tsk_need_resched(p);
 		if (likely(p->policy != SCHED_RR)) {
@@ -2806,16 +2806,16 @@
 			 * tasks to lose some of there bonus?
 			 */
 			calculate_pre_bonus_priority(p);
-			p->prio = effective_prio(p);
+			p->spa_data.prio = effective_prio(p);
 		}
-		p->time_slice = task_timeslice(p);
+		p->spa_data.time_slice = task_timeslice(p);
 		enqueue_task(p);
 	}
-	spin_unlock(&p->rq->lock);
+	spin_unlock(&p->spa_data.rq->lock);
 out:
-	rebalance_tick(cpu, p->rq, NOT_IDLE);
-	if (unlikely(promotions_due(p->rq)))
-		do_promotions(p->rq);
+	rebalance_tick(cpu, p->spa_data.rq, NOT_IDLE);
+	if (unlikely(promotions_due(p->spa_data.rq)))
+		do_promotions(p->spa_data.rq);
 }
 
 #ifdef CONFIG_SCHED_SMT
@@ -2894,7 +2894,7 @@
 	idx = sched_find_first_bit(this_rq->bitmap);
 	p = list_entry(this_rq->queues[idx].queue.next, task_t, run_list);
 	/* update prio in case p has been promoted since it was queued */
-	p->prio = idx;
+	p->spa_data.prio = idx;
 
 	for_each_cpu_mask(i, sibling_map) {
 		runqueue_t *smt_rq = cpu_rq(i);
@@ -2908,9 +2908,9 @@
 		 * task from using an unfair proportion of the
 		 * physical cpu's resources. -ck
 		 */
-		if (((smt_curr->time_slice * (100 - sd->per_cpu_gain) / 100) >
-			task_timeslice(p) || rt_task(smt_curr)) &&
-			p->mm && smt_curr->mm && !rt_task(p))
+		if (((smt_curr->spa_data.time_slice * (100 - sd->per_cpu_gain) / 100) >
+			task_timeslice(p) || spa_rt_task(smt_curr)) &&
+			p->mm && smt_curr->mm && !spa_rt_task(p))
 				ret = 1;
 
 		/*
@@ -2918,9 +2918,9 @@
 		 * or wake it up if it has been put to sleep for priority
 		 * reasons.
 		 */
-		if ((((p->time_slice * (100 - sd->per_cpu_gain) / 100) >
-			task_timeslice(smt_curr) || rt_task(p)) &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
+		if ((((p->spa_data.time_slice * (100 - sd->per_cpu_gain) / 100) >
+			task_timeslice(smt_curr) || spa_rt_task(p)) &&
+			smt_curr->mm && p->mm && !spa_rt_task(smt_curr)) ||
 			(smt_curr == smt_rq->idle && smt_rq->nr_running))
 				resched_task(smt_curr);
 	}
@@ -3201,7 +3201,7 @@
 need_resched:
 	preempt_disable();
 	prev = current;
-	rq = prev->rq;
+	rq = prev->spa_data.rq;
 
 	/*
 	 * The idle thread is not allowed to schedule!
@@ -3268,7 +3268,7 @@
 	/*
 	 * update prio just in case next has been promoted since it was queued
 	 */ 
-	next->prio = idx;
+	next->spa_data.prio = idx;
 
 switch_tasks:
 	prefetch(next);
@@ -3571,27 +3571,27 @@
 	 */
 	rql = task_rq_lock(p, &flags);
 
-	p->static_prio = NICE_TO_PRIO(nice);
-	p->eb_shares = nice_to_shares(nice);
+	p->spa_data.static_prio = NICE_TO_PRIO(nice);
+	p->spa_data.eb_shares = nice_to_shares(nice);
 	/*
 	 * The RT priorities are set via setscheduler(), but we still
 	 * allow the 'normal' nice value to be set - but as expected
 	 * it wont have any effect on scheduling until the task is
 	 * not SCHED_NORMAL:
 	 */
-	if (!rt_task(p) && task_queued(p)) {
-		int delta = -p->prio;
+	if (!spa_rt_task(p) && task_queued(p)) {
+		int delta = -p->spa_data.prio;
 
 		dequeue_task(p);
 		calculate_pre_bonus_priority(p);
-		delta += p->prio = effective_prio(p);
+		delta += p->spa_data.prio = effective_prio(p);
 		enqueue_task(p);
 		/*
 		 * If the task increased its priority or is running and
 		 * lowered its priority, then reschedule its CPU:
 		 */
 		if (delta < 0 || (delta > 0 && task_is_running(p)))
-			resched_task(p->rq->curr);
+			resched_task(p->spa_data.rq->curr);
 	}
 
 	task_rq_unlock(rql, &flags);
@@ -3634,7 +3634,7 @@
 	if (increment > 40)
 		increment = 40;
 
-	nice = PRIO_TO_NICE(current->static_prio) + increment;
+	nice = PRIO_TO_NICE(current->spa_data.static_prio) + increment;
 	if (nice < -20)
 		nice = -20;
 	if (nice > 19)
@@ -3668,7 +3668,7 @@
 	 * the task might be in the middle of scheduling on another CPU.
 	 */
 	rql = task_rq_lock(p, &flags);
-	delta = new_cap - p->cpu_rate_cap;
+	delta = new_cap - p->spa_data.cpu_rate_cap;
 	if (!is_allowed) {
 		/*
 		 * Ordinary users can set/change caps on their own tasks provided
@@ -3684,20 +3684,20 @@
 	 * set - but as expected it wont have any effect on scheduling until the
 	 * task becomes SCHED_NORMAL:
 	 */
-	p->cpu_rate_cap = new_cap;
-	if (!rt_task(p) && task_queued(p)) {
-		int delta = -p->prio;
+	p->spa_data.cpu_rate_cap = new_cap;
+	if (!spa_rt_task(p) && task_queued(p)) {
+		int delta = -p->spa_data.prio;
 
 		dequeue_task(p);
 		calculate_pre_bonus_priority(p);
-		delta += p->prio = effective_prio(p);
+		delta += p->spa_data.prio = effective_prio(p);
 		enqueue_task(p);
 		/*
 		 * If the task increased its priority or is running and
 		 * lowered its priority, then reschedule its CPU:
 		 */
 		if (delta < 0 || (delta > 0 && task_is_running(p)))
-			resched_task(p->rq->curr);
+			resched_task(p->spa_data.rq->curr);
 	}
 	task_rq_unlock(rql, &flags);
 	return 0;
@@ -3723,7 +3723,7 @@
 	 * the task might be in the middle of scheduling on another CPU.
 	 */
 	rql = task_rq_lock(p, &flags);
-	delta = new_cap - p->cpu_rate_hard_cap;
+	delta = new_cap - p->spa_data.cpu_rate_hard_cap;
 	if (!is_allowed) {
 		/*
 		 * Ordinary users can set/change caps on their own tasks provided
@@ -3739,7 +3739,7 @@
 	 * set - but as expected it wont have any effect on scheduling until the
 	 * task becomes SCHED_NORMAL:
 	 */
-	p->cpu_rate_hard_cap = new_cap;
+	p->spa_data.cpu_rate_hard_cap = new_cap;
 	/* (POSSIBLY) TODO: if it's sinbinned and the cap is relaxed then release
 	 *  it from the sinbin
 	 */
@@ -3756,7 +3756,7 @@
 	unsigned long flags;
 	spinlock_t *rql;
 
-	if (p->eb_shares == new_shares)
+	if (p->spa_data.eb_shares == new_shares)
 		return 0;
 
 	if ((new_shares < 1) || (new_shares > MAX_EB_SHARES))
@@ -3768,31 +3768,31 @@
 	 * the task might be in the middle of scheduling on another CPU.
 	 */
 	rql = task_rq_lock(p, &flags);
-	if (!is_allowed && (new_shares > p->eb_shares)) {
+	if (!is_allowed && (new_shares > p->spa_data.eb_shares)) {
 		result = -EPERM;
 		goto out_unlock;
 	}
-	p->static_prio = NICE_TO_PRIO(shares_to_nice(new_shares));
-	p->eb_shares = new_shares;
+	p->spa_data.static_prio = NICE_TO_PRIO(shares_to_nice(new_shares));
+	p->spa_data.eb_shares = new_shares;
 	/*
 	 * The RT priorities are set via setscheduler(), but we still
 	 * allow eb_shares value to be set - but as expected
 	 * it wont have any effect on scheduling until the task is
 	 * not SCHED_NORMAL:
 	 */
-	if (!rt_task(p) && task_queued(p)) {
-		int delta = -p->prio;
+	if (!spa_rt_task(p) && task_queued(p)) {
+		int delta = -p->spa_data.prio;
 
 		dequeue_task(p);
 		calculate_pre_bonus_priority(p);
-		delta += p->prio = effective_prio(p);
+		delta += p->spa_data.prio = effective_prio(p);
 		enqueue_task(p);
 		/*
 		 * If the task decreased its prio or is running and
 		 * increased its prio, then reschedule its CPU:
 		 */
 		if (delta < 0 || (delta > 0 && task_is_running(p)))
-			resched_task(p->rq->curr);
+			resched_task(p->spa_data.rq->curr);
 	}
 out_unlock:
 	task_rq_unlock(rql, &flags);
@@ -3812,7 +3812,7 @@
  */
 int task_prio(const task_t *p)
 {
-	return p->prio - MAX_RT_PRIO;
+	return p->spa_data.prio - MAX_RT_PRIO;
 }
 
 /**
@@ -3853,9 +3853,9 @@
 	p->policy = policy;
 	p->rt_priority = prio;
 	if (policy != SCHED_NORMAL)
-		p->prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
+		p->spa_data.prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
 	else
-		p->prio = p->static_prio;
+		p->spa_data.prio = p->spa_data.static_prio;
 }
 
 /*
@@ -3935,7 +3935,7 @@
 	if ((queued = task_queued(p)))
 		deactivate_task(p);
 	retval = 0;
-	oldprio = p->prio;
+	oldprio = p->spa_data.prio;
 	__setscheduler(p, policy, lp.sched_priority);
 	if (queued) {
 		__activate_task(p);
@@ -3945,7 +3945,7 @@
 		 * this runqueue and our priority is higher than the current's
 		 */
 		if (task_is_running(p)) {
-			if (p->prio > oldprio)
+			if (p->spa_data.prio > oldprio)
 				resched_task(p);
 		} else
 			preempt_curr_if_warranted(p);
@@ -4191,14 +4191,14 @@
 	unsigned long flags;
 	spinlock_t *rql = task_rq_lock(tsk, &flags);
 
-	stats->timestamp = tsk->rq->timestamp_last_tick;
-	stats->cycle_count = tsk->cycle_count;
-	stats->total_sleep = tsk->total_sleep;
-	stats->total_cpu = tsk->total_cpu;
-	stats->total_delay = tsk->total_delay;
-	stats->total_sinbin = tsk->total_sinbin;
-	stats->intr_wake_ups = tsk->intr_wake_ups;
-	timestamp = tsk->sched_timestamp;
+	stats->timestamp = tsk->spa_data.rq->timestamp_last_tick;
+	stats->cycle_count = tsk->spa_data.cycle_count;
+	stats->total_sleep = tsk->spa_data.total_sleep;
+	stats->total_cpu = tsk->spa_data.total_cpu;
+	stats->total_delay = tsk->spa_data.total_delay;
+	stats->total_sinbin = tsk->spa_data.total_sinbin;
+	stats->intr_wake_ups = tsk->spa_data.intr_wake_ups;
+	timestamp = tsk->spa_data.sched_timestamp;
 	if ((on_runq = task_queued(tsk)))
 		on_cpu = task_is_running(tsk);
 	else
@@ -4253,9 +4253,9 @@
 	spin_lock(&rq->lock);
 	idle = rq->curr == rq->idle;
 	stats->timestamp = rq->timestamp_last_tick;
-	idle_timestamp = rq->idle->sched_timestamp;
-	stats->total_idle = rq->idle->total_cpu;
-	stats->total_busy = rq->idle->total_delay;
+	idle_timestamp = rq->idle->spa_data.sched_timestamp;
+	stats->total_idle = rq->idle->spa_data.total_cpu;
+	stats->total_busy = rq->idle->spa_data.total_delay;
 	stats->total_delay = rq->total_delay;
 	stats->total_sinbin = rq->total_sinbin;
 	stats->nr_switches = rq->nr_switches;
@@ -4281,7 +4281,7 @@
 {
 	spinlock_t *rql = this_rq_lock();
 
-	schedstat_inc(current->rq, yld_cnt);
+	schedstat_inc(current->spa_data.rq, yld_cnt);
 	/* If there's other tasks on this CPU make sure that at least
 	 * one of them get some CPU before this task's next bite of the
 	 * cherry.  Dequeue before looking for the appropriate run
@@ -4292,14 +4292,14 @@
 	/*
 	 * special rule: RT tasks will just roundrobin.
 	 */
-	if (likely(!rt_task(current))) {
-		int idx = find_next_bit(current->rq->bitmap, IDLE_PRIO, current->prio);
+	if (likely(!spa_rt_task(current))) {
+		int idx = find_next_bit(current->spa_data.rq->bitmap, IDLE_PRIO, current->spa_data.prio);
 		if (idx < IDLE_PRIO)
-			current->prio = idx;
+			current->spa_data.prio = idx;
 	}
 	enqueue_task(current);
-	if (current->rq->nr_running == 1)
-		schedstat_inc(current->rq, yld_both_empty);
+	if (current->spa_data.rq->nr_running == 1)
+		schedstat_inc(current->spa_data.rq, yld_both_empty);
 
 	/*
 	 * Since we are going to call schedule() anyway, there's
@@ -4611,7 +4611,7 @@
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long flags;
 
-	idle->prio = IDLE_PRIO;
+	idle->spa_data.prio = IDLE_PRIO;
 	/*
 	 * Initialize scheduling statistics counters as they may provide
 	 * valuable about the CPU e.g. avg_cpu_time_per_cycle for the idle
@@ -4624,7 +4624,7 @@
 
 	spin_lock_irqsave(&rq->lock, flags);
 	rq->curr = rq->idle = idle;
-	idle->sched_timestamp = adjusted_sched_clock(idle);
+	idle->spa_data.sched_timestamp = adjusted_sched_clock(idle);
 	/*
 	 * Putting the idle process onto a run queue simplifies the selection of
 	 * the next task to run in schedule().
@@ -4699,7 +4699,7 @@
 	if (migrate_task(p, any_online_cpu(new_mask), &req)) {
 		/* Need help from migration thread: drop lock and wait. */
 		task_rq_unlock(rql, &flags);
-		wake_up_process(p->rq->migration_thread);
+		wake_up_process(p->spa_data.rq->migration_thread);
 		wait_for_completion(&req.done);
 		tlb_migrate_finish(p->mm);
 		return 0;
@@ -4999,7 +4999,7 @@
 		/* Idle task back to normal in IDLE_PRIO slot */
 		rql = task_rq_lock(rq->idle, &flags);
 		deactivate_task(rq->idle);
-		rq->idle->static_prio = IDLE_PRIO;
+		rq->idle->spa_data.static_prio = IDLE_PRIO;
 		__setscheduler(rq->idle, SCHED_NORMAL, 0);
 		enqueue_task(rq->idle);
 		migrate_dead_tasks(cpu);
@@ -5501,8 +5501,10 @@
 		rq->avg_nr_running = 0;
 		rq->total_sinbin = 0;
 	}
-	current->rq = this_rq();
-	current->sched_timestamp = sched_clock();
+	current->spa_data.rq = this_rq();
+	current->spa_data.sched_timestamp = sched_clock();
+
+	printk("Using %s version %s\n", spa_scheduler_name, spa_scheduler_version);
 
 	/*
 	 * The boot idle thread does lazy MMU switching as well:
Index: xx-sources/kernel/staircase-sched.c
===================================================================
--- xx-sources.orig/kernel/staircase-sched.c	2004-10-07 10:01:14.543355480 -0400
+++ xx-sources/kernel/staircase-sched.c	2004-10-07 10:12:12.139385688 -0400
@@ -46,13 +46,13 @@
 #include <linux/seq_file.h>
 #include <linux/syscalls.h>
 #include <linux/times.h>
+#include <linux/sysctl.h>
 
 #include <asm/tlb.h>
 
 #include <asm/unistd.h>
 
-const char *scheduler_name = "Staircase";
-const char *scheduler_version = "8.4";
+#define STAIRCASE_SCHEDULER_VERSION "8.4"
 
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
@@ -61,7 +61,7 @@
  */
 #define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
 #define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
-#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->staircase_data.static_prio)
 
 /*
  * 'User priority' is the nice value converted to something we
@@ -69,7 +69,7 @@
  * it's a [ 0 ... 39 ] range.
  */
 #define USER_PRIO(p)		((p)-MAX_RT_PRIO)
-#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
+#define TASK_USER_PRIO(p)	USER_PRIO((p)->staircase_data.static_prio)
 #define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
 
 /*
@@ -80,14 +80,14 @@
 #define JIFFIES_TO_US(TIME)	((TIME) * (1000000 / HZ))
 #define US_TO_JIFFIES(TIME)	((TIME) / (1000000 / HZ))
 
-int sched_compute = 0;
+int staircase_sched_compute = 0;
 /*
  *This is the time all tasks within the same priority round robin.
  *compute setting is reserved for dedicated computational scheduling
  *and has ten times larger intervals.
  */
 #define _RR_INTERVAL		(10000)		/* microseconds */
-#define RR_INTERVAL()		(_RR_INTERVAL * (1 + 9 * sched_compute))
+#define RR_INTERVAL()		(_RR_INTERVAL * (1 + 9 * staircase_sched_compute))
 
 #define task_hot(p, now, sd) ((long long) ((now) - (p)->last_ran)	\
 				< (long long) (sd)->cache_hot_time)
@@ -96,8 +96,6 @@
  * These are the runqueue data structures:
  */
 
-typedef struct runqueue runqueue_t;
-
 /*
  * This is the main, per-CPU runqueue data structure.
  *
@@ -459,15 +457,15 @@
 static void dequeue_task(struct task_struct *p, runqueue_t *rq)
 {
 	list_del_init(&p->run_list);
-	if (list_empty(rq->queue + p->prio))
-		__clear_bit(p->prio, rq->bitmap);
+	if (list_empty(rq->queue + p->staircase_data.prio))
+		__clear_bit(p->staircase_data.prio, rq->bitmap);
 }
 
 static void enqueue_task(struct task_struct *p, runqueue_t *rq)
 {
 	sched_info_queued(p);
-	list_add_tail(&p->run_list, rq->queue + p->prio);
-	__set_bit(p->prio, rq->bitmap);
+	list_add_tail(&p->run_list, rq->queue + p->staircase_data.prio);
+	__set_bit(p->staircase_data.prio, rq->bitmap);
 }
 
 /*
@@ -477,8 +475,8 @@
  */
 static inline void enqueue_task_head(struct task_struct *p, runqueue_t *rq)
 {
-	list_add(&p->run_list, rq->queue + p->prio);
-	__set_bit(p->prio, rq->bitmap);
+	list_add(&p->run_list, rq->queue + p->staircase_data.prio);
+	__set_bit(p->staircase_data.prio, rq->bitmap);
 }
 
 /*
@@ -505,25 +503,25 @@
  */
 static unsigned int burst(task_t *p)
 {
-	if (likely(!rt_task(p))) {
+	if (likely(!staircase_rt_task(p))) {
 		unsigned int task_user_prio = TASK_USER_PRIO(p);
 		return 39 - task_user_prio;
 	} else
-		return p->burst;
+		return p->staircase_data.burst;
 }
 
 static void inc_burst(task_t *p)
 {
 	unsigned int best_burst;
 	best_burst = burst(p);
-	if (p->burst < best_burst)
-		p->burst++;
+	if (p->staircase_data.burst < best_burst)
+		p->staircase_data.burst++;
 }
 
 static void dec_burst(task_t *p)
 {
-	if (p->burst)
-		p->burst--;
+	if (p->staircase_data.burst)
+		p->staircase_data.burst--;
 }
 
 /*
@@ -533,7 +531,7 @@
 static unsigned int slice(task_t *p)
 {
 	unsigned int slice = RR_INTERVAL();
-	if (likely(!rt_task(p)))
+	if (likely(!staircase_rt_task(p)))
 		slice += burst(p) * RR_INTERVAL();
 	return slice;
 }
@@ -541,7 +539,7 @@
 /*
  * sched_interactive - sysctl which allows interactive tasks to have bursts
  */
-int sched_interactive = 1;
+int staircase_sched_interactive = 1;
 
 /*
  * effective_prio - dynamic priority dependent on burst.
@@ -554,17 +552,17 @@
 	int prio;
 	unsigned int full_slice, used_slice, first_slice;
 	unsigned int best_burst;
-	if (rt_task(p))
-		return p->prio;
+	if (staircase_rt_task(p))
+		return p->staircase_data.prio;
 
 	best_burst = burst(p);
 	full_slice = slice(p);
-	used_slice = full_slice - p->slice;
-	if (p->burst > best_burst)
-		p->burst = best_burst;
+	used_slice = full_slice - p->staircase_data.slice;
+	if (p->staircase_data.burst > best_burst)
+		p->staircase_data.burst = best_burst;
 	first_slice = RR_INTERVAL();
-	if (sched_interactive && !sched_compute && p->mm)
-		first_slice *= (p->burst + 1);
+	if (staircase_sched_interactive && !staircase_sched_compute && p->mm)
+		first_slice *= (p->staircase_data.burst + 1);
 	prio = MAX_PRIO - 1 - best_burst;
 
 	if (used_slice < first_slice)
@@ -586,38 +584,38 @@
 	unsigned long sleep_time = NS_TO_US(_sleep_time);
 	unsigned long rr = RR_INTERVAL();
 	unsigned int best_burst = burst(p);
-	unsigned long minrun = rr * (p->burst + 1) / (best_burst + 1) ? : 1;
+	unsigned long minrun = rr * (p->staircase_data.burst + 1) / (best_burst + 1) ? : 1;
 
 	if (p->flags & PF_FORKED || (p->mm &&
-		(p->runtime + sleep_time < minrun || 
-		((!sched_interactive || sched_compute) && 
-		p->runtime + sleep_time < rr)))) {
-			unsigned long total_run = p->totalrun + p->runtime;
+		(p->staircase_data.runtime + sleep_time < minrun || 
+		((!staircase_sched_interactive || staircase_sched_compute) && 
+		p->staircase_data.runtime + sleep_time < rr)))) {
+			unsigned long total_run = p->staircase_data.totalrun + p->staircase_data.runtime;
 			p->flags &= ~PF_FORKED;
-			if (p->slice - total_run < 1) {
-				p->totalrun = 0;
+			if (p->staircase_data.slice - total_run < 1) {
+				p->staircase_data.totalrun = 0;
 				dec_burst(p);
 			} else {
 				unsigned int intervals = total_run / rr;
 				unsigned long remainder;
-				p->totalrun = total_run;
-				p->slice -= intervals * rr;
-				if (p->slice <= rr) {
-					p->totalrun = 0;
+				p->staircase_data.totalrun = total_run;
+				p->staircase_data.slice -= intervals * rr;
+				if (p->staircase_data.slice <= rr) {
+					p->staircase_data.totalrun = 0;
 					dec_burst(p);
 				} else {
-					remainder = p->slice % rr;
+					remainder = p->staircase_data.slice % rr;
 					if (remainder)
-						p->time_slice = remainder;
+						p->staircase_data.time_slice = remainder;
 				}
 			}
 	} else {
-		if (p->totalrun > (best_burst - p->burst) * rr)
+		if (p->staircase_data.totalrun > (best_burst - p->staircase_data.burst) * rr)
 			dec_burst(p);
-		else if (!((p->flags & PF_UISLEEP) || p->totalrun))
+		else if (!((p->flags & PF_UISLEEP) || p->staircase_data.totalrun))
 			inc_burst(p);
-		p->runtime = 0;
-		p->totalrun = 0;
+		p->staircase_data.runtime = 0;
+		p->staircase_data.totalrun = 0;
 	}
 }
 
@@ -635,11 +633,11 @@
 			+ rq->timestamp_last_tick;
 	}
 #endif
-	p->slice = slice(p);
-	p->time_slice = RR_INTERVAL();
+	p->staircase_data.slice = slice(p);
+	p->staircase_data.time_slice = RR_INTERVAL();
 	recalc_task_prio(p, now);
 	p->flags &= ~PF_UISLEEP;
-	p->prio = effective_prio(p);
+	p->staircase_data.prio = effective_prio(p);
 	p->timestamp = now;
 	__activate_task(p, rq);
 }
@@ -690,7 +688,7 @@
  * task_curr - is this task currently executing on a CPU?
  * @p: the task in question.
  */
-inline int task_curr(const task_t *p)
+inline int staircase_task_curr(const task_t *p)
 {
 	return cpu_curr(task_cpu(p)) == p;
 }
@@ -705,13 +703,13 @@
 {
 	task_t *curr = rq->curr;
 
-	if (task->prio > curr->prio)
+	if (task->staircase_data.prio > curr->staircase_data.prio)
 		return;
-	else if (task->prio == curr->prio && 
-		(task->totalrun || rt_task(curr)))
+	else if (task->staircase_data.prio == curr->staircase_data.prio && 
+		(task->staircase_data.totalrun || staircase_rt_task(curr)))
 			return;
-	else if (sched_compute && rq->cache_ticks < cache_delay &&
-		task->mm && !rt_task(task)) {
+	else if (staircase_sched_compute && rq->cache_ticks < cache_delay &&
+		task->mm && !staircase_rt_task(task)) {
 			rq->preempted = 1;
 			return;
 	}
@@ -772,7 +770,7 @@
  * smp_call_function() if an IPI is sent by the same process we are
  * waiting to become inactive.
  */
-void wait_task_inactive(task_t * p)
+void staircase_wait_task_inactive(task_t * p)
 {
 	unsigned long flags;
 	runqueue_t *rq;
@@ -800,19 +798,17 @@
  * Cause a process which is running on another CPU to enter
  * kernel-mode, without any delay. (to get signals handled.)
  */
-void kick_process(task_t *p)
+void staircase_kick_process(task_t *p)
 {
 	int cpu;
 
 	preempt_disable();
 	cpu = task_cpu(p);
-	if ((cpu != smp_processor_id()) && task_curr(p))
+	if ((cpu != smp_processor_id()) && staircase_task_curr(p))
 		smp_send_reschedule(cpu);
 	preempt_enable();
 }
 
-EXPORT_SYMBOL_GPL(kick_process);
-
 /*
  * Return a low guess at the load of a migration-source cpu.
  *
@@ -1022,15 +1018,13 @@
 	return success;
 }
 
-int fastcall wake_up_process(task_t * p)
+int fastcall staircase_wake_up_process(task_t * p)
 {
 	return try_to_wake_up(p, TASK_STOPPED | TASK_TRACED |
 		       		 TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE, 0);
 }
 
-EXPORT_SYMBOL(wake_up_process);
-
-int fastcall wake_up_state(task_t *p, unsigned int state)
+int fastcall staircase_wake_up_state(task_t *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
 }
@@ -1044,7 +1038,7 @@
  * Perform scheduler related setup for a newly forked process p.
  * p is forked by current.
  */
-void fastcall sched_fork(task_t *p)
+void fastcall staircase_sched_fork(task_t *p)
 {
 	/*
 	 * We mark the process as running here, but have not actually
@@ -1076,7 +1070,7 @@
  * that must be done for every newly created context, then puts the task
  * on the runqueue and wakes it.
  */
-void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
+void fastcall staircase_wake_up_new_task(task_t * p, unsigned long clone_flags)
 {
 	unsigned long flags;
 	int this_cpu, cpu;
@@ -1087,7 +1081,7 @@
 	/*
 	 * Forked process gets no burst to prevent fork bombs.
 	 */
-	p->burst = 0;
+	p->staircase_data.burst = 0;
 	p->parent->flags |= PF_FORKED;
 	cpu = task_cpu(p);
 	this_cpu = smp_processor_id();
@@ -1160,7 +1154,7 @@
  * schedule_tail - first thing a freshly forked thread must call.
  * @prev: the thread we just switched away from.
  */
-asmlinkage void schedule_tail(task_t *prev)
+asmlinkage void staircase_schedule_tail(task_t *prev)
 {
 	finish_task_switch(prev);
 
@@ -1204,7 +1198,7 @@
  * threads, current number of uninterruptible-sleeping threads, total
  * number of context switches performed since bootup.
  */
-unsigned long nr_running(void)
+unsigned long staircase_nr_running(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1214,7 +1208,7 @@
 	return sum;
 }
 
-unsigned long nr_uninterruptible(void)
+unsigned long staircase_nr_uninterruptible(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1224,7 +1218,7 @@
 	return sum;
 }
 
-unsigned long long nr_context_switches(void)
+unsigned long long staircase_nr_context_switches(void)
 {
 	unsigned long long i, sum = 0;
 
@@ -1234,7 +1228,7 @@
 	return sum;
 }
 
-unsigned long nr_iowait(void)
+unsigned long staircase_nr_iowait(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1364,7 +1358,7 @@
 		struct task_struct *mt = rq->migration_thread;
 		get_task_struct(mt);
 		task_rq_unlock(rq, &flags);
-		wake_up_process(mt);
+		staircase_wake_up_process(mt);
 		put_task_struct(mt);
 		wait_for_completion(&req.done);
 		return;
@@ -1380,7 +1374,7 @@
  * execve() is a valuable balancing opportunity, because at this point
  * the task has the smallest effective memory and cache footprint.
  */
-void sched_exec(void)
+void staircase_sched_exec(void)
 {
 	struct sched_domain *tmp, *sd = NULL;
 	int new_cpu, this_cpu = get_cpu();
@@ -1752,7 +1746,7 @@
 			}
 			spin_unlock(&busiest->lock);
 			if (wake)
-				wake_up_process(busiest->migration_thread);
+				staircase_wake_up_process(busiest->migration_thread);
 
 			/*
 			 * We've kicked active balancing, reset the failure
@@ -1987,10 +1981,6 @@
 	return 0;
 }
 
-DEFINE_PER_CPU(struct kernel_stat, kstat);
-
-EXPORT_PER_CPU_SYMBOL(kstat);
-
 /*
  * Tasks that run out of time_slice but still have slice left get
  * requeued with a lower priority && rr_interval time_slice.
@@ -1999,8 +1989,8 @@
 {
 	set_tsk_need_resched(p);
 	dequeue_task(p, rq);
-	p->prio = effective_prio(p);
-	p->time_slice = RR_INTERVAL();
+	p->staircase_data.prio = effective_prio(p);
+	p->staircase_data.time_slice = RR_INTERVAL();
 	enqueue_task(p, rq);
 }
 
@@ -2010,14 +2000,14 @@
 static void slice_expired(task_t *p, runqueue_t *rq)
 {
 	dec_burst(p);
-	p->slice = slice(p);
+	p->staircase_data.slice = slice(p);
 	time_slice_expired(p, rq);
 }
 /*
  * This function gets called by the timer code, with HZ frequency.
  * We call it with interrupts disabled.
  */
-void scheduler_tick(int user_ticks, int sys_ticks)
+void staircase_scheduler_tick(int user_ticks, int sys_ticks)
 {
 	int cpu = smp_processor_id();
 	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
@@ -2069,15 +2059,15 @@
 	_decrement = NS_TO_US(_decrement);
 	if (_decrement > 0 && _decrement < decrement)
 		decrement = _decrement;
-	if (p->slice > decrement && US_TO_JIFFIES(p->slice - decrement))
-		p->slice -= decrement;
+	if (p->staircase_data.slice > decrement && US_TO_JIFFIES(p->staircase_data.slice - decrement))
+		p->staircase_data.slice -= decrement;
 	else {
 		slice_expired(p, rq);
 		goto out_unlock;
 	}
-	if (p->time_slice > decrement && 
-		US_TO_JIFFIES(p->time_slice - decrement))
-			p->time_slice -= decrement;
+	if (p->staircase_data.time_slice > decrement && 
+		US_TO_JIFFIES(p->staircase_data.time_slice - decrement))
+			p->staircase_data.time_slice -= decrement;
 	else {
 		time_slice_expired(p, rq);
 		goto out_unlock;
@@ -2178,9 +2168,9 @@
 		 * task from using an unfair proportion of the
 		 * physical cpu's resources. -ck
 		 */
-		if (((smt_curr->time_slice * (100 - sd->per_cpu_gain) / 100) >
-			slice(p) || rt_task(smt_curr)) &&
-			p->mm && smt_curr->mm && !rt_task(p))
+		if (((smt_curr->staircase_data.time_slice * (100 - sd->per_cpu_gain) / 100) >
+			slice(p) || staircase_rt_task(smt_curr)) &&
+			p->mm && smt_curr->mm && !staircase_rt_task(p))
 				ret = 1;
 
 		/*
@@ -2188,9 +2178,9 @@
 		 * or wake it up if it has been put to sleep for priority
 		 * reasons.
 		 */
-		if ((((p->time_slice * (100 - sd->per_cpu_gain) / 100) >
-			slice(smt_curr) || rt_task(p)) &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
+		if ((((p->staircase_data.time_slice * (100 - sd->per_cpu_gain) / 100) >
+			slice(smt_curr) || staircase_rt_task(p)) &&
+			smt_curr->mm && p->mm && !staircase_rt_task(smt_curr)) ||
 			(smt_curr == smt_rq->idle && smt_rq->nr_running))
 				resched_task(smt_curr);
 	}
@@ -2210,91 +2200,6 @@
 }
 #endif
 
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
-
-void fastcall add_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(((int)preempt_count() < 0));
-	preempt_count() += val;
-	/*
-	 * Spinlock count overflowing soon?
-	 */
-	BUG_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
-}
-EXPORT_SYMBOL(add_preempt_count);
-
-void fastcall sub_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(val > preempt_count());
-	/*
-	 * Is the spinlock portion underflowing?
-	 */
-	BUG_ON((val < PREEMPT_MASK) && !(preempt_count() & PREEMPT_MASK));
-	preempt_count() -= val;
-}
-EXPORT_SYMBOL(sub_preempt_count);
-
-#ifdef __smp_processor_id
-/*
- * Debugging check.
- */
-unsigned int smp_processor_id(void)
-{
-	unsigned long preempt_count = preempt_count();
-	int this_cpu = __smp_processor_id();
-	cpumask_t this_mask;
-
-	if (likely(preempt_count))
-		goto out;
-
-	if (irqs_disabled())
-		goto out;
-
-	/*
-	 * Kernel threads bound to a single CPU can safely use
-	 * smp_processor_id():
-	 */
-	this_mask = cpumask_of_cpu(this_cpu);
-
-	if (cpus_equal(current->cpus_allowed, this_mask))
-		goto out;
-
-	/*
-	 * It is valid to assume CPU-locality during early bootup:
-	 */
-	if (system_state != SYSTEM_RUNNING)
-		goto out;
-
-	/*
-	 * Avoid recursion:
-	 */
-	preempt_disable();
-
-	if (!printk_ratelimit())
-		goto out_enable;
-
-	printk(KERN_ERR "using smp_processor_id() in preemptible code: %s/%d\n",
-		current->comm, current->pid);
-	dump_stack();
-
-out_enable:
-	preempt_enable_no_resched();
-out:
-	return this_cpu;
-}
-
-EXPORT_SYMBOL(smp_processor_id);
-
-#endif /* __smp_processor_id */
-
-#endif /* PREEMPT && DEBUG_PREEMPT */
-
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 
 #ifdef CONFIG_PREEMPT_BKL
@@ -2314,13 +2219,11 @@
  */
 static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
 
-int kernel_locked(void)
+int staircase_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
-
 /*
  * Release global kernel semaphore:
  */
@@ -2359,7 +2262,7 @@
 /*
  * Getting the big kernel semaphore.
  */
-void lock_kernel(void)
+void staircase_lock_kernel(void)
 {
 	struct task_struct *task = current;
 	int depth = task->lock_depth + 1;
@@ -2373,9 +2276,7 @@
 	task->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
-
-void unlock_kernel(void)
+void staircase_unlock_kernel(void)
 {
 	struct task_struct *task = current;
 
@@ -2385,19 +2286,15 @@
 		up(&kernel_sem);
 }
 
-EXPORT_SYMBOL(unlock_kernel);
-
 #else
 
 static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
 
-int kernel_locked(void)
+int staircase_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
-
 #define get_kernel_lock()	spin_lock(&kernel_flag)
 #define put_kernel_lock()	spin_unlock(&kernel_flag)
 
@@ -2426,7 +2323,7 @@
  * so we only need to worry about other
  * CPU's.
  */
-void lock_kernel(void)
+void staircase_lock_kernel(void)
 {
 	int depth = current->lock_depth+1;
 	if (likely(!depth))
@@ -2434,17 +2331,13 @@
 	current->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
-
-void unlock_kernel(void)
+void staircase_unlock_kernel(void)
 {
 	BUG_ON(current->lock_depth < 0);
 	if (likely(--current->lock_depth < 0))
 		put_kernel_lock();
 }
 
-EXPORT_SYMBOL(unlock_kernel);
-
 #endif
 
 #else
@@ -2458,7 +2351,7 @@
 /*
  * schedule() is the main scheduler function.
  */
-asmlinkage void __sched schedule(void)
+asmlinkage void __sched staircase_schedule(void)
 {
 	long *switch_count;
 	task_t *prev, *next;
@@ -2489,7 +2382,7 @@
 	release_kernel_sem(prev);
 	schedstat_inc(rq, sched_cnt);
 	now = sched_clock();
-	prev->runtime = NS_TO_US(now - prev->timestamp) ? : 1;
+	prev->staircase_data.runtime = NS_TO_US(now - prev->timestamp) ? : 1;
 	if (prev->mm && prev->policy != SCHED_FIFO &&
 		prev->state == TASK_RUNNING &&
 		prev->timestamp > rq->timestamp_last_tick) {
@@ -2497,15 +2390,15 @@
 			 * We have not run through a scheduler_tick and are
 			 * still running so charge us with the runtime.
 			 */
-			if (unlikely(US_TO_JIFFIES(prev->slice - 
-				prev->runtime) < 1))
+			if (unlikely(US_TO_JIFFIES(prev->staircase_data.slice - 
+				prev->staircase_data.runtime) < 1))
 					slice_expired(prev, rq);
-			else if (unlikely(US_TO_JIFFIES(prev->time_slice -
-				prev->runtime) < 1))
+			else if (unlikely(US_TO_JIFFIES(prev->staircase_data.time_slice -
+				prev->staircase_data.runtime) < 1))
 					time_slice_expired(prev, rq);
 			else {
-				 prev->slice -= prev->runtime;
-				 prev->time_slice -= prev->runtime;
+				 prev->staircase_data.slice -= prev->staircase_data.runtime;
+				 prev->staircase_data.time_slice -= prev->staircase_data.runtime;
 			}
 	}
 	prev->timestamp = now;
@@ -2569,7 +2462,7 @@
 	if (next->flags & PF_YIELDED) {
 		next->flags &= ~PF_YIELDED;
 		dequeue_task(next, rq);
-		next->prio = effective_prio(next);
+		next->staircase_data.prio = effective_prio(next);
 		enqueue_task_head(next, rq);
 	}
 
@@ -2604,7 +2497,7 @@
  * off of preempt_enable.  Kernel preemptions off return from interrupt
  * occur there and call schedule directly.
  */
-asmlinkage void __sched preempt_schedule(void)
+asmlinkage void __sched staircase_preempt_schedule(void)
 {
 	struct thread_info *ti = current_thread_info();
 #ifdef CONFIG_PREEMPT_BKL
@@ -2632,7 +2525,7 @@
 	saved_lock_depth = task->lock_depth;
 	task->lock_depth = -1;
 #endif
-	schedule();
+	staircase_schedule();
 #ifdef CONFIG_PREEMPT_BKL
 	task->lock_depth = saved_lock_depth;
 #endif
@@ -2644,17 +2537,14 @@
 		goto need_resched;
 }
 
-EXPORT_SYMBOL(preempt_schedule);
 #endif /* CONFIG_PREEMPT */
 
-int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
+int staircase_default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
 {
 	task_t *p = curr->task;
 	return try_to_wake_up(p, mode, sync);
 }
 
-EXPORT_SYMBOL(default_wake_function);
-
 /*
  * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
  * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve
@@ -2687,7 +2577,7 @@
  * @mode: which threads
  * @nr_exclusive: how many wake-one or wake-many threads to wake up
  */
-void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
+void fastcall staircase___wake_up(wait_queue_head_t *q, unsigned int mode,
 				int nr_exclusive, void *key)
 {
 	unsigned long flags;
@@ -2697,12 +2587,10 @@
 	spin_unlock_irqrestore(&q->lock, flags);
 }
 
-EXPORT_SYMBOL(__wake_up);
-
 /*
  * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
  */
-void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
+void fastcall staircase___wake_up_locked(wait_queue_head_t *q, unsigned int mode)
 {
 	__wake_up_common(q, mode, 1, 0, NULL);
 }
@@ -2720,7 +2608,7 @@
  *
  * On UP it can prevent extra preemption.
  */
-void fastcall __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
+void fastcall staircase___wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
 {
 	unsigned long flags;
 	int sync = 1;
@@ -2735,9 +2623,8 @@
 	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
-EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
 
-void fastcall complete(struct completion *x)
+void fastcall staircase_complete(struct completion *x)
 {
 	unsigned long flags;
 
@@ -2747,9 +2634,8 @@
 			 1, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete);
 
-void fastcall complete_all(struct completion *x)
+void fastcall staircase_complete_all(struct completion *x)
 {
 	unsigned long flags;
 
@@ -2759,9 +2645,8 @@
 			 0, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete_all);
 
-void fastcall __sched wait_for_completion(struct completion *x)
+void fastcall __sched staircase_wait_for_completion(struct completion *x)
 {
 	might_sleep();
 	spin_lock_irq(&x->wait.lock);
@@ -2773,7 +2658,7 @@
 		do {
 			__set_current_state(TASK_UNINTERRUPTIBLE);
 			spin_unlock_irq(&x->wait.lock);
-			schedule();
+			staircase_schedule();
 			spin_lock_irq(&x->wait.lock);
 		} while (!x->done);
 		__remove_wait_queue(&x->wait, &wait);
@@ -2781,7 +2666,6 @@
 	x->done--;
 	spin_unlock_irq(&x->wait.lock);
 }
-EXPORT_SYMBOL(wait_for_completion);
 
 #define	SLEEP_ON_VAR					\
 	unsigned long flags;				\
@@ -2798,20 +2682,18 @@
 	__remove_wait_queue(q, &wait);			\
 	spin_unlock_irqrestore(&q->lock, flags);
 
-void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+void fastcall __sched staircase_interruptible_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_INTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	staircase_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on);
-
-long fastcall __sched interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched staircase_interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -2824,22 +2706,18 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on_timeout);
-
-void fastcall __sched sleep_on(wait_queue_head_t *q)
+void fastcall __sched staircase_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_UNINTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	staircase_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(sleep_on);
-
-long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched staircase_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -2852,9 +2730,7 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(sleep_on_timeout);
-
-void set_user_nice(task_t *p, long nice)
+void staircase_set_user_nice(task_t *p, long nice)
 {
 	unsigned long flags;
 	runqueue_t *rq;
@@ -2873,18 +2749,18 @@
 	 * it wont have any effect on scheduling until the task is
 	 * not SCHED_NORMAL:
 	 */
-	if (rt_task(p)) {
-		p->static_prio = NICE_TO_PRIO(nice);
+	if (staircase_rt_task(p)) {
+		p->staircase_data.static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
 	if ((queued = task_queued(p)))
 		dequeue_task(p, rq);
 
-	old_prio = p->prio;
+	old_prio = p->staircase_data.prio;
 	new_prio = NICE_TO_PRIO(nice);
 	delta = new_prio - old_prio;
-	p->static_prio = NICE_TO_PRIO(nice);
-	p->prio += delta;
+	p->staircase_data.static_prio = NICE_TO_PRIO(nice);
+	p->staircase_data.prio += delta;
 
 	if (queued) {
 		enqueue_task(p, rq);
@@ -2899,8 +2775,6 @@
 	task_rq_unlock(rq, &flags);
 }
 
-EXPORT_SYMBOL(set_user_nice);
-
 #ifdef CONFIG_KGDB
 struct task_struct *kgdb_get_idle(int this_cpu)
 {
@@ -2917,7 +2791,7 @@
  * sys_setpriority is a more generic, but much slower function that
  * does similar things.
  */
-asmlinkage long sys_nice(int increment)
+asmlinkage long staircase_sys_nice(int increment)
 {
 	int retval;
 	long nice;
@@ -2936,7 +2810,7 @@
 	if (increment > 40)
 		increment = 40;
 
-	nice = PRIO_TO_NICE(current->static_prio) + increment;
+	nice = PRIO_TO_NICE(current->staircase_data.static_prio) + increment;
 	if (nice < -20)
 		nice = -20;
 	if (nice > 19)
@@ -2946,7 +2820,7 @@
 	if (retval)
 		return retval;
 
-	set_user_nice(current, nice);
+	staircase_set_user_nice(current, nice);
 	return 0;
 }
 
@@ -2960,33 +2834,29 @@
  * RT tasks are offset by -200. Normal tasks are centered
  * around 0, value goes from -16 to +15.
  */
-int task_prio(const task_t *p)
+int staircase_task_prio(const task_t *p)
 {
-	return p->prio - MAX_RT_PRIO;
+	return p->staircase_data.prio - MAX_RT_PRIO;
 }
 
 /**
  * task_nice - return the nice value of a given task.
  * @p: the task in question.
  */
-int task_nice(const task_t *p)
+int staircase_task_nice(const task_t *p)
 {
 	return TASK_NICE(p);
 }
 
-EXPORT_SYMBOL(task_nice);
-
 /**
  * idle_cpu - is a given cpu idle currently?
  * @cpu: the processor in question.
  */
-int idle_cpu(int cpu)
+int staircase_idle_cpu(int cpu)
 {
 	return cpu_curr(cpu) == cpu_rq(cpu)->idle;
 }
 
-EXPORT_SYMBOL_GPL(idle_cpu);
-
 /**
  * find_process_by_pid - find a process with a matching PID value.
  * @pid: the pid in question.
@@ -3003,9 +2873,9 @@
 	p->policy = policy;
 	p->rt_priority = prio;
 	if (policy != SCHED_NORMAL)
-		p->prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
+		p->staircase_data.prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
 	else
-		p->prio = p->static_prio;
+		p->staircase_data.prio = p->staircase_data.static_prio;
 }
 
 /*
@@ -3079,7 +2949,7 @@
 	if ((queued = task_queued(p)))
 		deactivate_task(p, task_rq(p));
 	retval = 0;
-	oldprio = p->prio;
+	oldprio = p->staircase_data.prio;
 	__setscheduler(p, policy, lp.sched_priority);
 	if (queued) {
 		__activate_task(p, task_rq(p));
@@ -3089,7 +2959,7 @@
 		 * this runqueue and our priority is higher than the current's
 		 */
 		if (task_running(rq, p)) {
-			if (p->prio > oldprio)
+			if (p->staircase_data.prio > oldprio)
 				resched_task(rq->curr);
 		} else
 			preempt(p, rq);
@@ -3110,7 +2980,7 @@
  * @policy: new policy
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
+asmlinkage long staircase_sys_sched_setscheduler(pid_t pid, int policy,
 				       struct sched_param __user *param)
 {
 	return setscheduler(pid, policy, param);
@@ -3121,7 +2991,7 @@
  * @pid: the pid in question.
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long staircase_sys_sched_setparam(pid_t pid, struct sched_param __user *param)
 {
 	return setscheduler(pid, -1, param);
 }
@@ -3130,7 +3000,7 @@
  * sys_sched_getscheduler - get the policy (scheduling class) of a thread
  * @pid: the pid in question.
  */
-asmlinkage long sys_sched_getscheduler(pid_t pid)
+asmlinkage long staircase_sys_sched_getscheduler(pid_t pid)
 {
 	int retval = -EINVAL;
 	task_t *p;
@@ -3157,7 +3027,7 @@
  * @pid: the pid in question.
  * @param: structure containing the RT priority.
  */
-asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long staircase_sys_sched_getparam(pid_t pid, struct sched_param __user *param)
 {
 	struct sched_param lp;
 	int retval = -EINVAL;
@@ -3192,7 +3062,7 @@
 	return retval;
 }
 
-long sched_setaffinity(pid_t pid, cpumask_t new_mask)
+long staircase_sched_setaffinity(pid_t pid, cpumask_t new_mask)
 {
 	task_t *p;
 	int retval;
@@ -3231,52 +3101,7 @@
 	return retval;
 }
 
-static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
-			     cpumask_t *new_mask)
-{
-	if (len < sizeof(cpumask_t)) {
-		memset(new_mask, 0, sizeof(cpumask_t));
-	} else if (len > sizeof(cpumask_t)) {
-		len = sizeof(cpumask_t);
-	}
-	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
-}
-
-/**
- * sys_sched_setaffinity - set the cpu affinity of a process
- * @pid: pid of the process
- * @len: length in bytes of the bitmask pointed to by user_mask_ptr
- * @user_mask_ptr: user-space pointer to the new cpu mask
- */
-asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
-				      unsigned long __user *user_mask_ptr)
-{
-	cpumask_t new_mask;
-	int retval;
-
-	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
-	if (retval)
-		return retval;
-
-	return sched_setaffinity(pid, new_mask);
-}
-
-/*
- * Represents all cpu's present in the system
- * In systems capable of hotplug, this map could dynamically grow
- * as new cpu's are detected in the system via any platform specific
- * method, such as ACPI for e.g.
- */
-
-cpumask_t cpu_present_map;
-EXPORT_SYMBOL(cpu_present_map);
-
-#ifndef CONFIG_SMP
-cpumask_t cpu_online_map = CPU_MASK_ALL;
-cpumask_t cpu_possible_map = CPU_MASK_ALL;
-#endif
-
-long sched_getaffinity(pid_t pid, cpumask_t *mask)
+long staircase_sched_getaffinity(pid_t pid, cpumask_t *mask)
 {
 	int retval;
 	task_t *p;
@@ -3307,7 +3132,7 @@
  * @len: length in bytes of the bitmask pointed to by user_mask_ptr
  * @user_mask_ptr: user-space pointer to hold the current cpu mask
  */
-asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
+asmlinkage long staircase_sys_sched_getaffinity(pid_t pid, unsigned int len,
 				      unsigned long __user *user_mask_ptr)
 {
 	int ret;
@@ -3332,20 +3157,20 @@
  * this function yields the current CPU by dropping the priority of current
  * to the lowest priority and setting the PF_YIELDED flag.
  */
-asmlinkage long sys_sched_yield(void)
+asmlinkage long staircase_sys_sched_yield(void)
 {
 	runqueue_t *rq = this_rq_lock();
 
 	schedstat_inc(rq, yld_cnt);
 
 	dequeue_task(current, rq);
-	current->slice = slice(current);
-	current->time_slice = RR_INTERVAL();
-	if (likely(!rt_task(current))) {
+	current->staircase_data.slice = slice(current);
+	current->staircase_data.time_slice = RR_INTERVAL();
+	if (likely(!staircase_rt_task(current))) {
 		current->flags |= PF_YIELDED;
-		current->prio = MAX_PRIO - 1;
+		current->staircase_data.prio = MAX_PRIO - 1;
 	}
-	current->burst = 0;
+	current->staircase_data.burst = 0;
 	enqueue_task(current, rq);
 
 	/*
@@ -3355,95 +3180,23 @@
 	_raw_spin_unlock(&rq->lock);
 	preempt_enable_no_resched();
 
-	schedule();
-
-	return 0;
-}
-
-static inline void __cond_resched(void)
-{
-
+	staircase_schedule();
 
-	if (preempt_count() & PREEMPT_ACTIVE)
-		return;
-	do {
-		add_preempt_count(PREEMPT_ACTIVE);
-		schedule();
-		sub_preempt_count(PREEMPT_ACTIVE);
-	} while (need_resched());
-}
-
-int __sched cond_resched(void)
-{
-	if (need_resched()) {
-		__cond_resched();
-		return 1;
-	}
 	return 0;
 }
 
-EXPORT_SYMBOL(cond_resched);
-
-/*
- * cond_resched_lock() - if a reschedule is pending, drop the given lock,
- * call schedule, and on return reacquire the lock.
- *
- * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
- * operations here to prevent schedule() from being called twice (once via
- * spin_unlock(), once by hand).
- */
-int cond_resched_lock(spinlock_t * lock)
-{
-#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT)
-	if (lock->break_lock) {
-		lock->break_lock = 0;
-		spin_unlock(lock);
-		cpu_relax();
-		spin_lock(lock);
-	}
-#endif
-	if (need_resched()) {
-		_raw_spin_unlock(lock);
-		preempt_enable_no_resched();
-		__cond_resched();
-		spin_lock(lock);
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched_lock);
-
-int __sched cond_resched_softirq(void)
-{
-	BUG_ON(!in_softirq());
-
-	if (need_resched()) {
-		__local_bh_enable();
-		__cond_resched();
-		local_bh_disable();
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched_softirq);
-
-
 /**
  * yield - yield the current processor to other threads.
  *
  * this is a shortcut for kernel-space yielding - it marks the
  * thread runnable and calls sys_sched_yield().
  */
-void __sched yield(void)
+void __sched staircase_yield(void)
 {
 	set_current_state(TASK_RUNNING);
-	sys_sched_yield();
+	staircase_sys_sched_yield();
 }
 
-EXPORT_SYMBOL(yield);
-
 /*
  * This task is about to go to sleep on IO.  Increment rq->nr_iowait so
  * that process accounting knows that this is a task in IO wait state.
@@ -3451,18 +3204,16 @@
  * But don't do that if it is a deliberate, throttling IO wait (this task
  * has set its backing_dev_info: the queue against which it should throttle)
  */
-void __sched io_schedule(void)
+void __sched staircase_io_schedule(void)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 
 	atomic_inc(&rq->nr_iowait);
-	schedule();
+	staircase_schedule();
 	atomic_dec(&rq->nr_iowait);
 }
 
-EXPORT_SYMBOL(io_schedule);
-
-long __sched io_schedule_timeout(long timeout)
+long __sched staircase_io_schedule_timeout(long timeout)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 	long ret;
@@ -3474,51 +3225,6 @@
 }
 
 /**
- * sys_sched_get_priority_max - return maximum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the maximum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_max(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = MAX_USER_RT_PRIO-1;
-		break;
-	case SCHED_NORMAL:
-		ret = 0;
-		break;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_get_priority_min - return minimum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the minimum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_min(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = 1;
-		break;
-	case SCHED_NORMAL:
-		ret = 0;
-	}
-	return ret;
-}
-
-/**
  * sys_sched_rr_get_interval - return the default timeslice of a process.
  * @pid: pid of the process.
  * @interval: userspace pointer to the timeslice value.
@@ -3527,7 +3233,7 @@
  * into the user-space timespec buffer. A value of '0' means infinity.
  */
 asmlinkage
-long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
+long staircase_sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
 {
 	int retval = -EINVAL;
 	struct timespec t;
@@ -3557,112 +3263,14 @@
 	return retval;
 }
 
-static inline struct task_struct *eldest_child(struct task_struct *p)
-{
-	if (list_empty(&p->children)) return NULL;
-	return list_entry(p->children.next,struct task_struct,sibling);
-}
-
-static inline struct task_struct *older_sibling(struct task_struct *p)
-{
-	if (p->sibling.prev==&p->parent->children) return NULL;
-	return list_entry(p->sibling.prev,struct task_struct,sibling);
-}
-
-static inline struct task_struct *younger_sibling(struct task_struct *p)
-{
-	if (p->sibling.next==&p->parent->children) return NULL;
-	return list_entry(p->sibling.next,struct task_struct,sibling);
-}
-
-static void show_task(task_t * p)
-{
-	task_t *relative;
-	unsigned state;
-	unsigned long free = 0;
-	static const char *stat_nam[] = { "R", "S", "D", "T", "t", "Z", "X" };
-
-	printk("%-13.13s ", p->comm);
-	state = p->state ? __ffs(p->state) + 1 : 0;
-	if (state < ARRAY_SIZE(stat_nam))
-		printk(stat_nam[state]);
-	else
-		printk("?");
-#if (BITS_PER_LONG == 32)
-	if (state == TASK_RUNNING)
-		printk(" running ");
-	else
-		printk(" %08lX ", thread_saved_pc(p));
-#else
-	if (state == TASK_RUNNING)
-		printk("  running task   ");
-	else
-		printk(" %016lx ", thread_saved_pc(p));
-#endif
-#ifdef CONFIG_DEBUG_STACK_USAGE
-	{
-		unsigned long * n = (unsigned long *) (p->thread_info+1);
-		while (!*n)
-			n++;
-		free = (unsigned long) n - (unsigned long)(p->thread_info+1);
-	}
-#endif
-	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
-	if ((relative = eldest_child(p)))
-		printk("%5d ", relative->pid);
-	else
-		printk("      ");
-	if ((relative = younger_sibling(p)))
-		printk("%7d", relative->pid);
-	else
-		printk("       ");
-	if ((relative = older_sibling(p)))
-		printk(" %5d", relative->pid);
-	else
-		printk("      ");
-	if (!p->mm)
-		printk(" (L-TLB)\n");
-	else
-		printk(" (NOTLB)\n");
-
-	if (state != TASK_RUNNING)
-		show_stack(p, NULL);
-}
-
-void show_state(void)
-{
-	task_t *g, *p;
-
-#if (BITS_PER_LONG == 32)
-	printk("\n"
-	       "                                               sibling\n");
-	printk("  task             PC      pid father child younger older\n");
-#else
-	printk("\n"
-	       "                                                       sibling\n");
-	printk("  task                 PC          pid father child younger older\n");
-#endif
-	read_lock(&tasklist_lock);
-	do_each_thread(g, p) {
-		/*
-		 * reset the NMI-timeout, listing all files on a slow
-		 * console might take alot of time:
-		 */
-		touch_nmi_watchdog();
-		show_task(p);
-	} while_each_thread(g, p);
-
-	read_unlock(&tasklist_lock);
-}
-
-void __devinit init_idle(task_t *idle, int cpu)
+void __devinit staircase_init_idle(task_t *idle, int cpu)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long flags;
 
-	idle->prio = MAX_PRIO;
+	idle->staircase_data.prio = MAX_PRIO;
 	idle->state = TASK_RUNNING;
-	idle->burst = 0;
+	idle->staircase_data.burst = 0;
 	set_task_cpu(idle, cpu);
 
 	spin_lock_irqsave(&rq->lock, flags);
@@ -3678,15 +3286,6 @@
 #endif
 }
 
-/*
- * In a system that switches off the HZ timer nohz_cpu_mask
- * indicates which cpus entered this state. This is used
- * in the rcu update to wait only for active cpus. For system
- * which do not switch off the HZ timer nohz_cpu_mask should
- * always be CPU_MASK_NONE.
- */
-cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
-
 #ifdef CONFIG_SMP
 /*
  * This is how migration works:
@@ -3713,7 +3312,7 @@
  * task must not exit() & deallocate itself prematurely.  The
  * call is not atomic; no spinlocks may be held.
  */
-int set_cpus_allowed(task_t *p, cpumask_t new_mask)
+int staircase_set_cpus_allowed(task_t *p, cpumask_t new_mask)
 {
 	unsigned long flags;
 	int ret = 0;
@@ -3736,7 +3335,7 @@
 	if (migrate_task(p, any_online_cpu(new_mask), &req)) {
 		/* Need help from migration thread: drop lock and wait. */
 		task_rq_unlock(rq, &flags);
-		wake_up_process(rq->migration_thread);
+		staircase_wake_up_process(rq->migration_thread);
 		wait_for_completion(&req.done);
 		tlb_migrate_finish(p->mm);
 		return 0;
@@ -3746,8 +3345,6 @@
 	return ret;
 }
 
-EXPORT_SYMBOL_GPL(set_cpus_allowed);
-
 /*
  * Move (not current) task off this cpu, onto dest cpu.  We're doing
  * this because either it can't run here any more (set_cpus_allowed()
@@ -3831,7 +3428,7 @@
 
 		if (list_empty(head)) {
 			spin_unlock_irq(&rq->lock);
-			schedule();
+			staircase_schedule();
 			set_current_state(TASK_INTERRUPTIBLE);
 			continue;
 		}
@@ -3851,7 +3448,7 @@
 			WARN_ON(1);
 		}
 
-		complete(&req->done);
+		staircase_complete(&req->done);
 	}
 	__set_current_state(TASK_RUNNING);
 	return 0;
@@ -3860,7 +3457,7 @@
 	/* Wait for kthread_stop */
 	set_current_state(TASK_INTERRUPTIBLE);
 	while (!kthread_should_stop()) {
-		schedule();
+		staircase_schedule();
 		set_current_state(TASK_INTERRUPTIBLE);
 	}
 	__set_current_state(TASK_RUNNING);
@@ -4014,7 +3611,7 @@
 		break;
 	case CPU_ONLINE:
 		/* Strictly unneccessary, as first user will wake it. */
-		wake_up_process(cpu_rq(cpu)->migration_thread);
+		staircase_wake_up_process(cpu_rq(cpu)->migration_thread);
 		break;
 #ifdef CONFIG_HOTPLUG_CPU
 	case CPU_UP_CANCELED:
@@ -4031,7 +3628,7 @@
 		/* Idle task back to normal (off runqueue, low prio) */
 		rq = task_rq_lock(rq->idle, &flags);
 		deactivate_task(rq->idle, rq);
-		rq->idle->static_prio = MAX_PRIO;
+		rq->idle->staircase_data.static_prio = MAX_PRIO;
 		__setscheduler(rq->idle, SCHED_NORMAL, 0);
 		migrate_dead_tasks(cpu);
 		task_rq_unlock(rq, &flags);
@@ -4047,7 +3644,7 @@
 					 migration_req_t, list);
 			BUG_ON(req->type != REQ_MOVE_TASK);
 			list_del_init(&req->list);
-			complete(&req->done);
+			staircase_complete(&req->done);
 		}
 		spin_unlock_irq(&rq->lock);
 		break;
@@ -4064,7 +3661,7 @@
 	.priority = 10
 };
 
-int __init migration_init(void)
+int __init staircase_migration_init(void)
 {
 	void *cpu = (void *)(long)smp_processor_id();
 	/* Start one for boot CPU. */
@@ -4080,7 +3677,7 @@
  * Attach the domain 'sd' to 'cpu' as its base domain.  Callers must
  * hold the hotplug lock.
  */
-void __devinit cpu_attach_domain(struct sched_domain *sd, int cpu)
+void __devinit staircase_cpu_attach_domain(struct sched_domain *sd, int cpu)
 {
 	migration_req_t req;
 	unsigned long flags;
@@ -4102,13 +3699,13 @@
 	spin_unlock_irqrestore(&rq->lock, flags);
 
 	if (!local) {
-		wake_up_process(rq->migration_thread);
+		staircase_wake_up_process(rq->migration_thread);
 		wait_for_completion(&req.done);
 	}
 }
 
 /* cpus with isolated domains */
-cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
+cpumask_t __devinitdata staircase_cpu_isolated_map = CPU_MASK_NONE;
 
 /* Setup the mask of cpus configured for isolated domains */
 static int __init isolated_cpu_setup(char *str)
@@ -4116,9 +3713,9 @@
 	int ints[NR_CPUS], i;
 
 	str = get_options(str, ARRAY_SIZE(ints), ints);
-	cpus_clear(cpu_isolated_map);
+	cpus_clear(staircase_cpu_isolated_map);
 	for (i = 1; i <= ints[0]; i++)
-		cpu_set(ints[i], cpu_isolated_map);
+		cpu_set(ints[i], staircase_cpu_isolated_map);
 	return 1;
 }
 
@@ -4135,7 +3732,7 @@
  * covered by the given span, and will set each group's ->cpumask correctly,
  * and ->cpu_power to 0.
  */
-void __devinit init_sched_build_groups(struct sched_group groups[],
+void __devinit staircase_init_sched_build_groups(struct sched_group groups[],
 			cpumask_t span, int (*group_fn)(int cpu))
 {
 	struct sched_group *first = NULL, *last = NULL;
@@ -4217,7 +3814,7 @@
 	 * For now this just excludes isolated cpus, but could be used to
 	 * exclude other special cases in the future.
 	 */
-	cpus_complement(cpu_default_map, cpu_isolated_map);
+	cpus_complement(cpu_default_map, staircase_cpu_isolated_map);
 	cpus_and(cpu_default_map, cpu_default_map, cpu_online_map);
 
 	/*
@@ -4266,7 +3863,7 @@
 		if (i != first_cpu(this_sibling_map))
 			continue;
 
-		init_sched_build_groups(sched_group_cpus, this_sibling_map,
+		staircase_init_sched_build_groups(sched_group_cpus, this_sibling_map,
 						&cpu_to_cpu_group);
 	}
 #endif
@@ -4279,13 +3876,13 @@
 		if (cpus_empty(nodemask))
 			continue;
 
-		init_sched_build_groups(sched_group_phys, nodemask,
+		staircase_init_sched_build_groups(sched_group_phys, nodemask,
 						&cpu_to_phys_group);
 	}
 
 #ifdef CONFIG_NUMA
 	/* Set up node groups */
-	init_sched_build_groups(sched_group_nodes, cpu_default_map,
+	staircase_init_sched_build_groups(sched_group_nodes, cpu_default_map,
 					&cpu_to_node_group);
 #endif
 
@@ -4321,7 +3918,7 @@
 #else
 		sd = &per_cpu(phys_domains, i);
 #endif
-		cpu_attach_domain(sd, i);
+		staircase_cpu_attach_domain(sd, i);
 	}
 }
 
@@ -4448,7 +4045,7 @@
 	case CPU_UP_PREPARE:
 	case CPU_DOWN_PREPARE:
 		for_each_online_cpu(i)
-			cpu_attach_domain(&sched_domain_dummy, i);
+			staircase_cpu_attach_domain(&sched_domain_dummy, i);
 		arch_destroy_sched_domains();
 		return NOTIFY_OK;
 
@@ -4473,7 +4070,7 @@
 }
 #endif
 
-void __init sched_init_smp(void)
+void __init staircase_sched_init_smp(void)
 {
 	lock_cpu_hotplug();
 	arch_init_sched_domains();
@@ -4483,21 +4080,54 @@
 	hotcpu_notifier(update_sched_domains, 0);
 }
 #else
-void __init sched_init_smp(void)
+void __init staircase_sched_init_smp(void)
 {
 }
 #endif /* CONFIG_SMP */
 
-int in_sched_functions(unsigned long addr)
+#if defined(CONFIG_SYSCTL)
+static struct ctl_table_header *staircase_table_header;
+
+enum
 {
-	/* Linker adds these: start and end of __sched functions */
-	extern char __sched_text_start[], __sched_text_end[];
-	return in_lock_functions(addr) ||
-		(addr >= (unsigned long)__sched_text_start
-		&& addr < (unsigned long)__sched_text_end);
-}
+	STAIRCASE_CPU_SCHED_END_OF_LIST=0,
+	STAIRCASE_CPU_SCHED_INTERACTIVE=1,
+	STAIRCASE_CPU_SCHED_COMPUTE,
+};
 
-void __init sched_init(void)
+static ctl_table staircase_cpu_sched_table[] = {
+	{
+		.ctl_name	= STAIRCASE_CPU_SCHED_INTERACTIVE,
+		.procname	= "interactive",
+		.data		= &staircase_sched_interactive,
+		.maxlen		= sizeof (int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{
+		.ctl_name	= STAIRCASE_CPU_SCHED_COMPUTE,
+		.procname	= "compute",
+		.data		= &staircase_sched_compute,
+		.maxlen		= sizeof (int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
+	{ .ctl_name = STAIRCASE_CPU_SCHED_END_OF_LIST }
+};
+
+static ctl_table staircase_root_table[] = {
+	{
+		.ctl_name	= CTL_SCHED,
+		.procname	= "sched",
+		.maxlen		= 0,
+		.mode		= 0555,
+		.child		= staircase_cpu_sched_table,
+	},
+	{ .ctl_name = 0 }
+};
+#endif
+
+void __init staircase_sched_init(void)
 {
 	runqueue_t *rq;
 	int i, j;
@@ -4531,6 +4161,11 @@
 		__set_bit(MAX_PRIO, rq->bitmap);
 	}
 
+	printk("Using %s version %s\n", STAIRCASE_SCHEDULER_NAME, STAIRCASE_SCHEDULER_VERSION);
+
+#if defined(CONFIG_SYSCTL)
+	staircase_table_header = register_sysctl_table(staircase_root_table, 1);
+#endif
 	/*
 	 * The boot idle thread does lazy MMU switching as well:
 	 */
@@ -4543,7 +4178,7 @@
 	 * but because we are the idle thread, we just pick up running again
 	 * when this runqueue becomes "idle".
 	 */
-	init_idle(current, smp_processor_id());
+	staircase_init_idle(current, smp_processor_id());
 }
 
 #ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
@@ -4567,3 +4202,72 @@
 }
 EXPORT_SYMBOL(__might_sleep);
 #endif
+
+scheduler_t sched_staircase = {
+	.schedule_tail_fn = 			staircase_schedule_tail,
+	.schedule_fn =				staircase_schedule,
+	.scheduler_tick_fn = 			staircase_scheduler_tick,
+	.yield_fn = 				staircase_yield,
+	.wait_for_completion_fn	=		staircase_wait_for_completion,
+	.idle_cpu_fn = 				staircase_idle_cpu,
+	.default_wake_function_fn = 		staircase_default_wake_function,
+	.__wake_up_fn = 			staircase___wake_up,
+	.__wake_up_locked_fn = 			staircase___wake_up_locked,
+	.__wake_up_sync_fn = 			staircase___wake_up_sync,
+	.complete_fn =				staircase_complete,
+	.complete_all_fn =			staircase_complete_all,
+	.interruptible_sleep_on_fn = 		staircase_interruptible_sleep_on,
+	.interruptible_sleep_on_timeout_fn = 	staircase_interruptible_sleep_on_timeout,
+	.sleep_on_fn =				staircase_sleep_on,
+	.sleep_on_timeout_fn = 			staircase_sleep_on_timeout,
+	.set_user_nice_fn = 			staircase_set_user_nice,
+	.task_nice_fn = 			staircase_task_nice,
+	.io_schedule_fn = 			staircase_io_schedule,
+	.io_schedule_timeout_fn = 		staircase_io_schedule_timeout,
+	.task_curr_fn = 			staircase_task_curr,
+	.wake_up_process_fn = 			staircase_wake_up_process,
+	.wake_up_state_fn = 			staircase_wake_up_state,
+	.nr_running_fn = 			staircase_nr_running,
+	.nr_uninterruptible_fn = 		staircase_nr_uninterruptible,
+	.nr_iowait_fn = 			staircase_nr_iowait,
+	.nr_context_switches_fn = 		staircase_nr_context_switches,
+	.sched_exec_fn = 			staircase_sched_exec,
+	.sched_setaffinity_fn = 		staircase_sched_setaffinity,
+	.sched_getaffinity_fn = 		staircase_sched_getaffinity,
+	.sys_nice_fn = 				staircase_sys_nice,
+	.sys_sched_setscheduler_fn = 		staircase_sys_sched_setscheduler,
+	.sys_sched_setparam_fn = 		staircase_sys_sched_setparam,
+	.sys_sched_getscheduler_fn = 		staircase_sys_sched_getscheduler,
+	.sys_sched_getparam_fn = 		staircase_sys_sched_getparam,
+	.sys_sched_getaffinity_fn = 		staircase_sys_sched_getaffinity,
+	.sys_sched_yield_fn = 			staircase_sys_sched_yield,
+	.sys_sched_rr_get_interval_fn = 	staircase_sys_sched_rr_get_interval,
+	.sched_init_fn = 			staircase_sched_init,
+	.sched_init_smp_fn = 			staircase_sched_init_smp,
+	.migration_init_fn = 			staircase_migration_init,
+	.sched_fork_fn = 			staircase_sched_fork,
+	.init_idle_fn = 			staircase_init_idle,
+	.wake_up_new_task_fn = 			staircase_wake_up_new_task,
+	.task_prio_fn = 			staircase_task_prio,
+#if defined(CONFIG_SMP)
+	.init_sched_build_groups_fn = 		staircase_init_sched_build_groups,
+	.cpu_attach_domain_fn =			staircase_cpu_attach_domain,
+	.set_cpus_allowed_fn = 			staircase_set_cpus_allowed,
+	.wait_task_inactive_fn = 		staircase_wait_task_inactive,
+	.kick_process_fn = 			staircase_kick_process,
+#endif
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+	.kernel_locked_fn = 			staircase_kernel_locked,
+	.lock_kernel_fn = 			staircase_lock_kernel,
+	.unlock_kernel_fn = 			staircase_unlock_kernel,
+#endif
+#if defined(CONFIG_PREEMPT)
+	.preempt_schedule_fn = 			staircase_preempt_schedule,
+#endif
+	
+	.name = 				STAIRCASE_SCHEDULER_NAME,
+	.version = 				STAIRCASE_SCHEDULER_VERSION,
+	.type = 				SCHED_STAIRCASE,
+};
+
+EXPORT_SYMBOL(sched_staircase);
Index: xx-sources/kernel/sysctl.c
===================================================================
--- xx-sources.orig/kernel/sysctl.c	2004-10-07 10:02:06.598441904 -0400
+++ xx-sources/kernel/sysctl.c	2004-10-07 10:13:35.906651128 -0400
@@ -150,9 +150,6 @@
 #ifdef CONFIG_UNIX98_PTYS
 extern ctl_table pty_table[];
 #endif
-#if defined(CONFIG_XSCHED) || defined(CONFIG_SPA)
-extern ctl_table cpu_sched_table[];
-#endif
 
 #ifdef HAVE_ARCH_PICK_MMAP_LAYOUT
 int sysctl_legacy_va_layout;
@@ -641,45 +638,6 @@
 		.proc_handler	= &proc_dointvec_jiffies,
 		.strategy	= &sysctl_jiffies,
 	},
-#if defined(CONFIG_NICKSCHED)
-	{
-		.ctl_name	= KERN_SCHED_TIMESLICE,
-		.procname	= "base_timeslice",
-		.data		= &sched_base_timeslice,
-		.maxlen		= sizeof (int),
-		.mode		= 0644,
-		.proc_handler	= &proc_dointvec_minmax,
-		.strategy	= &sysctl_intvec,
-		.extra1		= &sched_min_base,
-		.extra2		= &sched_max_base,
-	},
-#endif
-#if defined(CONFIG_STAIRCASE)
-	{
-		.ctl_name	= KERN_INTERACTIVE,
-		.procname	= "interactive",
-		.data		= &sched_interactive,
-		.maxlen		= sizeof (int),
-		.mode		= 0644,
-		.proc_handler	= &proc_dointvec,
-	},
-	{
-		.ctl_name	= KERN_COMPUTE,
-		.procname	= "compute",
-		.data		= &sched_compute,
-		.maxlen		= sizeof (int),
-		.mode		= 0644,
-		.proc_handler	= &proc_dointvec,
-	},
-#endif
-#if defined(CONFIG_XSCHED) || defined(CONFIG_SPA)
-	{
-		.ctl_name	= KERN_CPU_SCHED,
-		.procname	= "cpusched",
-		.mode		= 0555,
-		.child		= cpu_sched_table,
-	},
-#endif
 	{ .ctl_name = 0 }
 };
 
Index: xx-sources/kernel/xsched-sched.c
===================================================================
--- xx-sources.orig/kernel/xsched-sched.c	2004-10-07 10:02:01.196263160 -0400
+++ xx-sources/kernel/xsched-sched.c	2004-10-07 10:13:12.972137704 -0400
@@ -52,8 +52,7 @@
 
 #include <asm/unistd.h>
 
-const char *scheduler_name = "Xsched";
-const char *scheduler_version = "v02e";
+#define XSCHED_SCHEDULER_VERSION "v02e"
 
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
@@ -62,7 +61,7 @@
  */
 #define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 30)
 #define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 30)
-#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->xsched_data.static_prio)
 
 /*
  * 'User priority' is the nice value converted to something we
@@ -70,13 +69,13 @@
  * it's a [ 0 ... 39 ] range.
  */
 #define USER_PRIO(p)		((p)-MAX_RT_PRIO)
-#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
+#define MAX_USER_PRIO		(USER_PRIO(NICK_MAX_PRIO))
 
 #define US_TO_JIFFIES(x)	((x) * HZ / 1000000)
 #define JIFFIES_TO_US(x)	((x) * 1000000 / HZ)
 
-int sched_interactive = 1;
-int sched_compute = 0;
+int xsched_sched_interactive = 1;
+int xsched_sched_compute = 0;
 /*
  * MIN_TIMESLICE is the timeslice that a minimum priority process gets if there
  * is a maximum priority process runnable. MAX_TIMESLICE is derived from the
@@ -84,15 +83,15 @@
  * that the maximum priority process will get. Larger timeslices are attainable
  * by low priority processes however.
  */
-int sched_base_timeslice = 64; /* This gets divided by 8 */
+int xsched_sched_base_timeslice = 64; /* This gets divided by 8 */
 int sched_rt_timeslice = 50;
-int sched_min_base = 1;
-int sched_max_base = 10000;
+int xsched_sched_min_base = 1;
+int xsched_sched_max_base = 10000;
 
 int scaled_rt_timeslice = 50;
 
 #define RT_TIMESLICE		(sched_rt_timeslice)
-#define BASE_TIMESLICE		(sched_base_timeslice)
+#define BASE_TIMESLICE		(xsched_sched_base_timeslice)
 #define MIN_TIMESLICE		(BASE_TIMESLICE * HZ / 1000 / 4 / 8 ?: 1)
 #define MAX_TIMESLICE		(BASE_TIMESLICE * (MAX_USER_PRIO + 1)/3 * 2 / 8)
 
@@ -132,7 +131,7 @@
 int uisleep_factor = 2;
 int shatter_ratio = 8;
 int interactive_limit = 3;
-#define TASK_INTERACTIVE(p)			((p)->static_prio - (p)->prio >= interactive_limit)
+#define TASK_INTERACTIVE(p)			((p)->xsched_data.static_prio - (p)->xsched_data.prio >= interactive_limit)
 #define SLICE_SHATTER(slice, ratio)		((slice) % (slice / ratio) == 0)
 
 /*
@@ -149,17 +148,13 @@
 /*
  * These are the runqueue data structures:
  */
-#define IDLE_PRIO	(MAX_PRIO + 1)
-#define BATCH_PRIO	(MAX_PRIO)
-#define NUM_PRIO_SLOTS (IDLE_PRIO + 1)
+#define NUM_PRIO_SLOTS (NICK_MAX_PRIO + 1)
 
 /*
  * Is the run queue idle?
  */
 #define RUNQUEUE_IDLE(rq) ((rq)->curr == (rq)->idle)
 
-typedef struct runqueue runqueue_t;
-
 struct prio_slot {
 	unsigned int prio;
 	struct list_head queue;
@@ -526,8 +521,8 @@
 
 static inline int task_preempts_curr(const struct task_struct *p, runqueue_t *rq)
 {
-	if (p->prio < rq->current_prio_slot->prio) {
-		if (rt_task(p) || rq->cache_ticks >= cache_decay_ticks ||
+	if (p->xsched_data.prio < rq->current_prio_slot->prio) {
+		if (xsched_rt_task(p) || rq->cache_ticks >= cache_decay_ticks ||
 			!p->mm || rq->curr == rq->idle)
 				return 1;
 		rq->preempted = 1;
@@ -543,11 +538,11 @@
 
 static inline void update_min_prio(const task_t *p, runqueue_t *rq)
 {
-	if (likely(!rt_task(p))) {
-		if (p->prio < rq->min_prio)
-			rq->min_prio = p->prio;
-		if (p->static_prio < rq->min_nice)
-			rq->min_nice = p->static_prio;
+	if (likely(!xsched_rt_task(p))) {
+		if (p->xsched_data.prio < rq->min_prio)
+			rq->min_prio = p->xsched_data.prio;
+		if (p->xsched_data.static_prio < rq->min_nice)
+			rq->min_nice = p->xsched_data.static_prio;
 	}
 }
 
@@ -564,9 +559,9 @@
 
 static void enqueue_task(struct task_struct *p, runqueue_t *rq)
 {
-	struct list_head *entry = &rq->queues[p->prio].queue;
+	struct list_head *entry = &rq->queues[p->xsched_data.prio].queue;
 	sched_info_queued(p);
-	if (!rt_task(p)) {
+	if (!xsched_rt_task(p)) {
 		/*
 		 * Cycle tasks on the same priority level. This reduces their
 		 * timeslice fluctuations due to higher priority tasks expiring.
@@ -575,7 +570,7 @@
 			entry = entry->next;
 	}
 	list_add_tail(&p->run_list, entry);
-	__set_bit(p->prio, rq->bitmap);
+	__set_bit(p->xsched_data.prio, rq->bitmap);
 }
 
 /*
@@ -585,11 +580,11 @@
  */
 static inline void enqueue_task_head(struct task_struct *p, runqueue_t *rq)
 {
-	struct list_head *entry = &rq->queues[p->prio].queue;
+	struct list_head *entry = &rq->queues[p->xsched_data.prio].queue;
 	if (!list_empty(entry))
 		entry = entry->next;
 	list_add(&p->run_list, entry);
-	__set_bit(p->prio, rq->bitmap);
+	__set_bit(p->xsched_data.prio, rq->bitmap);
 }
 
 /*
@@ -617,20 +612,20 @@
 		time = max_affect;
 
 	ratio = MAX_SLEEP - time;
-	tmp = (unsigned long long)ratio*p->total_time + MAX_SLEEP/2;
+	tmp = (unsigned long long)ratio*p->xsched_data.total_time + MAX_SLEEP/2;
 	tmp >>= max_sleep_shift;
-	p->total_time = (unsigned long)tmp;
+	p->xsched_data.total_time = (unsigned long)tmp;
 
-	tmp = (unsigned long long)ratio*p->sleep_time + MAX_SLEEP/2;
+	tmp = (unsigned long long)ratio*p->xsched_data.sleep_time + MAX_SLEEP/2;
 	tmp >>= max_sleep_shift;
-	p->sleep_time = (unsigned long)tmp;
+	p->xsched_data.sleep_time = (unsigned long)tmp;
 
 	if (type != STIME_WAIT) {
-		p->total_time += time;
+		p->xsched_data.total_time += time;
 		if (type == STIME_SLEEP)
-			p->sleep_time += time;
+			p->xsched_data.sleep_time += time;
 
-		p->sleep_avg = (sleep_factor * p->sleep_time) / p->total_time;
+		p->xsched_data.sleep_avg = (sleep_factor * p->xsched_data.sleep_time) / p->xsched_data.total_time;
 	}
 }
 
@@ -647,8 +642,8 @@
 	int idx, base, delta;
 	int timeslice;
 
-	if (rt_task(p)) {
-		if (!sched_interactive)
+	if (xsched_rt_task(p)) {
+		if (!xsched_sched_interactive)
 			return RT_TIMESLICE;
 		timeslice = scaled_rt_timeslice;
 		if (scaled_rt_timeslice != RT_TIMESLICE)
@@ -656,12 +651,12 @@
 		return timeslice;
 	}
 
-	idx = min(p->prio, rq->min_prio);
-	delta = p->prio - idx;
+	idx = min(p->xsched_data.prio, rq->min_prio);
+	delta = p->xsched_data.prio - idx;
 	base = BASE_TIMESLICE * (MAX_USER_PRIO + 1) / (delta + 3);
 
-	idx = min(rq->min_nice, p->static_prio);
-	delta = p->static_prio - idx;
+	idx = min(rq->min_nice, p->xsched_data.static_prio);
+	delta = p->xsched_data.static_prio - idx;
 	timeslice = base * 2 / (delta + 2);
 
 	timeslice = timeslice * 30 / (60 - USER_PRIO(idx));
@@ -670,10 +665,10 @@
 	timeslice *= (1000 / HZ);
 	timeslice >>= 3;
 
-	if (sched_compute || batch_task(p))
+	if (xsched_sched_compute)
 		timeslice <<= 3;
 
-	if ((timeslice > scaled_rt_timeslice) && sched_interactive) {
+	if ((timeslice > scaled_rt_timeslice) && xsched_sched_interactive) {
 		delta = timeslice + scaled_rt_timeslice;
 		delta >>= 2;
 		scaled_rt_timeslice = timeslice + delta;
@@ -699,39 +694,31 @@
 {
  	int prio, bonus;
 
-	if (rt_task(p))
-		return p->prio;
-
-	if (batch_task(p)) {
-		if (unlikely(p->flags & PF_UISLEEP)) {
-			p->flags |= PF_YIELDED;
-			return MAX_PRIO-29;
-		}
-		return BATCH_PRIO;
-	}
+	if (xsched_rt_task(p))
+		return p->xsched_data.prio;
 
 	if (p->flags & PF_YIELDED)
-		return MAX_PRIO-1;
+		return NICK_MAX_PRIO-1;
 
-	prio = USER_PRIO(p->static_prio) + 10;
+	prio = USER_PRIO(p->xsched_data.static_prio) + 10;
 
-	bonus = (((MAX_USER_PRIO + 1) / 3) * p->sleep_avg + (sleep_factor / 2))
+	bonus = (((MAX_USER_PRIO + 1) / 3) * p->xsched_data.sleep_avg + (sleep_factor / 2))
 					/ sleep_factor;
 
-	if ((p->flags & PF_UISLEEP) && sched_interactive)
+	if ((p->flags & PF_UISLEEP) && xsched_sched_interactive)
 		bonus /= uisleep_factor;
 
 	bonus = MAX_RT_PRIO - bonus;
 
-	if (((p->mm == NULL) && sched_interactive) || iso_task(p))
+	if ((p->mm == NULL) && xsched_sched_interactive)
 		bonus /= uisleep_factor;
 
 	prio += bonus;
 
 	if (prio < MAX_RT_PRIO)
 		return MAX_RT_PRIO;
-	if (prio > MAX_PRIO-1)
-		return MAX_PRIO-1;
+	if (prio > NICK_MAX_PRIO-1)
+		return NICK_MAX_PRIO-1;
 
 	return prio;
 }
@@ -749,7 +736,7 @@
 {
 	int slice, ratio;
 
-	ratio = (p->prio - rq->min_prio) / shatter_ratio;
+	ratio = (p->xsched_data.prio - rq->min_prio) / shatter_ratio;
 
 	if (ratio <= 1)
 		return;
@@ -808,7 +795,7 @@
 	sleep = now - p->timestamp;
 	add_task_time(p, sleep, STIME_SLEEP);
 	p->flags &= ~PF_UISLEEP;
-	p->prio = task_priority(p);
+	p->xsched_data.prio = task_priority(p);
 	p->timestamp = now;
 
 	__activate_task(p, rq);
@@ -856,7 +843,7 @@
  * task_curr - is this task currently executing on a CPU?
  * @p: the task in question.
  */
-inline int task_curr(const task_t *p)
+inline int xsched_task_curr(const task_t *p)
 {
 	return cpu_curr(task_cpu(p)) == p;
 }
@@ -915,7 +902,7 @@
  * smp_call_function() if an IPI is sent by the same process we are
  * waiting to become inactive.
  */
-void wait_task_inactive(task_t * p)
+void xsched_wait_task_inactive(task_t * p)
 {
 	unsigned long flags;
 	runqueue_t *rq;
@@ -943,19 +930,17 @@
  * Cause a process which is running on another CPU to enter
  * kernel-mode, without any delay. (to get signals handled.)
  */
-void kick_process(task_t *p)
+void xsched_kick_process(task_t *p)
 {
 	int cpu;
 
 	preempt_disable();
 	cpu = task_cpu(p);
-	if ((cpu != smp_processor_id()) && task_curr(p))
+	if ((cpu != smp_processor_id()) && xsched_task_curr(p))
 		smp_send_reschedule(cpu);
 	preempt_enable();
 }
 
-EXPORT_SYMBOL_GPL(kick_process);
-
 /*
  * Return a low guess at the load of a migration-source cpu.
  *
@@ -1159,15 +1144,13 @@
 	return success;
 }
 
-int fastcall wake_up_process(task_t * p)
+int fastcall xsched_wake_up_process(task_t * p)
 {
 	return try_to_wake_up(p, TASK_STOPPED | TASK_TRACED |
 		       		 TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE, 0);
 }
 
-EXPORT_SYMBOL(wake_up_process);
-
-int fastcall wake_up_state(task_t *p, unsigned int state)
+int fastcall xsched_wake_up_state(task_t *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
 }
@@ -1181,7 +1164,7 @@
  * Perform scheduler related setup for a newly forked process p.
  * p is forked by current.
  */
-void fastcall sched_fork(task_t *p)
+void fastcall xsched_sched_fork(task_t *p)
 {
 	/*
 	 * We mark the process as running here, but have not actually
@@ -1207,14 +1190,14 @@
 	/*
 	 * Get MIN_HISTORY of history with the same sleep_avg as parent.
 	 */
-	p->sleep_avg = current->sleep_avg;
-	p->total_time = MIN_HISTORY;
-	p->sleep_time = p->total_time * p->sleep_avg / sleep_factor;
+	p->xsched_data.sleep_avg = current->xsched_data.sleep_avg;
+	p->xsched_data.total_time = MIN_HISTORY;
+	p->xsched_data.sleep_time = p->xsched_data.total_time * p->xsched_data.sleep_avg / sleep_factor;
 
 	/*
 	 * Parent loses 1/4 sleep_time for forking.
 	 */
-	current->sleep_time = 3*current->sleep_time / 4;
+	current->xsched_data.sleep_time = 3*current->xsched_data.sleep_time / 4;
 }
 
 /*
@@ -1224,7 +1207,7 @@
  * that must be done for every newly created context, then puts the task
  * on the runqueue and wakes it.
  */
-void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
+void fastcall xsched_wake_up_new_task(task_t * p, unsigned long clone_flags)
 {
 	unsigned long flags;
 	int this_cpu, cpu;
@@ -1237,7 +1220,7 @@
 	cpu = task_cpu(p);
 	this_cpu = smp_processor_id();
 
-	p->prio = task_priority(p);
+	p->xsched_data.prio = task_priority(p);
 	current->flags |= PF_FORKED;
 
 	if (likely(cpu == this_cpu)) {
@@ -1247,8 +1230,8 @@
 		 	 * to make sure that its one and only fork() doesn't end up in the idle
 		 	 * priority slot.  Just testing for empty run list is no longer adequate.
 		 	 */
-			if (p->prio >= current->prio && !RUNQUEUE_IDLE(rq)) {
-				p->prio = current->prio;
+			if (p->xsched_data.prio >= current->xsched_data.prio && !RUNQUEUE_IDLE(rq)) {
+				p->xsched_data.prio = current->xsched_data.prio;
 				list_add_tail(&p->run_list, &current->run_list);
 				rq->nr_running++;
 			} else
@@ -1301,17 +1284,17 @@
  * artificially, because any timeslice recovered here
  * was given away by the parent in the first place.)
  */
-void fastcall sched_exit(task_t * p)
+void fastcall xsched_sched_exit(task_t * p)
 {
 	task_t *parent = p->parent;
 	unsigned long flags;
 	runqueue_t *rq;
 
 	rq = task_rq_lock(p->parent, &flags);
-	parent->sleep_avg += 7*p->sleep_avg / 8;
-	if (parent->sleep_avg > sleep_factor)
-		parent->sleep_avg = sleep_factor;
-	parent->sleep_time = parent->sleep_avg * parent->total_time / sleep_factor;
+	parent->xsched_data.sleep_avg += 7*p->xsched_data.sleep_avg / 8;
+	if (parent->xsched_data.sleep_avg > sleep_factor)
+		parent->xsched_data.sleep_avg = sleep_factor;
+	parent->xsched_data.sleep_time = parent->xsched_data.sleep_avg * parent->xsched_data.total_time / sleep_factor;
 	task_rq_unlock(rq, &flags);
 }
 
@@ -1359,7 +1342,7 @@
  * schedule_tail - first thing a freshly forked thread must call.
  * @prev: the thread we just switched away from.
  */
-asmlinkage void schedule_tail(task_t *prev)
+asmlinkage void xsched_schedule_tail(task_t *prev)
 {
 	finish_task_switch(prev);
 
@@ -1403,7 +1386,7 @@
  * threads, current number of uninterruptible-sleeping threads, total
  * number of context switches performed since bootup.
  */
-unsigned long nr_running(void)
+unsigned long xsched_nr_running(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1413,7 +1396,7 @@
 	return sum;
 }
 
-unsigned long nr_uninterruptible(void)
+unsigned long xsched_nr_uninterruptible(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1423,7 +1406,7 @@
 	return sum;
 }
 
-unsigned long long nr_context_switches(void)
+unsigned long long xsched_nr_context_switches(void)
 {
 	unsigned long long i, sum = 0;
 
@@ -1433,7 +1416,7 @@
 	return sum;
 }
 
-unsigned long nr_iowait(void)
+unsigned long xsched_nr_iowait(void)
 {
 	unsigned long i, sum = 0;
 
@@ -1563,7 +1546,7 @@
 		struct task_struct *mt = rq->migration_thread;
 		get_task_struct(mt);
 		task_rq_unlock(rq, &flags);
-		wake_up_process(mt);
+		xsched_wake_up_process(mt);
 		put_task_struct(mt);
 		wait_for_completion(&req.done);
 		return;
@@ -1579,7 +1562,7 @@
  * execve() is a valuable balancing opportunity, because at this point
  * the task has the smallest effective memory and cache footprint.
  */
-void sched_exec(void)
+void xsched_sched_exec(void)
 {
 	struct sched_domain *tmp, *sd = NULL;
 	int new_cpu, this_cpu = get_cpu();
@@ -1683,8 +1666,8 @@
 	if (!idx)
 		idx = sched_find_first_bit(busiest->bitmap);
 	else
-		idx = find_next_bit(busiest->bitmap, IDLE_PRIO, idx);
-	if (idx >= IDLE_PRIO)
+		idx = find_next_bit(busiest->bitmap, NICK_MAX_PRIO, idx);
+	if (idx >= NICK_MAX_PRIO)
 		goto out;
 
 	head = &busiest->queues[idx].queue;
@@ -1953,7 +1936,7 @@
 			}
 			spin_unlock(&busiest->lock);
 			if (wake)
-				wake_up_process(busiest->migration_thread);
+				xsched_wake_up_process(busiest->migration_thread);
 
 			/*
 			 * We've kicked active balancing, reset the failure
@@ -2197,10 +2180,6 @@
 	return 0;
 }
 
-DEFINE_PER_CPU(struct kernel_stat, kstat);
-
-EXPORT_PER_CPU_SYMBOL(kstat);
-
 /*
  * This function gets called by the timer code, with HZ frequency.
  * We call it with interrupts disabled.
@@ -2208,7 +2187,7 @@
  * It also gets called by the fork code, when changing the parent's
  * timeslices.
  */
-void scheduler_tick(int user_ticks, int sys_ticks)
+void xsched_scheduler_tick(int user_ticks, int sys_ticks)
 {
 	enum idle_type cpu_status;
 	int cpu = smp_processor_id();
@@ -2244,7 +2223,7 @@
  		goto out;
 	}
 
-	if (TASK_NICE(p) > 0 || batch_task(p))
+	if (TASK_NICE(p) > 0)
 		cpustat->nice += user_ticks;
 	else
 		cpustat->user += user_ticks;
@@ -2282,13 +2261,13 @@
 		dequeue_task(p);
 		set_tsk_need_resched(p);
 		rq->current_prio_slot = rq->queues + task_priority(p);
-		p->prio = rq->current_prio_slot->prio;
+		p->xsched_data.prio = rq->current_prio_slot->prio;
 		update_min_prio(p, rq);
 		p->flags &= ~PF_FORKED;
 		goto out_unlock;
 	} else {
 		/* Attempt to shatter the slice if it's interactive */
-		if (TASK_INTERACTIVE(p) && (p->mm != NULL) && sched_interactive &&
+		if (TASK_INTERACTIVE(p) && (p->mm != NULL) && xsched_sched_interactive &&
 				!(p->flags & (PF_UISLEEP | PF_FORKED | PF_YIELDED)))
 			timeslice_shatter(p, rq);
 	}
@@ -2388,8 +2367,8 @@
 		 * task from using an unfair proportion of the
 		 * physical cpu's resources. -ck
 		 */
-		if ((smt_curr->static_prio + 5 < p->static_prio) &&
-			p->mm && smt_curr->mm && !rt_task(p))
+		if ((smt_curr->xsched_data.static_prio + 5 < p->xsched_data.static_prio) &&
+			p->mm && smt_curr->mm && !xsched_rt_task(p))
 				ret = 1;
 
 		/*
@@ -2397,8 +2376,8 @@
 		 * or wake it up if it has been put to sleep for priority
 		 * reasons.
 		 */
-		if ((p->static_prio + 5 < smt_curr->static_prio &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
+		if ((p->xsched_data.static_prio + 5 < smt_curr->xsched_data.static_prio &&
+			smt_curr->mm && p->mm && !xsched_rt_task(smt_curr)) ||
 			(smt_curr == smt_rq->idle && smt_rq->nr_running))
 				resched_task(smt_curr);
 	}
@@ -2418,91 +2397,6 @@
 }
 #endif
 
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
-
-void fastcall add_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(((int)preempt_count() < 0));
-	preempt_count() += val;
-	/*
-	 * Spinlock count overflowing soon?
-	 */
-	BUG_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
-}
-EXPORT_SYMBOL(add_preempt_count);
-
-void fastcall sub_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	BUG_ON(val > preempt_count());
-	/*
-	 * Is the spinlock portion underflowing?
-	 */
-	BUG_ON((val < PREEMPT_MASK) && !(preempt_count() & PREEMPT_MASK));
-	preempt_count() -= val;
-}
-EXPORT_SYMBOL(sub_preempt_count);
-
-#ifdef __smp_processor_id
-/*
- * Debugging check.
- */
-unsigned int smp_processor_id(void)
-{
-	unsigned long preempt_count = preempt_count();
-	int this_cpu = __smp_processor_id();
-	cpumask_t this_mask;
-
-	if (likely(preempt_count))
-		goto out;
-
-	if (irqs_disabled())
-		goto out;
-
-	/*
-	 * Kernel threads bound to a single CPU can safely use
-	 * smp_processor_id():
-	 */
-	this_mask = cpumask_of_cpu(this_cpu);
-
-	if (cpus_equal(current->cpus_allowed, this_mask))
-		goto out;
-
-	/*
-	 * It is valid to assume CPU-locality during early bootup:
-	 */
-	if (system_state != SYSTEM_RUNNING)
-		goto out;
-
-	/*
-	 * Avoid recursion:
-	 */
-	preempt_disable();
-
-	if (!printk_ratelimit())
-		goto out_enable;
-
-	printk(KERN_ERR "using smp_processor_id() in preemptible code: %s/%d\n",
-		current->comm, current->pid);
-	dump_stack();
-
-out_enable:
-	preempt_enable_no_resched();
-out:
-	return this_cpu;
-}
-
-EXPORT_SYMBOL(smp_processor_id);
-
-#endif /* __smp_processor_id */
-
-#endif /* PREEMPT && DEBUG_PREEMPT */
-
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 
 #ifdef CONFIG_PREEMPT_BKL
@@ -2522,13 +2416,11 @@
  */
 static __cacheline_aligned_in_smp DECLARE_MUTEX(kernel_sem);
 
-int kernel_locked(void)
+int xsched_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
-
 /*
  * Release global kernel semaphore:
  */
@@ -2567,7 +2459,7 @@
 /*
  * Getting the big kernel semaphore.
  */
-void lock_kernel(void)
+void xsched_lock_kernel(void)
 {
 	struct task_struct *task = current;
 	int depth = task->lock_depth + 1;
@@ -2581,9 +2473,7 @@
 	task->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
-
-void unlock_kernel(void)
+void xsched_unlock_kernel(void)
 {
 	struct task_struct *task = current;
 
@@ -2593,19 +2483,15 @@
 		up(&kernel_sem);
 }
 
-EXPORT_SYMBOL(unlock_kernel);
-
 #else
 
 static spinlock_t kernel_flag = SPIN_LOCK_UNLOCKED;
 
-int kernel_locked(void)
+int xsched_kernel_locked(void)
 {
 	return current->lock_depth >= 0;
 }
 
-EXPORT_SYMBOL(kernel_locked);
-
 #define get_kernel_lock()	spin_lock(&kernel_flag)
 #define put_kernel_lock()	spin_unlock(&kernel_flag)
 
@@ -2634,7 +2520,7 @@
  * so we only need to worry about other
  * CPU's.
  */
-void lock_kernel(void)
+void xsched_lock_kernel(void)
 {
 	int depth = current->lock_depth+1;
 	if (likely(!depth))
@@ -2642,17 +2528,13 @@
 	current->lock_depth = depth;
 }
 
-EXPORT_SYMBOL(lock_kernel);
-
-void unlock_kernel(void)
+void xsched_unlock_kernel(void)
 {
 	BUG_ON(current->lock_depth < 0);
 	if (likely(--current->lock_depth < 0))
 		put_kernel_lock();
 }
 
-EXPORT_SYMBOL(unlock_kernel);
-
 #endif
 
 #else
@@ -2666,7 +2548,7 @@
 /*
  * schedule() is the main scheduler function.
  */
-asmlinkage void __sched schedule(void)
+asmlinkage void __sched xsched_schedule(void)
 {
 	long *switch_count;
 	task_t *prev, *next;
@@ -2733,13 +2615,13 @@
 
 	if (unlikely(task_timeslice(prev, rq) <= 1)) {
 		set_tsk_need_resched(prev);
-		if (rt_task(prev)) {
+		if (xsched_rt_task(prev)) {
 			list_del_init(&prev->run_list);
 			list_add_tail(&prev->run_list, &rq->current_prio_slot->queue);
 		} else {
 			dequeue_task(prev);
-			prev->prio = task_priority(prev);
-			rq->current_prio_slot = rq->queues + prev->prio;
+			prev->xsched_data.prio = task_priority(prev);
+			rq->current_prio_slot = rq->queues + prev->xsched_data.prio;
 			enqueue_task(prev, rq);
 			update_min_prio(prev, rq);
 		}
@@ -2751,8 +2633,8 @@
 go_idle:
 		idle_balance(cpu, rq);
 		if (!rq->nr_running) {
-			rq->min_prio = IDLE_PRIO;
-			rq->min_nice = IDLE_PRIO;
+			rq->min_prio = NICK_MAX_PRIO;
+			rq->min_nice = NICK_MAX_PRIO;
  			next = rq->idle;
  			wake_sleeping_dependent(cpu, rq);
 			/*
@@ -2788,15 +2670,15 @@
 
 	if (next->flags & PF_YIELDED) {
 		next->flags &= ~PF_YIELDED;
-		if (rt_task(next)) {
+		if (xsched_rt_task(next)) {
 			if (next->policy == SCHED_RR) {
 				list_del_init(&next->run_list);
 				list_add(&next->run_list, &rq->current_prio_slot->queue);
 			}
 		} else {
 			dequeue_task(next);
-			next->prio = task_priority(next);
-			rq->current_prio_slot = rq->queues + next->prio;
+			next->xsched_data.prio = task_priority(next);
+			rq->current_prio_slot = rq->queues + next->xsched_data.prio;
 			enqueue_task_head(next, rq);
 			update_min_prio(next, rq);
 		}
@@ -2826,15 +2708,13 @@
 		goto need_resched;
 }
 
-EXPORT_SYMBOL(schedule);
-
 #ifdef CONFIG_PREEMPT
 /*
  * this is is the entry point to schedule() from in-kernel preemption
  * off of preempt_enable.  Kernel preemptions off return from interrupt
  * occur there and call schedule directly.
  */
-asmlinkage void __sched preempt_schedule(void)
+asmlinkage void __sched xsched_preempt_schedule(void)
 {
 	struct thread_info *ti = current_thread_info();
 #ifdef CONFIG_PREEMPT_BKL
@@ -2862,7 +2742,7 @@
 	saved_lock_depth = task->lock_depth;
 	task->lock_depth = -1;
 #endif
-	schedule();
+	xsched_schedule();
 #ifdef CONFIG_PREEMPT_BKL
 	task->lock_depth = saved_lock_depth;
 #endif
@@ -2874,17 +2754,14 @@
 		goto need_resched;
 }
 
-EXPORT_SYMBOL(preempt_schedule);
 #endif /* CONFIG_PREEMPT */
 
-int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
+int xsched_default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
 {
 	task_t *p = curr->task;
 	return try_to_wake_up(p, mode, sync);
 }
 
-EXPORT_SYMBOL(default_wake_function);
-
 /*
  * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
  * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve
@@ -2917,7 +2794,7 @@
  * @mode: which threads
  * @nr_exclusive: how many wake-one or wake-many threads to wake up
  */
-void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
+void fastcall xsched___wake_up(wait_queue_head_t *q, unsigned int mode,
 				int nr_exclusive, void *key)
 {
 	unsigned long flags;
@@ -2927,12 +2804,10 @@
 	spin_unlock_irqrestore(&q->lock, flags);
 }
 
-EXPORT_SYMBOL(__wake_up);
-
 /*
  * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
  */
-void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
+void fastcall xsched___wake_up_locked(wait_queue_head_t *q, unsigned int mode)
 {
 	__wake_up_common(q, mode, 1, 0, NULL);
 }
@@ -2950,7 +2825,7 @@
  *
  * On UP it can prevent extra preemption.
  */
-void fastcall __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
+void fastcall xsched___wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
 {
 	unsigned long flags;
 	int sync = 1;
@@ -2965,9 +2840,8 @@
 	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
-EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
 
-void fastcall complete(struct completion *x)
+void fastcall xsched_complete(struct completion *x)
 {
 	unsigned long flags;
 
@@ -2977,9 +2851,8 @@
 			 1, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete);
 
-void fastcall complete_all(struct completion *x)
+void fastcall xsched_complete_all(struct completion *x)
 {
 	unsigned long flags;
 
@@ -2989,9 +2862,8 @@
 			 0, 0, NULL);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
-EXPORT_SYMBOL(complete_all);
 
-void fastcall __sched wait_for_completion(struct completion *x)
+void fastcall __sched xsched_wait_for_completion(struct completion *x)
 {
 	might_sleep();
 	spin_lock_irq(&x->wait.lock);
@@ -3003,7 +2875,7 @@
 		do {
 			__set_current_state(TASK_UNINTERRUPTIBLE);
 			spin_unlock_irq(&x->wait.lock);
-			schedule();
+			xsched_schedule();
 			spin_lock_irq(&x->wait.lock);
 		} while (!x->done);
 		__remove_wait_queue(&x->wait, &wait);
@@ -3011,7 +2883,6 @@
 	x->done--;
 	spin_unlock_irq(&x->wait.lock);
 }
-EXPORT_SYMBOL(wait_for_completion);
 
 #define	SLEEP_ON_VAR					\
 	unsigned long flags;				\
@@ -3028,20 +2899,18 @@
 	__remove_wait_queue(q, &wait);			\
 	spin_unlock_irqrestore(&q->lock, flags);
 
-void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+void fastcall __sched xsched_interruptible_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_INTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	xsched_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on);
-
-long fastcall __sched interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched xsched_interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -3054,22 +2923,18 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(interruptible_sleep_on_timeout);
-
-void fastcall __sched sleep_on(wait_queue_head_t *q)
+void fastcall __sched xsched_sleep_on(wait_queue_head_t *q)
 {
 	SLEEP_ON_VAR
 
 	current->state = TASK_UNINTERRUPTIBLE;
 
 	SLEEP_ON_HEAD
-	schedule();
+	xsched_schedule();
 	SLEEP_ON_TAIL
 }
 
-EXPORT_SYMBOL(sleep_on);
-
-long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long fastcall __sched xsched_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
 	SLEEP_ON_VAR
 
@@ -3082,9 +2947,7 @@
 	return timeout;
 }
 
-EXPORT_SYMBOL(sleep_on_timeout);
-
-void set_user_nice(task_t *p, long nice)
+void xsched_set_user_nice(task_t *p, long nice)
 {
 	unsigned long flags;
 	runqueue_t *rq;
@@ -3103,31 +2966,31 @@
 	 * it wont have any effect on scheduling until the task is
 	 * not SCHED_NORMAL:
 	 */
-	if (rt_task(p)) {
-		p->static_prio = NICE_TO_PRIO(nice);
+	if (xsched_rt_task(p)) {
+		p->xsched_data.static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
-	if ((queued = (!rt_task(p) && task_queued(p))))
+	if ((queued = (!xsched_rt_task(p) && task_queued(p))))
 		dequeue_task(p);
 
-	old_prio = p->prio;
+	old_prio = p->xsched_data.prio;
 	new_prio = NICE_TO_PRIO(nice);
 	delta = new_prio - old_prio;
-	p->static_prio = NICE_TO_PRIO(nice);
-	p->prio += delta;
+	p->xsched_data.static_prio = NICE_TO_PRIO(nice);
+	p->xsched_data.prio += delta;
 
 	if (queued) {
-		p->prio = task_priority(p);
+		p->xsched_data.prio = task_priority(p);
 		enqueue_task(p, rq);
 		update_min_prio(p, rq);
 		if (task_running(rq, p))
-			rq->current_prio_slot = rq->queues + p->prio;
+			rq->current_prio_slot = rq->queues + p->xsched_data.prio;
 
 		/*
 		 * If the task increased its setting or is running and lowered
 		 * its setting, then reschedule its CPU:
 		 */
-		if (delta < 0 || ((delta > 0 || batch_task(p)) &&
+		if (delta < 0 || (delta > 0 &&
 			task_running(rq, p)))
 				resched_task(rq->curr);
 			resched_task(rq->curr);
@@ -3136,8 +2999,6 @@
 	task_rq_unlock(rq, &flags);
 }
 
-EXPORT_SYMBOL(set_user_nice);
-
 #ifdef CONFIG_KGDB
 struct task_struct *kgdb_get_idle(int this_cpu)
 {
@@ -3154,7 +3015,7 @@
  * sys_setpriority is a more generic, but much slower function that
  * does similar things.
  */
-asmlinkage long sys_nice(int increment)
+asmlinkage long xsched_sys_nice(int increment)
 {
 	int retval;
 	long nice;
@@ -3173,7 +3034,7 @@
 	if (increment > 40)
 		increment = 40;
 
-	nice = PRIO_TO_NICE(current->static_prio) + increment;
+	nice = PRIO_TO_NICE(current->xsched_data.static_prio) + increment;
 	if (nice < -20)
 		nice = -20;
 	if (nice > 19)
@@ -3183,7 +3044,7 @@
 	if (retval)
 		return retval;
 
-	set_user_nice(current, nice);
+	xsched_set_user_nice(current, nice);
 	return 0;
 }
 
@@ -3197,33 +3058,31 @@
  * RT tasks are offset by -200. Normal tasks are centered
  * around 0, value goes from -16 to +15.
  */
-int task_prio(const task_t *p)
+int xsched_task_prio(const task_t *p)
 {
-	return p->prio - MAX_RT_PRIO;
+	return p->xsched_data.prio - MAX_RT_PRIO;
 }
 
 /**
  * task_nice - return the nice value of a given task.
  * @p: the task in question.
  */
-int task_nice(const task_t *p)
+int xsched_task_nice(const task_t *p)
 {
 	return TASK_NICE(p);
 }
 
-EXPORT_SYMBOL(task_nice);
+EXPORT_SYMBOL(xsched_task_nice);
 
 /**
  * idle_cpu - is a given cpu idle currently?
  * @cpu: the processor in question.
  */
-int idle_cpu(int cpu)
+int xsched_idle_cpu(int cpu)
 {
 	return cpu_curr(cpu) == cpu_rq(cpu)->idle;
 }
 
-EXPORT_SYMBOL_GPL(idle_cpu);
-
 /**
  * find_process_by_pid - find a process with a matching PID value.
  * @pid: the pid in question.
@@ -3239,10 +3098,10 @@
 	BUG_ON(task_queued(p));
 	p->policy = policy;
 	p->rt_priority = prio;
-	if (SCHED_RT(policy))
-		p->prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
+	if (policy != SCHED_NORMAL)
+		p->xsched_data.prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
 	else
-		p->prio = p->static_prio;
+		p->xsched_data.prio = p->xsched_data.static_prio;
 }
 
 /*
@@ -3286,7 +3145,8 @@
 		policy = p->policy;
 	else {
 		retval = -EINVAL;
-		if (!SCHED_RANGE(policy))
+		if (policy != SCHED_FIFO && policy != SCHED_RR &&
+				policy != SCHED_NORMAL)
 			goto out_unlock;
 	}
 	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
@@ -3298,17 +3158,15 @@
 	retval = -EINVAL;
 	if (lp.sched_priority < 0 || lp.sched_priority > MAX_USER_RT_PRIO-1)
 		goto out_unlock;
-	if (!SCHED_RT(policy) != (lp.sched_priority == 0))
+	if ((policy == SCHED_NORMAL) != (lp.sched_priority == 0))
 		goto out_unlock;
 
 	retval = -EPERM;
-	if (SCHED_RT(policy) && !capable(CAP_SYS_NICE))
-		policy = SCHED_ISO;
-	if ((current->euid != p->euid) && (current->euid != p->uid) &&
+	if ((policy == SCHED_FIFO || policy == SCHED_RR) &&
 	    !capable(CAP_SYS_NICE))
 		goto out_unlock;
-
-	if (!(p->mm) && policy == SCHED_BATCH)
+	if ((current->euid != p->euid) && (current->euid != p->uid) &&
+	    !capable(CAP_SYS_NICE))
 		goto out_unlock;
 
 	retval = security_task_setscheduler(p, policy, &lp);
@@ -3318,7 +3176,7 @@
  	if ((queued = task_queued(p)))
 		deactivate_task(p, task_rq(p));
 	retval = 0;
-	oldprio = p->prio;
+	oldprio = p->xsched_data.prio;
 	__setscheduler(p, policy, lp.sched_priority);
 	if (policy == SCHED_FIFO || policy == SCHED_RR)
 		set_tsk_need_resched(p);
@@ -3331,11 +3189,11 @@
 		 * this runqueue and our priority is higher than the current's
 		 */
 		if (task_running(rq, p)) {
-			if (p->prio > oldprio)
+			if (p->xsched_data.prio > oldprio)
 				resched_task(rq->curr);
 		} else if (task_preempts_curr(p, rq))
 			resched_task(rq->curr);
-		rq->current_prio_slot = rq->queues + p->prio;
+		rq->current_prio_slot = rq->queues + p->xsched_data.prio;
 	}
 
 out_unlock:
@@ -3353,7 +3211,7 @@
  * @policy: new policy
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
+asmlinkage long xsched_sys_sched_setscheduler(pid_t pid, int policy,
 				       struct sched_param __user *param)
 {
 	return setscheduler(pid, policy, param);
@@ -3364,7 +3222,7 @@
  * @pid: the pid in question.
  * @param: structure containing the new RT priority.
  */
-asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long xsched_sys_sched_setparam(pid_t pid, struct sched_param __user *param)
 {
 	return setscheduler(pid, -1, param);
 }
@@ -3373,7 +3231,7 @@
  * sys_sched_getscheduler - get the policy (scheduling class) of a thread
  * @pid: the pid in question.
  */
-asmlinkage long sys_sched_getscheduler(pid_t pid)
+asmlinkage long xsched_sys_sched_getscheduler(pid_t pid)
 {
 	int retval = -EINVAL;
 	task_t *p;
@@ -3400,7 +3258,7 @@
  * @pid: the pid in question.
  * @param: structure containing the RT priority.
  */
-asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
+asmlinkage long xsched_sys_sched_getparam(pid_t pid, struct sched_param __user *param)
 {
 	struct sched_param lp;
 	int retval = -EINVAL;
@@ -3435,7 +3293,7 @@
 	return retval;
 }
 
-long sched_setaffinity(pid_t pid, cpumask_t new_mask)
+long xsched_sched_setaffinity(pid_t pid, cpumask_t new_mask)
 {
 	task_t *p;
 	int retval;
@@ -3474,52 +3332,7 @@
 	return retval;
 }
 
-static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
-			     cpumask_t *new_mask)
-{
-	if (len < sizeof(cpumask_t)) {
-		memset(new_mask, 0, sizeof(cpumask_t));
-	} else if (len > sizeof(cpumask_t)) {
-		len = sizeof(cpumask_t);
-	}
-	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
-}
-
-/**
- * sys_sched_setaffinity - set the cpu affinity of a process
- * @pid: pid of the process
- * @len: length in bytes of the bitmask pointed to by user_mask_ptr
- * @user_mask_ptr: user-space pointer to the new cpu mask
- */
-asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
-				      unsigned long __user *user_mask_ptr)
-{
-	cpumask_t new_mask;
-	int retval;
-
-	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
-	if (retval)
-		return retval;
-
-	return sched_setaffinity(pid, new_mask);
-}
-
-/*
- * Represents all cpu's present in the system
- * In systems capable of hotplug, this map could dynamically grow
- * as new cpu's are detected in the system via any platform specific
- * method, such as ACPI for e.g.
- */
-
-cpumask_t cpu_present_map;
-EXPORT_SYMBOL(cpu_present_map);
-
-#ifndef CONFIG_SMP
-cpumask_t cpu_online_map = CPU_MASK_ALL;
-cpumask_t cpu_possible_map = CPU_MASK_ALL;
-#endif
-
-long sched_getaffinity(pid_t pid, cpumask_t *mask)
+long xsched_sched_getaffinity(pid_t pid, cpumask_t *mask)
 {
 	int retval;
 	task_t *p;
@@ -3550,7 +3363,7 @@
  * @len: length in bytes of the bitmask pointed to by user_mask_ptr
  * @user_mask_ptr: user-space pointer to hold the current cpu mask
  */
-asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
+asmlinkage long xsched_sys_sched_getaffinity(pid_t pid, unsigned int len,
 				      unsigned long __user *user_mask_ptr)
 {
 	int ret;
@@ -3576,11 +3389,11 @@
  * to the expired array. If there are no other threads running on this
  * CPU then this function will return.
  */
-asmlinkage long sys_sched_yield(void)
+asmlinkage long xsched_sys_sched_yield(void)
 {
 	runqueue_t *rq = this_rq_lock();
 
-	if (likely(!rt_task(current))) {
+	if (likely(!xsched_rt_task(current))) {
 		int idx;
 
 		/* If there's other tasks on this CPU make sure that at least
@@ -3591,10 +3404,10 @@
 		 */
 		dequeue_task(current);
 		current->flags |= PF_YIELDED;
-		current->prio = task_priority(current);
-		idx = find_next_bit(rq->bitmap, IDLE_PRIO, current->prio);
-		if (idx < IDLE_PRIO)
-			current->prio = idx;
+		current->xsched_data.prio = task_priority(current);
+		idx = find_next_bit(rq->bitmap, NICK_MAX_PRIO, current->xsched_data.prio);
+		if (idx < NICK_MAX_PRIO)
+			current->xsched_data.prio = idx;
 		enqueue_task(current, rq);
 	} else {
 		list_del_init(&current->run_list);
@@ -3609,91 +3422,21 @@
 	_raw_spin_unlock(&rq->lock);
 	preempt_enable_no_resched();
 
-	schedule();
-
-	return 0;
-}
-
-static inline void __cond_resched(void)
-{
-
-
-	if (preempt_count() & PREEMPT_ACTIVE)
-		return;
-	do {
-		add_preempt_count(PREEMPT_ACTIVE);
-		schedule();
-		sub_preempt_count(PREEMPT_ACTIVE);
-	} while (need_resched());
-}
-
-int __sched cond_resched(void)
-{
-	if (need_resched()) {
-		__cond_resched();
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched);
-
-/*
- * cond_resched_lock() - if a reschedule is pending, drop the given lock,
- * call schedule, and on return reacquire the lock.
- *
- * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
- * operations here to prevent schedule() from being called twice (once via
- * spin_unlock(), once by hand).
- */
-int cond_resched_lock(spinlock_t * lock)
-{
-#if defined(CONFIG_SMP) && defined(CONFIG_PREEMPT)
-	if (lock->break_lock) {
-		lock->break_lock = 0;
-		spin_unlock(lock);
-		cpu_relax();
-		spin_lock(lock);
-	}
-#endif
-	if (need_resched()) {
-		_raw_spin_unlock(lock);
-		preempt_enable_no_resched();
-		__cond_resched();
-		spin_lock(lock);
-		return 1;
-	}
-	return 0;
-}
-
-EXPORT_SYMBOL(cond_resched_lock);
+	xsched_schedule();
 
-int __sched cond_resched_softirq(void)
-{
-	BUG_ON(!in_softirq());
-
-	if (need_resched()) {
-		__local_bh_enable();
-		__cond_resched();
-		local_bh_disable();
-		return 1;
-	}
 	return 0;
 }
 
-EXPORT_SYMBOL(cond_resched_softirq);
-
-
 /**
  * yield - yield the current processor to other threads.
  *
  * this is a shortcut for kernel-space yielding - it marks the
  * thread runnable and calls sys_sched_yield().
  */
-void __sched yield(void)
+void __sched xsched_yield(void)
 {
 	set_current_state(TASK_RUNNING);
-	sys_sched_yield();
+	xsched_sys_sched_yield();
 }
 
 EXPORT_SYMBOL(yield);
@@ -3705,18 +3448,18 @@
  * But don't do that if it is a deliberate, throttling IO wait (this task
  * has set its backing_dev_info: the queue against which it should throttle)
  */
-void __sched io_schedule(void)
+void __sched xsched_io_schedule(void)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 
 	atomic_inc(&rq->nr_iowait);
-	schedule();
+	xsched_schedule();
 	atomic_dec(&rq->nr_iowait);
 }
 
-EXPORT_SYMBOL(io_schedule);
+EXPORT_SYMBOL(xsched_io_schedule);
 
-long __sched io_schedule_timeout(long timeout)
+long __sched xsched_io_schedule_timeout(long timeout)
 {
 	struct runqueue *rq = &per_cpu(runqueues, _smp_processor_id());
 	long ret;
@@ -3728,55 +3471,6 @@
 }
 
 /**
- * sys_sched_get_priority_max - return maximum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the maximum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_max(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = MAX_USER_RT_PRIO-1;
-		break;
-	case SCHED_NORMAL:
-	case SCHED_BATCH:
-	case SCHED_ISO:
-		ret = 0;
-		break;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_get_priority_min - return minimum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the minimum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_min(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = 1;
-		break;
-	case SCHED_NORMAL:
-	case SCHED_BATCH:
-	case SCHED_ISO:
-		ret = 0;
-	}
-	return ret;
-}
-
-/**
  * sys_sched_rr_get_interval - return the default timeslice of a process.
  * @pid: pid of the process.
  * @interval: userspace pointer to the timeslice value.
@@ -3785,7 +3479,7 @@
  * into the user-space timespec buffer. A value of '0' means infinity.
  */
 asmlinkage
-long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
+long xsched_sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
 {
 	int retval = -EINVAL;
 	struct timespec t;
@@ -3818,111 +3512,13 @@
 	return retval;
 }
 
-static inline struct task_struct *eldest_child(struct task_struct *p)
-{
-	if (list_empty(&p->children)) return NULL;
-	return list_entry(p->children.next,struct task_struct,sibling);
-}
-
-static inline struct task_struct *older_sibling(struct task_struct *p)
-{
-	if (p->sibling.prev==&p->parent->children) return NULL;
-	return list_entry(p->sibling.prev,struct task_struct,sibling);
-}
-
-static inline struct task_struct *younger_sibling(struct task_struct *p)
-{
-	if (p->sibling.next==&p->parent->children) return NULL;
-	return list_entry(p->sibling.next,struct task_struct,sibling);
-}
-
-static void show_task(task_t * p)
-{
-	task_t *relative;
-	unsigned state;
-	unsigned long free = 0;
-	static const char *stat_nam[] = { "R", "S", "D", "T", "t", "Z", "X" };
-
-	printk("%-13.13s ", p->comm);
-	state = p->state ? __ffs(p->state) + 1 : 0;
-	if (state < ARRAY_SIZE(stat_nam))
-		printk(stat_nam[state]);
-	else
-		printk("?");
-#if (BITS_PER_LONG == 32)
-	if (state == TASK_RUNNING)
-		printk(" running ");
-	else
-		printk(" %08lX ", thread_saved_pc(p));
-#else
-	if (state == TASK_RUNNING)
-		printk("  running task   ");
-	else
-		printk(" %016lx ", thread_saved_pc(p));
-#endif
-#ifdef CONFIG_DEBUG_STACK_USAGE
-	{
-		unsigned long * n = (unsigned long *) (p->thread_info+1);
-		while (!*n)
-			n++;
-		free = (unsigned long) n - (unsigned long)(p->thread_info+1);
-	}
-#endif
-	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
-	if ((relative = eldest_child(p)))
-		printk("%5d ", relative->pid);
-	else
-		printk("      ");
-	if ((relative = younger_sibling(p)))
-		printk("%7d", relative->pid);
-	else
-		printk("       ");
-	if ((relative = older_sibling(p)))
-		printk(" %5d", relative->pid);
-	else
-		printk("      ");
-	if (!p->mm)
-		printk(" (L-TLB)\n");
-	else
-		printk(" (NOTLB)\n");
-
-	if (state != TASK_RUNNING)
-		show_stack(p, NULL);
-}
-
-void show_state(void)
-{
-	task_t *g, *p;
-
-#if (BITS_PER_LONG == 32)
-	printk("\n"
-	       "                                               sibling\n");
-	printk("  task             PC      pid father child younger older\n");
-#else
-	printk("\n"
-	       "                                                       sibling\n");
-	printk("  task                 PC          pid father child younger older\n");
-#endif
-	read_lock(&tasklist_lock);
-	do_each_thread(g, p) {
-		/*
-		 * reset the NMI-timeout, listing all files on a slow
-		 * console might take alot of time:
-		 */
-		touch_nmi_watchdog();
-		show_task(p);
-	} while_each_thread(g, p);
-
-	read_unlock(&tasklist_lock);
-}
-
-void __devinit init_idle(task_t *idle, int cpu)
+void __devinit xsched_init_idle(task_t *idle, int cpu)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long flags;
 
-	idle->sleep_avg = 0;
-	idle->prio = IDLE_PRIO;
+	idle->xsched_data.sleep_avg = 0;
+	idle->xsched_data.prio = NICK_MAX_PRIO;
 	idle->state = TASK_RUNNING;
 	set_task_cpu(idle, cpu);
 
@@ -3939,15 +3535,6 @@
 #endif
 }
 
-/*
- * In a system that switches off the HZ timer nohz_cpu_mask
- * indicates which cpus entered this state. This is used
- * in the rcu update to wait only for active cpus. For system
- * which do not switch off the HZ timer nohz_cpu_mask should
- * always be CPU_MASK_NONE.
- */
-cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
-
 #ifdef CONFIG_SMP
 /*
  * This is how migration works:
@@ -3974,7 +3561,7 @@
  * task must not exit() & deallocate itself prematurely.  The
  * call is not atomic; no spinlocks may be held.
  */
-int set_cpus_allowed(task_t *p, cpumask_t new_mask)
+int xsched_set_cpus_allowed(task_t *p, cpumask_t new_mask)
 {
 	unsigned long flags;
 	int ret = 0;
@@ -3997,7 +3584,7 @@
 	if (migrate_task(p, any_online_cpu(new_mask), &req)) {
 		/* Need help from migration thread: drop lock and wait. */
 		task_rq_unlock(rq, &flags);
-		wake_up_process(rq->migration_thread);
+		xsched_wake_up_process(rq->migration_thread);
 		wait_for_completion(&req.done);
 		tlb_migrate_finish(p->mm);
 		return 0;
@@ -4007,8 +3594,6 @@
 	return ret;
 }
 
-EXPORT_SYMBOL_GPL(set_cpus_allowed);
-
 /*
  * Move (not current) task off this cpu, onto dest cpu.  We're doing
  * this because either it can't run here any more (set_cpus_allowed()
@@ -4099,7 +3684,7 @@
 
 		if (list_empty(head)) {
 			spin_unlock_irq(&rq->lock);
-			schedule();
+			xsched_schedule();
 			set_current_state(TASK_INTERRUPTIBLE);
 			continue;
 		}
@@ -4128,7 +3713,7 @@
 	/* Wait for kthread_stop */
 	set_current_state(TASK_INTERRUPTIBLE);
 	while (!kthread_should_stop()) {
-		schedule();
+		xsched_schedule();
 		set_current_state(TASK_INTERRUPTIBLE);
 	}
 	__set_current_state(TASK_RUNNING);
@@ -4244,7 +3829,7 @@
 	struct runqueue *rq = cpu_rq(dead_cpu);
 
 	for (arr = 0; arr < 2; arr++) {
-		for (i = 0; i < MAX_PRIO; i++) {
+		for (i = 0; i < NICK_MAX_PRIO; i++) {
 			struct list_head *list = &rq->arrays[arr].queue[i];
 			while (!list_empty(list))
 				migrate_dead(dead_cpu,
@@ -4282,7 +3867,7 @@
 		break;
 	case CPU_ONLINE:
 		/* Strictly unneccessary, as first user will wake it. */
-		wake_up_process(cpu_rq(cpu)->migration_thread);
+		xsched_wake_up_process(cpu_rq(cpu)->migration_thread);
 		break;
 #ifdef CONFIG_HOTPLUG_CPU
 	case CPU_UP_CANCELED:
@@ -4299,9 +3884,9 @@
 		/* Idle task back to normal (off runqueue, low prio) */
 		rq = task_rq_lock(rq->idle, &flags);
 		deactivate_task(rq->idle, rq);
-		rq->idle->static_prio = IDLE_PRIO;
+		rq->idle->xsched_data.static_prio = NICK_MAX_PRIO;
 		__setscheduler(rq->idle, SCHED_NORMAL, 0);
-		rq->idle->prio = IDLE_PRIO;
+		rq->idle->xsched_data.prio = NICK_MAX_PRIO;
 		enqueue_task(rq->idle, rq);
 		update_min_prio(rq->idle, rq);
 		migrate_dead_tasks(cpu);
@@ -4335,7 +3920,7 @@
 	.priority = 10
 };
 
-int __init migration_init(void)
+int __init xsched_migration_init(void)
 {
 	void *cpu = (void *)(long)smp_processor_id();
 	/* Start one for boot CPU. */
@@ -4351,7 +3936,7 @@
  * Attach the domain 'sd' to 'cpu' as its base domain.  Callers must
  * hold the hotplug lock.
  */
-void __devinit cpu_attach_domain(struct sched_domain *sd, int cpu)
+void __devinit xsched_cpu_attach_domain(struct sched_domain *sd, int cpu)
 {
 	migration_req_t req;
 	unsigned long flags;
@@ -4373,13 +3958,13 @@
 	spin_unlock_irqrestore(&rq->lock, flags);
 
 	if (!local) {
-		wake_up_process(rq->migration_thread);
+		xsched_wake_up_process(rq->migration_thread);
 		wait_for_completion(&req.done);
 	}
 }
 
 /* cpus with isolated domains */
-cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
+cpumask_t __devinitdata xsched_cpu_isolated_map = CPU_MASK_NONE;
 
 /* Setup the mask of cpus configured for isolated domains */
 static int __init isolated_cpu_setup(char *str)
@@ -4387,9 +3972,9 @@
 	int ints[NR_CPUS], i;
 
 	str = get_options(str, ARRAY_SIZE(ints), ints);
-	cpus_clear(cpu_isolated_map);
+	cpus_clear(xsched_cpu_isolated_map);
 	for (i = 1; i <= ints[0]; i++)
-		cpu_set(ints[i], cpu_isolated_map);
+		cpu_set(ints[i], xsched_cpu_isolated_map);
 	return 1;
 }
 
@@ -4406,7 +3991,7 @@
  * covered by the given span, and will set each group's ->cpumask correctly,
  * and ->cpu_power to 0.
  */
-void __devinit init_sched_build_groups(struct sched_group groups[],
+void __devinit xsched_init_sched_build_groups(struct sched_group groups[],
 			cpumask_t span, int (*group_fn)(int cpu))
 {
 	struct sched_group *first = NULL, *last = NULL;
@@ -4488,7 +4073,7 @@
 	 * For now this just excludes isolated cpus, but could be used to
 	 * exclude other special cases in the future.
 	 */
-	cpus_complement(cpu_default_map, cpu_isolated_map);
+	cpus_complement(cpu_default_map, xsched_cpu_isolated_map);
 	cpus_and(cpu_default_map, cpu_default_map, cpu_online_map);
 
 	/*
@@ -4537,7 +4122,7 @@
 		if (i != first_cpu(this_sibling_map))
 			continue;
 
-		init_sched_build_groups(sched_group_cpus, this_sibling_map,
+		xsched_init_sched_build_groups(sched_group_cpus, this_sibling_map,
 						&cpu_to_cpu_group);
 	}
 #endif
@@ -4550,13 +4135,13 @@
 		if (cpus_empty(nodemask))
 			continue;
 
-		init_sched_build_groups(sched_group_phys, nodemask,
+		xsched_init_sched_build_groups(sched_group_phys, nodemask,
 						&cpu_to_phys_group);
 	}
 
 #ifdef CONFIG_NUMA
 	/* Set up node groups */
-	init_sched_build_groups(sched_group_nodes, cpu_default_map,
+	xsched_init_sched_build_groups(sched_group_nodes, cpu_default_map,
 					&cpu_to_node_group);
 #endif
 
@@ -4592,7 +4177,7 @@
 #else
 		sd = &per_cpu(phys_domains, i);
 #endif
-		cpu_attach_domain(sd, i);
+		xsched_cpu_attach_domain(sd, i);
 	}
 }
 
@@ -4719,7 +4304,7 @@
 	case CPU_UP_PREPARE:
 	case CPU_DOWN_PREPARE:
 		for_each_online_cpu(i)
-			cpu_attach_domain(&sched_domain_dummy, i);
+			xsched_cpu_attach_domain(&sched_domain_dummy, i);
 		arch_destroy_sched_domains();
 		return NOTIFY_OK;
 
@@ -4744,7 +4329,7 @@
 }
 #endif
 
-void __init sched_init_smp(void)
+void __init xsched_sched_init_smp(void)
 {
 	lock_cpu_hotplug();
 	arch_init_sched_domains();
@@ -4754,97 +4339,14 @@
 	hotcpu_notifier(update_sched_domains, 0);
 }
 #else
-void __init sched_init_smp(void)
+void __init xsched_sched_init_smp(void)
 {
 }
 #endif /* CONFIG_SMP */
 
-int in_sched_functions(unsigned long addr)
-{
-	/* Linker adds these: start and end of __sched functions */
-	extern char __sched_text_start[], __sched_text_end[];
-	return in_lock_functions(addr) ||
-		(addr >= (unsigned long)__sched_text_start
-		&& addr < (unsigned long)__sched_text_end);
-}
-
-void __init sched_init(void)
-{
-	runqueue_t *rq;
-	int i, j;
-
-	for (i = 0; i < NR_CPUS; i++) {
-		rq = cpu_rq(i);
-		spin_lock_init(&rq->lock);
-
-		rq->cache_ticks = 0;
-		rq->preempted = 0;
-
-#ifdef CONFIG_SMP
-		rq->sd = &sched_domain_dummy;
-		rq->cpu_load = 0;
-		rq->active_balance = 0;
-		rq->push_cpu = 0;
-		rq->migration_thread = NULL;
-		INIT_LIST_HEAD(&rq->migration_queue);
-#endif
-		atomic_set(&rq->nr_iowait, 0);
-
-		rq->min_prio = IDLE_PRIO;
-		rq->min_nice = IDLE_PRIO;
-		for (j = 0; j <= IDLE_PRIO; j++) {
-			rq->queues[j].prio = j;
-			INIT_LIST_HEAD(&rq->queues[j].queue);
-		}
-		bitmap_zero(rq->bitmap, NUM_PRIO_SLOTS);
-		/*
-		 * delimiter for bitsearch
-		 */
-		__set_bit(IDLE_PRIO, rq->bitmap);
-		rq->current_prio_slot = rq->queues + (MAX_PRIO - 29);
-#ifdef CONFIG_SMP
-		rq->timestamp_last_tick = clock_us();
-#endif
-	}
-
-	/*
-	 * The boot idle thread does lazy MMU switching as well:
-	 */
-	atomic_inc(&init_mm.mm_count);
-	enter_lazy_tlb(&init_mm, current);
-
-	/*
-	 * Make us the idle thread. Technically, schedule() should not be
-	 * called from this thread, however somewhere below it might be,
-	 * but because we are the idle thread, we just pick up running again
-	 * when this runqueue becomes "idle".
-	 */
-	init_idle(current, smp_processor_id());
-}
-
-#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
-void __might_sleep(char *file, int line)
-{
-#if defined(in_atomic)
-	static unsigned long prev_jiffy;	/* ratelimiting */
-
-	if ((in_atomic() || irqs_disabled()) &&
-	    system_state == SYSTEM_RUNNING) {
-		if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
-			return;
-		prev_jiffy = jiffies;
-		printk(KERN_ERR "Debug: sleeping function called from invalid"
-				" context at %s:%d\n", file, line);
-		printk("in_atomic():%d, irqs_disabled():%d\n",
-			in_atomic(), irqs_disabled());
-		dump_stack();
-	}
-#endif
-}
-EXPORT_SYMBOL(__might_sleep);
-#endif
-
 #if defined(CONFIG_SYSCTL)
+static struct ctl_table_header *xsched_table_header;
+
 enum
 {
 	CPU_SCHED_END_OF_LIST=0,
@@ -4882,7 +4384,7 @@
 int minsleepfactor = 1;
 int maxsleepfactor = 2048;
 
-ctl_table cpu_nicksched_table[] = {
+static ctl_table cpu_nicksched_table[] = {
 	{
 		.ctl_name	= CPU_NICKSCHED_RT_TIMESLICE,
 		.procname	= "rt_timeslice",
@@ -4891,19 +4393,19 @@
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
 		.strategy	= &sysctl_intvec,
-		.extra1		= &sched_min_base,
-		.extra2		= &sched_max_base,
+		.extra1		= &xsched_sched_min_base,
+		.extra2		= &xsched_sched_max_base,
 	},
 	{
 		.ctl_name	= CPU_NICKSCHED_BASE_TIMESLICE,
 		.procname	= "base_timeslice",
-		.data		= &sched_base_timeslice,
+		.data		= &xsched_sched_base_timeslice,
 		.maxlen		= sizeof(int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
 		.strategy	= &sysctl_intvec,
-		.extra1		= &sched_min_base,
-		.extra2		= &sched_max_base,
+		.extra1		= &xsched_sched_min_base,
+		.extra2		= &xsched_sched_max_base,
 	},
 	{
 		.ctl_name	= CPU_NICKSCHED_MAX_SLEEP_SHIFT,
@@ -4963,7 +4465,7 @@
 	{ .ctl_name = CPU_NICKSCHED_END_OF_LIST }
 };
 
-ctl_table cpu_sched_table[] = {
+static ctl_table xsched_cpu_sched_table[] = {
 	{
 		.ctl_name	= CPU_NICKSCHED,
 		.procname	= "nicksched",
@@ -5015,7 +4517,7 @@
 	{
 		.ctl_name	= CPU_INTERACTIVE,
 		.procname	= "interactive",
-		.data		= &sched_interactive,
+		.data		= &xsched_sched_interactive,
 		.maxlen		= sizeof(int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
@@ -5024,7 +4526,7 @@
 	{
 		.ctl_name	= CPU_COMPUTE,
 		.procname	= "compute",
-		.data		= &sched_compute,
+		.data		= &xsched_sched_compute,
 		.maxlen		= sizeof(int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
@@ -5033,4 +4535,165 @@
 	{ .ctl_name = CPU_SCHED_END_OF_LIST }
 };
 
+static ctl_table xsched_root_table[] = {
+	{
+		.ctl_name	= CTL_SCHED,
+		.procname	= "sched",
+		.maxlen		= 0,
+		.mode		= 0555,
+		.child		= xsched_cpu_sched_table,
+	},
+	{ .ctl_name = 0 }
+};
+#endif
+
+void __init xsched_sched_init(void)
+{
+	runqueue_t *rq;
+	int i, j;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		rq = cpu_rq(i);
+		spin_lock_init(&rq->lock);
+
+		rq->cache_ticks = 0;
+		rq->preempted = 0;
+
+#ifdef CONFIG_SMP
+		rq->sd = &sched_domain_dummy;
+		rq->cpu_load = 0;
+		rq->active_balance = 0;
+		rq->push_cpu = 0;
+		rq->migration_thread = NULL;
+		INIT_LIST_HEAD(&rq->migration_queue);
+#endif
+		atomic_set(&rq->nr_iowait, 0);
+
+		rq->min_prio = NICK_MAX_PRIO;
+		rq->min_nice = NICK_MAX_PRIO;
+		for (j = 0; j <= NICK_MAX_PRIO; j++) {
+			rq->queues[j].prio = j;
+			INIT_LIST_HEAD(&rq->queues[j].queue);
+		}
+		bitmap_zero(rq->bitmap, NUM_PRIO_SLOTS);
+		/*
+		 * delimiter for bitsearch
+		 */
+		__set_bit(NICK_MAX_PRIO, rq->bitmap);
+		rq->current_prio_slot = rq->queues + (NICK_MAX_PRIO - 29);
+#ifdef CONFIG_SMP
+		rq->timestamp_last_tick = clock_us();
 #endif
+	}
+
+	printk("Using %s version %s\n", XSCHED_SCHEDULER_NAME, XSCHED_SCHEDULER_VERSION);
+
+#if defined(CONFIG_SYSCTL)
+	xsched_table_header = register_sysctl_table(xsched_root_table, 1);
+#endif
+	/*
+	 * The boot idle thread does lazy MMU switching as well:
+	 */
+	atomic_inc(&init_mm.mm_count);
+	enter_lazy_tlb(&init_mm, current);
+
+	/*
+	 * Make us the idle thread. Technically, schedule() should not be
+	 * called from this thread, however somewhere below it might be,
+	 * but because we are the idle thread, we just pick up running again
+	 * when this runqueue becomes "idle".
+	 */
+	xsched_init_idle(current, smp_processor_id());
+}
+
+#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
+void __might_sleep(char *file, int line)
+{
+#if defined(in_atomic)
+	static unsigned long prev_jiffy;	/* ratelimiting */
+
+	if ((in_atomic() || irqs_disabled()) &&
+	    system_state == SYSTEM_RUNNING) {
+		if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
+			return;
+		prev_jiffy = jiffies;
+		printk(KERN_ERR "Debug: sleeping function called from invalid"
+				" context at %s:%d\n", file, line);
+		printk("in_atomic():%d, irqs_disabled():%d\n",
+			in_atomic(), irqs_disabled());
+		dump_stack();
+	}
+#endif
+}
+EXPORT_SYMBOL(__might_sleep);
+#endif
+
+scheduler_t sched_xsched = {
+	.schedule_tail_fn = 			xsched_schedule_tail,
+	.schedule_fn =				xsched_schedule,
+	.scheduler_tick_fn = 			xsched_scheduler_tick,
+	.yield_fn = 				xsched_yield,
+	.wait_for_completion_fn	=		xsched_wait_for_completion,
+	.idle_cpu_fn = 				xsched_idle_cpu,
+	.default_wake_function_fn = 		xsched_default_wake_function,
+	.__wake_up_fn = 			xsched___wake_up,
+	.__wake_up_locked_fn = 			xsched___wake_up_locked,
+	.__wake_up_sync_fn = 			xsched___wake_up_sync,
+	.complete_fn =				xsched_complete,
+	.complete_all_fn =			xsched_complete_all,
+	.interruptible_sleep_on_fn = 		xsched_interruptible_sleep_on,
+	.interruptible_sleep_on_timeout_fn = 	xsched_interruptible_sleep_on_timeout,
+	.sleep_on_fn =				xsched_sleep_on,
+	.sleep_on_timeout_fn = 			xsched_sleep_on_timeout,
+	.set_user_nice_fn = 			xsched_set_user_nice,
+	.task_nice_fn = 			xsched_task_nice,
+	.io_schedule_fn = 			xsched_io_schedule,
+	.io_schedule_timeout_fn = 		xsched_io_schedule_timeout,
+	.task_curr_fn = 			xsched_task_curr,
+	.wake_up_process_fn = 			xsched_wake_up_process,
+	.wake_up_state_fn = 			xsched_wake_up_state,
+	.nr_running_fn = 			xsched_nr_running,
+	.nr_uninterruptible_fn = 		xsched_nr_uninterruptible,
+	.nr_iowait_fn = 			xsched_nr_iowait,
+	.nr_context_switches_fn = 		xsched_nr_context_switches,
+	.sched_exec_fn = 			xsched_sched_exec,
+	.sched_setaffinity_fn = 		xsched_sched_setaffinity,
+	.sched_getaffinity_fn = 		xsched_sched_getaffinity,
+	.sys_nice_fn = 				xsched_sys_nice,
+	.sys_sched_setscheduler_fn = 		xsched_sys_sched_setscheduler,
+	.sys_sched_setparam_fn = 		xsched_sys_sched_setparam,
+	.sys_sched_getscheduler_fn = 		xsched_sys_sched_getscheduler,
+	.sys_sched_getparam_fn = 		xsched_sys_sched_getparam,
+	.sys_sched_getaffinity_fn = 		xsched_sys_sched_getaffinity,
+	.sys_sched_yield_fn = 			xsched_sys_sched_yield,
+	.sys_sched_rr_get_interval_fn = 	xsched_sys_sched_rr_get_interval,
+	.sched_init_fn = 			xsched_sched_init,
+	.sched_init_smp_fn = 			xsched_sched_init_smp,
+	.migration_init_fn = 			xsched_migration_init,
+	.sched_fork_fn = 			xsched_sched_fork,
+	.sched_exit_fn = 			xsched_sched_exit,
+	.init_idle_fn = 			xsched_init_idle,
+	.wake_up_new_task_fn = 			xsched_wake_up_new_task,
+	.task_prio_fn = 			xsched_task_prio,
+#if defined(CONFIG_SMP)
+	.init_sched_build_groups_fn = 		xsched_init_sched_build_groups,
+	.cpu_attach_domain_fn = 		xsched_cpu_attach_domain,
+	.set_cpus_allowed_fn = 			xsched_set_cpus_allowed,
+	.wait_task_inactive_fn = 		xsched_wait_task_inactive,
+	.kick_process_fn = 			xsched_kick_process,
+#endif
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
+	.kernel_locked_fn = 			xsched_kernel_locked,
+	.lock_kernel_fn = 			xsched_lock_kernel,
+	.unlock_kernel_fn = 			xsched_unlock_kernel,
+#endif
+#if defined(CONFIG_PREEMPT)
+	.preempt_schedule_fn = 			xsched_preempt_schedule,
+#endif
+	
+	.name = 				XSCHED_SCHEDULER_NAME,
+	.version = 				XSCHED_SCHEDULER_VERSION,
+	.type = 				SCHED_XSCHED,
+};
+
+EXPORT_SYMBOL(sched_xsched);
