---

 linux-2.6.7-rc3-xx1-xiphux/include/linux/mm_inline.h  |   34 +
 linux-2.6.7-rc3-xx1-xiphux/include/linux/mmzone.h     |   29 -
 linux-2.6.7-rc3-xx1-xiphux/include/linux/page-flags.h |   61 +--
 linux-2.6.7-rc3-xx1-xiphux/include/linux/rmap.h       |    2 
 linux-2.6.7-rc3-xx1-xiphux/include/linux/swap.h       |   10 
 linux-2.6.7-rc3-xx1-xiphux/kernel/sysctl.c            |    9 
 linux-2.6.7-rc3-xx1-xiphux/mm/filemap.c               |    6 
 linux-2.6.7-rc3-xx1-xiphux/mm/hugetlb.c               |    9 
 linux-2.6.7-rc3-xx1-xiphux/mm/page-writeback.c        |    8 
 linux-2.6.7-rc3-xx1-xiphux/mm/page_alloc.c            |   27 -
 linux-2.6.7-rc3-xx1-xiphux/mm/rmap.c                  |   45 +-
 linux-2.6.7-rc3-xx1-xiphux/mm/shmem.c                 |    6 
 linux-2.6.7-rc3-xx1-xiphux/mm/swap.c                  |   58 ---
 linux-2.6.7-rc3-xx1-xiphux/mm/vmscan.c                |  338 +++++++++---------
 14 files changed, 335 insertions(+), 307 deletions(-)

diff -puN include/linux/mm_inline.h~nickvm-267r2m2 include/linux/mm_inline.h
--- linux-2.6.7-rc3-xx1/include/linux/mm_inline.h~nickvm-267r2m2	2004-06-09 06:02:55.467172392 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/include/linux/mm_inline.h	2004-06-09 06:02:55.512165552 -0400
@@ -1,9 +1,16 @@
 
 static inline void
-add_page_to_active_list(struct zone *zone, struct page *page)
+add_page_to_active_mapped_list(struct zone *zone, struct page *page)
 {
-	list_add(&page->lru, &zone->active_list);
-	zone->nr_active++;
+	list_add(&page->lru, &zone->active_mapped_list);
+	zone->nr_active_mapped++;
+}
+
+static inline void
+add_page_to_active_unmapped_list(struct zone *zone, struct page *page)
+{
+	list_add(&page->lru, &zone->active_unmapped_list);
+	zone->nr_active_unmapped++;
 }
 
 static inline void
@@ -14,10 +21,17 @@ add_page_to_inactive_list(struct zone *z
 }
 
 static inline void
-del_page_from_active_list(struct zone *zone, struct page *page)
+del_page_from_active_mapped_list(struct zone *zone, struct page *page)
+{
+	list_del(&page->lru);
+	zone->nr_active_mapped--;
+}
+
+static inline void
+del_page_from_active_unmapped_list(struct zone *zone, struct page *page)
 {
 	list_del(&page->lru);
-	zone->nr_active--;
+	zone->nr_active_unmapped--;
 }
 
 static inline void
@@ -31,10 +45,14 @@ static inline void
 del_page_from_lru(struct zone *zone, struct page *page)
 {
 	list_del(&page->lru);
-	if (PageActive(page)) {
-		ClearPageActive(page);
-		zone->nr_active--;
+	if (PageActiveMapped(page)) {
+		ClearPageActiveMapped(page);
+		zone->nr_active_mapped--;
+	} else if (PageActiveUnmapped(page)) {
+		ClearPageActiveUnmapped(page);
+		zone->nr_active_unmapped--;
 	} else {
+		ClearPageUsedOnce(page);
 		zone->nr_inactive--;
 	}
 }
diff -puN include/linux/mmzone.h~nickvm-267r2m2 include/linux/mmzone.h
--- linux-2.6.7-rc3-xx1/include/linux/mmzone.h~nickvm-267r2m2	2004-06-09 06:02:55.469172088 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/include/linux/mmzone.h	2004-06-09 06:02:55.514165248 -0400
@@ -104,11 +104,15 @@ struct zone {
 	ZONE_PADDING(_pad1_)
 
 	spinlock_t		lru_lock;	
-	struct list_head	active_list;
+	struct list_head	active_mapped_list;
+	struct list_head	active_unmapped_list;
 	struct list_head	inactive_list;
-	atomic_t		nr_scan_active;
+	atomic_t		nr_scan_active_mapped;
+	atomic_t		nr_scan_active_unmapped;
 	atomic_t		nr_scan_inactive;
-	unsigned long		nr_active;
+	atomic_t		nr_dirty_inactive;
+	unsigned long		nr_active_mapped;
+	unsigned long		nr_active_unmapped;
 	unsigned long		nr_inactive;
 	int			all_unreclaimable; /* All pages pinned */
 	unsigned long		pages_scanned;	   /* since last reclaim */
@@ -116,25 +120,6 @@ struct zone {
 	ZONE_PADDING(_pad2_)
 
 	/*
-	 * prev_priority holds the scanning priority for this zone.  It is
-	 * defined as the scanning priority at which we achieved our reclaim
-	 * target at the previous try_to_free_pages() or balance_pgdat()
-	 * invokation.
-	 *
-	 * We use prev_priority as a measure of how much stress page reclaim is
-	 * under - it drives the swappiness decision: whether to unmap mapped
-	 * pages.
-	 *
-	 * temp_priority is used to remember the scanning priority at which
-	 * this zone was successfully refilled to free_pages == pages_high.
-	 *
-	 * Access to both these fields is quite racy even on uniprocessor.  But
-	 * it is expected to average out OK.
-	 */
-	int temp_priority;
-	int prev_priority;
-
-	/*
 	 * free areas of different sizes
 	 */
 	struct free_area	free_area[MAX_ORDER];
diff -puN include/linux/page-flags.h~nickvm-267r2m2 include/linux/page-flags.h
--- linux-2.6.7-rc3-xx1/include/linux/page-flags.h~nickvm-267r2m2	2004-06-09 06:02:55.472171632 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/include/linux/page-flags.h	2004-06-09 06:02:55.519164488 -0400
@@ -58,26 +58,27 @@
 
 #define PG_dirty	 	 4
 #define PG_lru			 5
-#define PG_active		 6
-#define PG_slab			 7	/* slab debug (Suparna wants this) */
-
-#define PG_highmem		 8
-#define PG_checked		 9	/* kill me in 2.5.<early>. */
-#define PG_arch_1		10
-#define PG_reserved		11
-
-#define PG_private		12	/* Has something at ->private */
-#define PG_writeback		13	/* Page is under writeback */
-#define PG_nosave		14	/* Used for system suspend/resume */
-#define PG_maplock		15	/* Lock bit for rmap to ptes */
-
-#define PG_swapcache		16	/* Swap page: swp_entry_t in private */
-#define PG_mappedtodisk		17	/* Has blocks allocated on-disk */
-#define PG_reclaim		18	/* To be reclaimed asap */
-#define PG_compound		19	/* Part of a compound page */
-
-#define PG_anon			20	/* Anonymous: anon_vma in mapping */
+#define PG_active_mapped	 6
+#define PG_active_unmapped	 7
 
+#define PG_slab			 8	/* slab debug (Suparna wants this) */
+#define PG_highmem		 9
+#define PG_checked		10	/* kill me in 2.5.<early>. */
+#define PG_arch_1		11
+
+#define PG_reserved		12
+#define PG_private		13	/* Has something at ->private */
+#define PG_writeback		14	/* Page is under writeback */
+#define PG_nosave		15	/* Used for system suspend/resume */
+
+#define PG_maplock		16	/* Lock bit for rmap to ptes */
+#define PG_swapcache		17	/* Swap page: swp_entry_t in private */
+#define PG_mappedtodisk		18	/* Has blocks allocated on-disk */
+#define PG_reclaim		19	/* To be reclaimed asap */
+
+#define PG_compound		20	/* Part of a compound page */
+#define PG_anon			21	/* Anonymous: anon_vma in mapping */
+#define PG_usedonce		22	/* LRU page has been touched once */
 
 /*
  * Global page accounting.  One instance per CPU.  Only unsigned longs are
@@ -217,11 +218,17 @@ extern unsigned long __read_page_state(u
 #define TestSetPageLRU(page)	test_and_set_bit(PG_lru, &(page)->flags)
 #define TestClearPageLRU(page)	test_and_clear_bit(PG_lru, &(page)->flags)
 
-#define PageActive(page)	test_bit(PG_active, &(page)->flags)
-#define SetPageActive(page)	set_bit(PG_active, &(page)->flags)
-#define ClearPageActive(page)	clear_bit(PG_active, &(page)->flags)
-#define TestClearPageActive(page) test_and_clear_bit(PG_active, &(page)->flags)
-#define TestSetPageActive(page) test_and_set_bit(PG_active, &(page)->flags)
+#define PageActiveMapped(page)		test_bit(PG_active_mapped, &(page)->flags)
+#define SetPageActiveMapped(page)	set_bit(PG_active_mapped, &(page)->flags)
+#define ClearPageActiveMapped(page)	clear_bit(PG_active_mapped, &(page)->flags)
+#define TestClearPageActiveMapped(page) test_and_clear_bit(PG_active_mapped, &(page)->flags)
+#define TestSetPageActiveMapped(page) test_and_set_bit(PG_active_mapped, &(page)->flags)
+
+#define PageActiveUnmapped(page)	test_bit(PG_active_unmapped, &(page)->flags)
+#define SetPageActiveUnmapped(page)	set_bit(PG_active_unmapped, &(page)->flags)
+#define ClearPageActiveUnmapped(page)	clear_bit(PG_active_unmapped, &(page)->flags)
+#define TestClearPageActiveUnmapped(page) test_and_clear_bit(PG_active_unmapped, &(page)->flags)
+#define TestSetPageActiveUnmapped(page) test_and_set_bit(PG_active_unmapped, &(page)->flags)
 
 #define PageSlab(page)		test_bit(PG_slab, &(page)->flags)
 #define SetPageSlab(page)	set_bit(PG_slab, &(page)->flags)
@@ -302,6 +309,12 @@ extern unsigned long __read_page_state(u
 #define SetPageAnon(page)	set_bit(PG_anon, &(page)->flags)
 #define ClearPageAnon(page)	clear_bit(PG_anon, &(page)->flags)
 
+#define PageUsedOnce(page)	test_bit(PG_usedonce, &(page)->flags)
+#define SetPageUsedOnce(page)	set_bit(PG_usedonce, &(page)->flags)
+#define TestSetPageUsedOnce(page) test_and_set_bit(PG_usedonce, &(page)->flags)
+#define ClearPageUsedOnce(page)	clear_bit(PG_usedonce, &(page)->flags)
+#define TestClearPageUsedOnce(page) test_and_clear_bit(PG_usedonce, &(page)->flags)
+
 #ifdef CONFIG_SWAP
 #define PageSwapCache(page)	test_bit(PG_swapcache, &(page)->flags)
 #define SetPageSwapCache(page)	set_bit(PG_swapcache, &(page)->flags)
diff -puN include/linux/rmap.h~nickvm-267r2m2 include/linux/rmap.h
--- linux-2.6.7-rc3-xx1/include/linux/rmap.h~nickvm-267r2m2	2004-06-09 06:02:55.475171176 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/include/linux/rmap.h	2004-06-09 06:02:55.520164336 -0400
@@ -95,7 +95,7 @@ static inline void page_dup_rmap(struct 
 /*
  * Called from mm/vmscan.c to handle paging out
  */
-int page_referenced(struct page *);
+void page_gather(struct page *, int *, int *);
 int try_to_unmap(struct page *);
 
 #else	/* !CONFIG_MMU */
diff -puN include/linux/swap.h~nickvm-267r2m2 include/linux/swap.h
--- linux-2.6.7-rc3-xx1/include/linux/swap.h~nickvm-267r2m2	2004-06-09 06:02:55.478170720 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/include/linux/swap.h	2004-06-09 06:02:55.522164032 -0400
@@ -165,16 +165,20 @@ extern unsigned int nr_free_pagecache_pa
 /* linux/mm/swap.c */
 extern void FASTCALL(lru_cache_add(struct page *));
 extern void FASTCALL(lru_cache_add_active(struct page *));
-extern void FASTCALL(activate_page(struct page *));
-extern void FASTCALL(mark_page_accessed(struct page *));
 extern void lru_add_drain(void);
 extern int rotate_reclaimable_page(struct page *page);
 extern void swap_setup(void);
 
+/* Mark a page as having seen activity. */
+#define mark_page_accessed(page)	\
+do {					\
+	SetPageReferenced(page);	\
+} while (0)
+
 /* linux/mm/vmscan.c */
 extern int try_to_free_pages(struct zone **, unsigned int, unsigned int);
 extern int shrink_all_memory(int);
-extern int vm_swappiness;
+extern int vm_mapped_page_cost;
 
 #ifdef CONFIG_MMU
 /* linux/mm/shmem.c */
diff -puN kernel/sysctl.c~nickvm-267r2m2 kernel/sysctl.c
--- linux-2.6.7-rc3-xx1/kernel/sysctl.c~nickvm-267r2m2	2004-06-09 06:02:55.482170112 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/kernel/sysctl.c	2004-06-09 06:02:55.528163120 -0400
@@ -642,6 +642,7 @@ static ctl_table kern_table[] = {
 /* Constants for minimum and maximum testing in vm_table.
    We use these as one-element integer vectors. */
 static int zero;
+static int one = 1;
 static int one_hundred = 100;
 
 
@@ -718,13 +719,13 @@ static ctl_table vm_table[] = {
 	},
 	{
 		.ctl_name	= VM_SWAPPINESS,
-		.procname	= "swappiness",
-		.data		= &vm_swappiness,
-		.maxlen		= sizeof(vm_swappiness),
+		.procname	= "mapped_page_cost",
+		.data		= &vm_mapped_page_cost,
+		.maxlen		= sizeof(vm_mapped_page_cost),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
 		.strategy	= &sysctl_intvec,
-		.extra1		= &zero,
+		.extra1		= &one,
 		.extra2		= &one_hundred,
 	},
 #ifdef CONFIG_HUGETLB_PAGE
diff -puN mm/filemap.c~nickvm-267r2m2 mm/filemap.c
--- linux-2.6.7-rc3-xx1/mm/filemap.c~nickvm-267r2m2	2004-06-09 06:02:55.488169200 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/filemap.c	2004-06-09 06:02:55.531162664 -0400
@@ -699,11 +699,7 @@ page_ok:
 		if (mapping_writably_mapped(mapping))
 			flush_dcache_page(page);
 
-		/*
-		 * Mark the page accessed if we read the beginning.
-		 */
-		if (!offset)
-			mark_page_accessed(page);
+		mark_page_accessed(page);
 
 		/*
 		 * Ok, we have the page, and it's up-to-date, so
diff -puN mm/hugetlb.c~nickvm-267r2m2 mm/hugetlb.c
--- linux-2.6.7-rc3-xx1/mm/hugetlb.c~nickvm-267r2m2	2004-06-09 06:02:55.491168744 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/hugetlb.c	2004-06-09 06:02:55.532162512 -0400
@@ -120,9 +120,12 @@ static void update_and_free_page(struct 
 	int i;
 	nr_huge_pages--;
 	for (i = 0; i < (HPAGE_SIZE / PAGE_SIZE); i++) {
-		page[i].flags &= ~(1 << PG_locked | 1 << PG_error | 1 << PG_referenced |
-				1 << PG_dirty | 1 << PG_active | 1 << PG_reserved |
-				1 << PG_private | 1<< PG_writeback);
+		page[i].flags &= ~(
+			1 << PG_locked		| 1 << PG_error		|
+			1 << PG_referenced	| 1 << PG_dirty		|
+			1 << PG_active_mapped	| 1 << PG_active_unmapped |
+			1 << PG_reserved	| 1 << PG_private	|
+			1 << PG_writeback);
 		set_page_count(&page[i], 0);
 	}
 	set_page_count(page, 1);
diff -puN mm/page-writeback.c~nickvm-267r2m2 mm/page-writeback.c
--- linux-2.6.7-rc3-xx1/mm/page-writeback.c~nickvm-267r2m2	2004-06-09 06:02:55.494168288 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/page-writeback.c	2004-06-09 06:02:55.534162208 -0400
@@ -153,9 +153,11 @@ get_dirty_limits(struct writeback_state 
 	if (dirty_ratio < 5)
 		dirty_ratio = 5;
 
-	background_ratio = dirty_background_ratio;
-	if (background_ratio >= dirty_ratio)
-		background_ratio = dirty_ratio / 2;
+	/*
+	 * Keep the ratio between dirty_ratio and background_ratio roughly
+	 * what the sysctls are after dirty_ratio has been scaled (above).
+	 */
+	background_ratio = dirty_background_ratio * dirty_ratio/vm_dirty_ratio;
 
 	background = (background_ratio * total_pages) / 100;
 	dirty = (dirty_ratio * total_pages) / 100;
diff -puN mm/page_alloc.c~nickvm-267r2m2 mm/page_alloc.c
--- linux-2.6.7-rc3-xx1/mm/page_alloc.c~nickvm-267r2m2	2004-06-09 06:02:55.497167832 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/page_alloc.c	2004-06-09 06:02:55.537161752 -0400
@@ -82,7 +82,7 @@ static void bad_page(const char *functio
 	page->flags &= ~(1 << PG_private	|
 			1 << PG_locked	|
 			1 << PG_lru	|
-			1 << PG_active	|
+			1 << PG_active_mapped	|
 			1 << PG_dirty	|
 			1 << PG_maplock |
 			1 << PG_anon    |
@@ -225,7 +225,8 @@ static inline void free_pages_check(cons
 			1 << PG_lru	|
 			1 << PG_private |
 			1 << PG_locked	|
-			1 << PG_active	|
+			1 << PG_active_mapped	|
+			1 << PG_active_unmapped	|
 			1 << PG_reclaim	|
 			1 << PG_slab	|
 			1 << PG_maplock |
@@ -335,7 +336,8 @@ static void prep_new_page(struct page *p
 			1 << PG_private	|
 			1 << PG_locked	|
 			1 << PG_lru	|
-			1 << PG_active	|
+			1 << PG_active_mapped	|
+			1 << PG_active_unmapped	|
 			1 << PG_dirty	|
 			1 << PG_reclaim	|
 			1 << PG_maplock |
@@ -819,7 +821,8 @@ unsigned int nr_used_zone_pages(void)
 	struct zone *zone;
 
 	for_each_zone(zone)
-		pages += zone->nr_active + zone->nr_inactive;
+		pages += zone->nr_active_mapped + zone->nr_active_unmapped
+			+ zone->nr_inactive;
 
 	return pages;
 }
@@ -975,7 +978,7 @@ void get_zone_counts(unsigned long *acti
 	*inactive = 0;
 	*free = 0;
 	for_each_zone(zone) {
-		*active += zone->nr_active;
+		*active += zone->nr_active_mapped + zone->nr_active_unmapped;
 		*inactive += zone->nr_inactive;
 		*free += zone->free_pages;
 	}
@@ -1093,7 +1096,7 @@ void show_free_areas(void)
 			K(zone->pages_min),
 			K(zone->pages_low),
 			K(zone->pages_high),
-			K(zone->nr_active),
+			K(zone->nr_active_mapped + zone->nr_active_unmapped),
 			K(zone->nr_inactive),
 			K(zone->present_pages)
 			);
@@ -1440,8 +1443,6 @@ static void __init free_area_init_core(s
 		zone->zone_pgdat = pgdat;
 		zone->free_pages = 0;
 
-		zone->temp_priority = zone->prev_priority = DEF_PRIORITY;
-
 		/*
 		 * The per-cpu-pages pools are set to around 1000th of the
 		 * size of the zone.  But no more than 1/4 of a meg - there's
@@ -1475,11 +1476,15 @@ static void __init free_area_init_core(s
 		}
 		printk("  %s zone: %lu pages, LIFO batch:%lu\n",
 				zone_names[j], realsize, batch);
-		INIT_LIST_HEAD(&zone->active_list);
+		INIT_LIST_HEAD(&zone->active_mapped_list);
+		INIT_LIST_HEAD(&zone->active_unmapped_list);
 		INIT_LIST_HEAD(&zone->inactive_list);
-		atomic_set(&zone->nr_scan_active, 0);
+		atomic_set(&zone->nr_scan_active_mapped, 0);
+		atomic_set(&zone->nr_scan_active_unmapped, 0);
 		atomic_set(&zone->nr_scan_inactive, 0);
-		zone->nr_active = 0;
+		atomic_set(&zone->nr_dirty_inactive, 0);
+		zone->nr_active_mapped = 0;
+		zone->nr_active_unmapped = 0;
 		zone->nr_inactive = 0;
 		if (!size)
 			continue;
diff -puN mm/rmap.c~nickvm-267r2m2 mm/rmap.c
--- linux-2.6.7-rc3-xx1/mm/rmap.c~nickvm-267r2m2	2004-06-09 06:02:55.499167528 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/rmap.c	2004-06-09 06:02:55.539161448 -0400
@@ -192,15 +192,15 @@ vma_address(struct page *page, struct vm
  * Subfunctions of page_referenced: page_referenced_one called
  * repeatedly from either page_referenced_anon or page_referenced_file.
  */
-static int page_referenced_one(struct page *page,
-	struct vm_area_struct *vma, unsigned int *mapcount)
+static void page_gather_one(struct page *page,
+		struct vm_area_struct *vma, unsigned int *mapcount,
+		int *referenced, int *dirty)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long address;
 	pgd_t *pgd;
 	pmd_t *pmd;
 	pte_t *pte;
-	int referenced = 0;
 
 	if (!mm->rss)
 		goto out;
@@ -227,7 +227,10 @@ static int page_referenced_one(struct pa
 		goto out_unmap;
 
 	if (ptep_clear_flush_young(vma, address, pte))
-		referenced++;
+		(*referenced)++;
+
+	if (pte_dirty(*pte))
+		(*dirty)++;
 
 	(*mapcount)--;
 
@@ -236,25 +239,24 @@ out_unmap:
 out_unlock:
 	spin_unlock(&mm->page_table_lock);
 out:
-	return referenced;
+	;
 }
 
-static inline int page_referenced_anon(struct page *page)
+static inline void
+page_gather_anon(struct page *page, int *referenced, int *dirty)
 {
 	unsigned int mapcount = page->mapcount;
 	struct anon_vma *anon_vma = (struct anon_vma *) page->mapping;
 	struct vm_area_struct *vma;
-	int referenced = 0;
 
 	spin_lock(&anon_vma->lock);
 	BUG_ON(list_empty(&anon_vma->head));
 	list_for_each_entry(vma, &anon_vma->head, anon_vma_node) {
-		referenced += page_referenced_one(page, vma, &mapcount);
+		page_gather_one(page, vma, &mapcount, referenced, dirty);
 		if (!mapcount)
 			break;
 	}
 	spin_unlock(&anon_vma->lock);
-	return referenced;
 }
 
 /**
@@ -271,32 +273,31 @@ static inline int page_referenced_anon(s
  * The spinlock address_space->i_mmap_lock is tried.  If it can't be gotten,
  * assume a reference count of 0, so try_to_unmap will then have a go.
  */
-static inline int page_referenced_file(struct page *page)
+static inline void
+page_gather_file(struct page *page, int *referenced, int *dirty)
 {
 	unsigned int mapcount = page->mapcount;
 	struct address_space *mapping = page->mapping;
 	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
 	struct vm_area_struct *vma = NULL;
 	struct prio_tree_iter iter;
-	int referenced = 0;
 
 	if (!spin_trylock(&mapping->i_mmap_lock))
-		return 0;
+		return;
 
 	while ((vma = vma_prio_tree_next(vma, &mapping->i_mmap,
 					&iter, pgoff, pgoff)) != NULL) {
 		if ((vma->vm_flags & (VM_LOCKED|VM_MAYSHARE))
 				  == (VM_LOCKED|VM_MAYSHARE)) {
-			referenced++;
+			(*referenced)++;
 			break;
 		}
-		referenced += page_referenced_one(page, vma, &mapcount);
+		page_gather_one(page, vma, &mapcount, referenced, dirty);
 		if (!mapcount)
 			break;
 	}
 
 	spin_unlock(&mapping->i_mmap_lock);
-	return referenced;
 }
 
 /**
@@ -307,23 +308,23 @@ static inline int page_referenced_file(s
  * returns the number of ptes which referenced the page.
  * Caller needs to hold the rmap lock.
  */
-int page_referenced(struct page *page)
+void page_gather(struct page *page, int *referenced, int *dirty)
 {
-	int referenced = 0;
+	*referenced = 0;
+	*dirty = 0;
 
 	if (page_test_and_clear_young(page))
-		referenced++;
+		(*referenced)++;
 
 	if (TestClearPageReferenced(page))
-		referenced++;
+		(*referenced)++;
 
 	if (page->mapcount && page->mapping) {
 		if (PageAnon(page))
-			referenced += page_referenced_anon(page);
+			page_gather_anon(page, referenced, dirty);
 		else
-			referenced += page_referenced_file(page);
+			page_gather_file(page, referenced, dirty);
 	}
-	return referenced;
 }
 
 /**
diff -puN mm/shmem.c~nickvm-267r2m2 mm/shmem.c
--- linux-2.6.7-rc3-xx1/mm/shmem.c~nickvm-267r2m2	2004-06-09 06:02:55.502167072 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/shmem.c	2004-06-09 06:02:55.542160992 -0400
@@ -1431,11 +1431,7 @@ static void do_shmem_file_read(struct fi
 			 */
 			if (mapping_writably_mapped(mapping))
 				flush_dcache_page(page);
-			/*
-			 * Mark the page accessed if we read the beginning.
-			 */
-			if (!offset)
-				mark_page_accessed(page);
+			mark_page_accessed(page);
 		}
 
 		/*
diff -puN mm/swap.c~nickvm-267r2m2 mm/swap.c
--- linux-2.6.7-rc3-xx1/mm/swap.c~nickvm-267r2m2	2004-06-09 06:02:55.505166616 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/swap.c	2004-06-09 06:02:55.544160688 -0400
@@ -78,14 +78,18 @@ int rotate_reclaimable_page(struct page 
 		return 1;
 	if (PageDirty(page))
 		return 1;
-	if (PageActive(page))
+	if (PageActiveMapped(page))
+		return 1;
+	if (PageActiveUnmapped(page))
 		return 1;
 	if (!PageLRU(page))
 		return 1;
 
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lru_lock, flags);
-	if (PageLRU(page) && !PageActive(page)) {
+	if (PageLRU(page)
+		&& !PageActiveMapped(page) && !PageActiveUnmapped(page)) {
+
 		list_del(&page->lru);
 		list_add_tail(&page->lru, &zone->inactive_list);
 		inc_page_state(pgrotated);
@@ -96,42 +100,6 @@ int rotate_reclaimable_page(struct page 
 	return 0;
 }
 
-/*
- * FIXME: speed this up?
- */
-void fastcall activate_page(struct page *page)
-{
-	struct zone *zone = page_zone(page);
-
-	spin_lock_irq(&zone->lru_lock);
-	if (PageLRU(page) && !PageActive(page)) {
-		del_page_from_inactive_list(zone, page);
-		SetPageActive(page);
-		add_page_to_active_list(zone, page);
-		inc_page_state(pgactivate);
-	}
-	spin_unlock_irq(&zone->lru_lock);
-}
-
-/*
- * Mark a page as having seen activity.
- *
- * inactive,unreferenced	->	inactive,referenced
- * inactive,referenced		->	active,unreferenced
- * active,unreferenced		->	active,referenced
- */
-void fastcall mark_page_accessed(struct page *page)
-{
-	if (!PageActive(page) && PageReferenced(page) && PageLRU(page)) {
-		activate_page(page);
-		ClearPageReferenced(page);
-	} else if (!PageReferenced(page)) {
-		SetPageReferenced(page);
-	}
-}
-
-EXPORT_SYMBOL(mark_page_accessed);
-
 /**
  * lru_cache_add: add a page to the page lists
  * @page: the page to add
@@ -303,6 +271,7 @@ void __pagevec_lru_add(struct pagevec *p
 		}
 		if (TestSetPageLRU(page))
 			BUG();
+		ClearPageUsedOnce(page);
 		add_page_to_inactive_list(zone, page);
 	}
 	if (zone)
@@ -330,9 +299,16 @@ void __pagevec_lru_add_active(struct pag
 		}
 		if (TestSetPageLRU(page))
 			BUG();
-		if (TestSetPageActive(page))
-			BUG();
-		add_page_to_active_list(zone, page);
+		ClearPageUsedOnce(page);
+		if (page_mapped(page)) {
+			if (TestSetPageActiveMapped(page))
+				BUG();
+			add_page_to_active_mapped_list(zone, page);
+		} else {
+			if (TestSetPageActiveUnmapped(page))
+				BUG();
+			add_page_to_active_unmapped_list(zone, page);
+		}
 	}
 	if (zone)
 		spin_unlock_irq(&zone->lru_lock);
diff -puN mm/vmscan.c~nickvm-267r2m2 mm/vmscan.c
--- linux-2.6.7-rc3-xx1/mm/vmscan.c~nickvm-267r2m2	2004-06-09 06:02:55.508166160 -0400
+++ linux-2.6.7-rc3-xx1-xiphux/mm/vmscan.c	2004-06-09 06:05:14.999960184 -0400
@@ -39,10 +39,9 @@
 #include <linux/swapops.h>
 
 /*
- * From 0 .. 100.  Higher means more swappy.
+ * From 1 .. 100.  Higher means less swappy.
  */
-int vm_swappiness = 60;
-static long total_memory;
+int vm_mapped_page_cost = 32;
 
 #define lru_to_page(_head) (list_entry((_head)->prev, struct page, lru))
 
@@ -175,27 +174,6 @@ static int shrink_slab(unsigned long sca
 	return 0;
 }
 
-/* Must be called with page's rmap lock held. */
-static inline int page_mapping_inuse(struct page *page)
-{
-	struct address_space *mapping;
-
-	/* Page is in somebody's page tables. */
-	if (page_mapped(page))
-		return 1;
-
-	/* Be more reluctant to reclaim swapcache than pagecache */
-	if (PageSwapCache(page))
-		return 1;
-
-	mapping = page_mapping(page);
-	if (!mapping)
-		return 0;
-
-	/* File is mmap'd by somebody? */
-	return mapping_mapped(mapping);
-}
-
 static inline int is_page_cache_freeable(struct page *page)
 {
 	return page_count(page) - !!PagePrivate(page) == 2;
@@ -252,7 +230,7 @@ typedef enum {
 } pageout_t;
 
 /*
- * pageout is called by shrink_list() for each dirty page. Calls ->writepage().
+ * pageout is called by hrink_list() for each dirty page. Calls ->writepage().
  */
 static pageout_t pageout(struct page *page, struct address_space *mapping)
 {
@@ -317,6 +295,12 @@ struct scan_control {
 	/* Incremented by the number of inactive pages that were scanned */
 	unsigned long nr_scanned;
 
+	/* Number of dirty pages found on the end of the inactive list */
+	unsigned long nr_dirty;
+
+	/* Number of dirty pages we're putting on the inactive list */
+	unsigned long nr_dirty_inactive;
+
 	/* Incremented by the number of pages reclaimed */
 	unsigned long nr_reclaimed;
 
@@ -348,7 +332,7 @@ static int shrink_list(struct list_head 
 		struct address_space *mapping;
 		struct page *page;
 		int may_enter_fs;
-		int referenced;
+		int referenced, dirty, mapped;
 
 		page = lru_to_page(page_list);
 		list_del(&page->lru);
@@ -356,22 +340,41 @@ static int shrink_list(struct list_head 
 		if (TestSetPageLocked(page))
 			goto keep;
 
-		BUG_ON(PageActive(page));
+		BUG_ON(PageActiveMapped(page) || PageActiveUnmapped(page));
 
-		if (PageWriteback(page))
-			goto keep_locked;
+		if (PageWriteback(page)) {
+			SetPageReclaim(page);
+			if (likely(PageWriteback(page)))
+				goto keep_locked;
+			/* We've raced. Can fall through but must clear
+			 * PageReclaim */
+			ClearPageReclaim(page);
+		}
+
+		page_map_lock(page);
+		mapped = page_mapped(page);
 
 		sc->nr_scanned++;
-		/* Double the slab pressure for mapped and swapcache pages */
-		if (page_mapped(page) || PageSwapCache(page))
-			sc->nr_scanned++;
+		/* Increase the slab pressure for mapped and swapcache pages */
+		if (mapped || PageSwapCache(page))
+			sc->nr_scanned += vm_mapped_page_cost;
 
-		page_map_lock(page);
-		referenced = page_referenced(page);
-		if (referenced && page_mapping_inuse(page)) {
-			/* In active use or really unfreeable.  Activate it. */
+		page_gather(page, &referenced, &dirty);
+		if (referenced) {
+			/*
+			 * Has been referenced.  Activate used twice or
+			 * mapped pages, otherwise give it another chance
+			 * on the inactive list
+			 */
 			page_map_unlock(page);
-			goto activate_locked;
+			if (mapped || TestSetPageUsedOnce(page))
+				goto activate_locked;
+
+			if (dirty) {
+				set_page_dirty(page);
+				sc->nr_dirty_inactive++;
+			}
+			goto keep_locked;
 		}
 
 #ifdef CONFIG_SWAP
@@ -412,8 +415,7 @@ static int shrink_list(struct list_head 
 		page_map_unlock(page);
 
 		if (PageDirty(page)) {
-			if (referenced)
-				goto keep_locked;
+			sc->nr_dirty++;
 			if (!may_enter_fs)
 				goto keep_locked;
 			if (laptop_mode && !sc->may_writepage)
@@ -508,7 +510,10 @@ free_it:
 		continue;
 
 activate_locked:
-		SetPageActive(page);
+		if (page_mapped(page))
+			SetPageActiveMapped(page);
+		else
+			SetPageActiveUnmapped(page);
 		pgactivate++;
 keep_locked:
 		unlock_page(page);
@@ -598,9 +603,13 @@ static void shrink_cache(struct zone *zo
 			if (TestSetPageLRU(page))
 				BUG();
 			list_del(&page->lru);
-			if (PageActive(page))
-				add_page_to_active_list(zone, page);
-			else
+			if (PageActiveMapped(page)) {
+				ClearPageUsedOnce(page);
+				add_page_to_active_mapped_list(zone, page);
+			} else if (PageActiveUnmapped(page)) {
+				ClearPageUsedOnce(page);
+				add_page_to_active_unmapped_list(zone, page);
+			} else
 				add_page_to_inactive_list(zone, page);
 			if (!pagevec_add(&pvec, page)) {
 				spin_unlock_irq(&zone->lru_lock);
@@ -632,9 +641,9 @@ done:
  * But we had to alter page->flags anyway.
  */
 static void
-refill_inactive_zone(struct zone *zone, struct scan_control *sc)
+shrink_active_list(struct zone *zone, struct list_head *list, unsigned long *nr_list_pages, struct scan_control *sc)
 {
-	int pgmoved;
+	int pgmoved, pgmoved_unmapped;
 	int pgdeactivate = 0;
 	int pgscanned = 0;
 	int nr_pages = sc->nr_to_scan;
@@ -643,17 +652,14 @@ refill_inactive_zone(struct zone *zone, 
 	LIST_HEAD(l_active);	/* Pages to go onto the active_list */
 	struct page *page;
 	struct pagevec pvec;
-	int reclaim_mapped = 0;
-	long mapped_ratio;
-	long distress;
-	long swap_tendency;
 
 	lru_add_drain();
 	pgmoved = 0;
+
 	spin_lock_irq(&zone->lru_lock);
-	while (pgscanned < nr_pages && !list_empty(&zone->active_list)) {
-		page = lru_to_page(&zone->active_list);
-		prefetchw_prev_lru_page(page, &zone->active_list, flags);
+	while (pgscanned < nr_pages && !list_empty(list)) {
+		page = lru_to_page(list);
+		prefetchw_prev_lru_page(page, list, flags);
 		if (!TestClearPageLRU(page))
 			BUG();
 		list_del(&page->lru);
@@ -666,63 +672,22 @@ refill_inactive_zone(struct zone *zone, 
 			 */
 			__put_page(page);
 			SetPageLRU(page);
-			list_add(&page->lru, &zone->active_list);
+			list_add(&page->lru, list);
 		} else {
 			list_add(&page->lru, &l_hold);
 			pgmoved++;
 		}
 		pgscanned++;
 	}
-	zone->nr_active -= pgmoved;
+	*nr_list_pages -= pgmoved;
 	spin_unlock_irq(&zone->lru_lock);
 
-	/*
-	 * `distress' is a measure of how much trouble we're having reclaiming
-	 * pages.  0 -> no problems.  100 -> great trouble.
-	 */
-	distress = 100 >> zone->prev_priority;
-
-	/*
-	 * The point of this algorithm is to decide when to start reclaiming
-	 * mapped memory instead of just pagecache.  Work out how much memory
-	 * is mapped.
-	 */
-	mapped_ratio = (sc->nr_mapped * 100) / total_memory;
-
-	/*
-	 * Now decide how much we really want to unmap some pages.  The mapped
-	 * ratio is downgraded - just because there's a lot of mapped memory
-	 * doesn't necessarily mean that page reclaim isn't succeeding.
-	 *
-	 * The distress ratio is important - we don't want to start going oom.
-	 *
-	 * A 100% value of vm_swappiness overrides this algorithm altogether.
-	 */
-	swap_tendency = mapped_ratio / 2 + distress + vm_swappiness;
-
-	/*
-	 * Now use this metric to decide whether to start moving mapped memory
-	 * onto the inactive list.
-	 */
-	if (swap_tendency >= 100)
-		reclaim_mapped = 1;
-
 	while (!list_empty(&l_hold)) {
+		int referenced, dirty;
+
 		page = lru_to_page(&l_hold);
 		list_del(&page->lru);
-		if (page_mapped(page)) {
-			if (!reclaim_mapped) {
-				list_add(&page->lru, &l_active);
-				continue;
-			}
-			page_map_lock(page);
-			if (page_referenced(page)) {
-				page_map_unlock(page);
-				list_add(&page->lru, &l_active);
-				continue;
-			}
-			page_map_unlock(page);
-		}
+
 		/*
 		 * FIXME: need to consider page_count(page) here if/when we
 		 * reap orphaned pages via the LRU (Daniel's locking stuff)
@@ -731,6 +696,20 @@ refill_inactive_zone(struct zone *zone, 
 			list_add(&page->lru, &l_active);
 			continue;
 		}
+
+		page_map_lock(page);
+		page_gather(page, &referenced, &dirty);
+		page_map_unlock(page);
+
+		if (referenced) {
+			list_add(&page->lru, &l_active);
+			continue;
+		}
+		if (dirty) {
+			set_page_dirty(page);
+			sc->nr_dirty_inactive++;
+		}
+
 		list_add(&page->lru, &l_inactive);
 	}
 
@@ -742,7 +721,8 @@ refill_inactive_zone(struct zone *zone, 
 		prefetchw_prev_lru_page(page, &l_inactive, flags);
 		if (TestSetPageLRU(page))
 			BUG();
-		if (!TestClearPageActive(page))
+		if (!TestClearPageActiveMapped(page)
+				&& !TestClearPageActiveUnmapped(page))
 			BUG();
 		list_move(&page->lru, &zone->inactive_list);
 		pgmoved++;
@@ -766,23 +746,37 @@ refill_inactive_zone(struct zone *zone, 
 	}
 
 	pgmoved = 0;
+	pgmoved_unmapped = 0;
 	while (!list_empty(&l_active)) {
 		page = lru_to_page(&l_active);
 		prefetchw_prev_lru_page(page, &l_active, flags);
 		if (TestSetPageLRU(page))
 			BUG();
-		BUG_ON(!PageActive(page));
-		list_move(&page->lru, &zone->active_list);
-		pgmoved++;
+		if(!TestClearPageActiveMapped(page)
+				&& !TestClearPageActiveUnmapped(page))
+			BUG();
+		if (page_mapped(page)) {
+			SetPageActiveMapped(page);
+			list_move(&page->lru, &zone->active_mapped_list);
+			pgmoved++;
+		} else {
+			SetPageActiveUnmapped(page);
+			list_move(&page->lru, &zone->active_unmapped_list);
+			pgmoved_unmapped++;
+		}
+
 		if (!pagevec_add(&pvec, page)) {
-			zone->nr_active += pgmoved;
+			zone->nr_active_mapped += pgmoved;
 			pgmoved = 0;
+			zone->nr_active_unmapped += pgmoved_unmapped;
+			pgmoved_unmapped = 0;
 			spin_unlock_irq(&zone->lru_lock);
 			__pagevec_release(&pvec);
 			spin_lock_irq(&zone->lru_lock);
 		}
 	}
-	zone->nr_active += pgmoved;
+	zone->nr_active_mapped += pgmoved;
+	zone->nr_active_unmapped += pgmoved_unmapped;
 	spin_unlock_irq(&zone->lru_lock);
 	pagevec_release(&pvec);
 
@@ -797,10 +791,14 @@ refill_inactive_zone(struct zone *zone, 
 static void
 shrink_zone(struct zone *zone, struct scan_control *sc)
 {
-	unsigned long scan_active, scan_inactive;
-	int count;
+	unsigned long long tmp;
+	unsigned long scan_active, scan_active_mapped, scan_active_unmapped;
+	unsigned long scan_inactive;
+	unsigned long nr_active = zone->nr_active_mapped
+					+ zone->nr_active_unmapped;
+  	int count;
 
-	scan_inactive = (zone->nr_active + zone->nr_inactive) >> sc->priority;
+	scan_inactive = (nr_active + zone->nr_inactive) >> sc->priority;
 
 	/*
 	 * Try to keep the active list 2/3 of the size of the cache.  And
@@ -812,25 +810,51 @@ shrink_zone(struct zone *zone, struct sc
 	 * `scan_active' just to make sure that the kernel will slowly sift
 	 * through the active list.
 	 */
-	if (zone->nr_active >= 4*(zone->nr_inactive*2 + 1)) {
-		/* Don't scan more than 4 times the inactive list scan size */
-		scan_active = 4*scan_inactive;
+	if (nr_active >= (zone->nr_inactive*2 + 1)) {
+		if (nr_active >= 4*(zone->nr_inactive*2 + 1)) {
+			/* Don't scan more than 4x inactive list scan size */
+			scan_active = 4*scan_inactive;
+		} else {
+			/* Cast to long long so the multiply doesn't overflow */
+			tmp = (unsigned long long)scan_inactive * nr_active;
+			do_div(tmp, zone->nr_inactive*2 + 1);
+			scan_active = (unsigned long)tmp;
+		}
+
+		tmp = scan_active * zone->nr_active_mapped;
+		do_div(tmp, nr_active + 1);
+		scan_active_mapped = tmp;
+		scan_active_unmapped = scan_active - tmp;
 	} else {
-		unsigned long long tmp;
+		/* Don't scan the active list if the inactive list is large */
+		scan_active_mapped = scan_active_unmapped = 0;
+	}
+
+	/* Reset this before commencing the scanning */
+	sc->nr_dirty_inactive = 0;
 
-		/* Cast to long long so the multiply doesn't overflow */
+	if (sc->priority == DEF_PRIORITY) {
+		/* Keep things ticking */
+		scan_active_mapped++;
+		scan_active_unmapped++;
+	}
 
-		tmp = (unsigned long long)scan_inactive * zone->nr_active;
-		do_div(tmp, zone->nr_inactive*2 + 1);
-		scan_active = (unsigned long)tmp;
+	atomic_add(scan_active_mapped, &zone->nr_scan_active_mapped);
+	count = atomic_read(&zone->nr_scan_active_mapped);
+	if (count >= SWAP_CLUSTER_MAX) {
+		atomic_set(&zone->nr_scan_active_mapped, 0);
+		sc->nr_to_scan = count;
+		shrink_active_list(zone, &zone->active_mapped_list,
+					&zone->nr_active_mapped, sc);
 	}
 
-	atomic_add(scan_active + 1, &zone->nr_scan_active);
-	count = atomic_read(&zone->nr_scan_active);
+	atomic_add(scan_active_unmapped, &zone->nr_scan_active_unmapped);
+	count = atomic_read(&zone->nr_scan_active_unmapped);
 	if (count >= SWAP_CLUSTER_MAX) {
-		atomic_set(&zone->nr_scan_active, 0);
+		atomic_set(&zone->nr_scan_active_unmapped, 0);
 		sc->nr_to_scan = count;
-		refill_inactive_zone(zone, sc);
+		shrink_active_list(zone, &zone->active_unmapped_list,
+					&zone->nr_active_unmapped, sc);
 	}
 
 	atomic_add(scan_inactive, &zone->nr_scan_inactive);
@@ -840,6 +864,24 @@ shrink_zone(struct zone *zone, struct sc
 		sc->nr_to_scan = count;
 		return shrink_cache(zone, sc);
 	}
+
+	/*
+	 * Try to write back as many pages as the number of dirty ones
+	 * we're adding to the inactive list.  This tends to cause slow
+	 * streaming writers to write data to the disk smoothly, at the
+	 * dirtying rate, which is nice.   But that's undesirable in
+	 * laptop mode, where we *want* lumpy writeout.  So in laptop
+	 * mode, write out the whole world.
+	 */
+	atomic_add(sc->nr_dirty_inactive, &zone->nr_dirty_inactive);
+	count = atomic_read(&zone->nr_dirty_inactive);
+	if (count > zone->nr_inactive / 2
+		|| (!(laptop_mode && !sc->may_writepage)
+			&& count > SWAP_CLUSTER_MAX)) {
+		atomic_set(&zone->nr_dirty_inactive, 0);
+		wakeup_bdflush(laptop_mode ? 0 : count*2);
+		sc->may_writepage = 1;
+	}
 }
 
 /*
@@ -866,10 +908,6 @@ shrink_caches(struct zone **zones, struc
 	for (i = 0; zones[i] != NULL; i++) {
 		struct zone *zone = zones[i];
 
-		zone->temp_priority = sc->priority;
-		if (zone->prev_priority > sc->priority)
-			zone->prev_priority = sc->priority;
-
 		if (zone->all_unreclaimable && sc->priority != DEF_PRIORITY)
 			continue;	/* Let kswapd poll it */
 
@@ -898,17 +936,16 @@ int try_to_free_pages(struct zone **zone
 	int total_scanned = 0, total_reclaimed = 0;
 	struct reclaim_state *reclaim_state = current->reclaim_state;
 	struct scan_control sc;
-	int i;
 
 	sc.gfp_mask = gfp_mask;
 	sc.may_writepage = 0;
+	sc.nr_dirty = 0;
 
 	inc_page_state(allocstall);
 
-	for (i = 0; zones[i] != 0; i++)
-		zones[i]->temp_priority = DEF_PRIORITY;
-
 	for (priority = DEF_PRIORITY; priority >= 0; priority--) {
+		int threshold;
+
 		sc.nr_mapped = read_page_state(nr_mapped);
 		sc.nr_scanned = 0;
 		sc.nr_reclaimed = 0;
@@ -927,13 +964,14 @@ int try_to_free_pages(struct zone **zone
 		total_reclaimed += sc.nr_reclaimed;
 
 		/*
-		 * Try to write back as many pages as we just scanned.  This
-		 * tends to cause slow streaming writers to write data to the
-		 * disk smoothly, at the dirtying rate, which is nice.   But
-		 * that's undesirable in laptop mode, where we *want* lumpy
-		 * writeout.  So in laptop mode, write out the whole world.
+		 * If we're in laptop mode, and more than a quarter of
+		 * the pages we're scanning are dirty, start writing.
 		 */
-		if (total_scanned > SWAP_CLUSTER_MAX + SWAP_CLUSTER_MAX/2) {
+		threshold = sc.nr_dirty * 10;
+		if (laptop_mode)
+			threshold = sc.nr_dirty * 2;
+		if (total_scanned > SWAP_CLUSTER_MAX
+				&& total_scanned < threshold) {
 			wakeup_bdflush(laptop_mode ? 0 : total_scanned);
 			sc.may_writepage = 1;
 		}
@@ -945,8 +983,6 @@ int try_to_free_pages(struct zone **zone
 	if ((gfp_mask & __GFP_FS) && !(gfp_mask & __GFP_NORETRY))
 		out_of_memory();
 out:
-	for (i = 0; zones[i] != 0; i++)
-		zones[i]->prev_priority = zones[i]->temp_priority;
 	return ret;
 }
 
@@ -986,16 +1022,11 @@ static int balance_pgdat(pg_data_t *pgda
 
 	sc.gfp_mask = GFP_KERNEL;
 	sc.may_writepage = 0;
+	sc.nr_dirty = 0;
 	sc.nr_mapped = read_page_state(nr_mapped);
 
 	inc_page_state(pageoutrun);
 
-	for (i = 0; i < pgdat->nr_zones; i++) {
-		struct zone *zone = pgdat->node_zones + i;
-
-		zone->temp_priority = DEF_PRIORITY;
-	}
-
 	for (priority = DEF_PRIORITY; priority >= 0; priority--) {
 		int all_zones_ok = 1;
 		int end_zone = 0;	/* Inclusive.  0 = ZONE_DMA */
@@ -1033,6 +1064,7 @@ scan:
 		 * cause too much scanning of the lower zones.
 		 */
 		for (i = 0; i <= end_zone; i++) {
+			int threshold;
 			struct zone *zone = pgdat->node_zones + i;
 
 			if (zone->all_unreclaimable && priority != DEF_PRIORITY)
@@ -1042,9 +1074,6 @@ scan:
 				if (zone->free_pages <= zone->pages_high)
 					all_zones_ok = 0;
 			}
-			zone->temp_priority = priority;
-			if (zone->prev_priority > priority)
-				zone->prev_priority = priority;
 			sc.nr_scanned = 0;
 			sc.nr_reclaimed = 0;
 			sc.priority = priority;
@@ -1057,14 +1086,19 @@ scan:
 				continue;
 			if (zone->pages_scanned > zone->present_pages * 2)
 				zone->all_unreclaimable = 1;
+
 			/*
-			 * If we've done a decent amount of scanning and
-			 * the reclaim ratio is low, start doing writepage
-			 * even in laptop mode
+			 * If we're in laptop mode, and more than a quarter of
+			 * the pages we're scanning are dirty, start writing.
 			 */
-			if (total_scanned > SWAP_CLUSTER_MAX * 2 &&
-			    total_scanned > total_reclaimed+total_reclaimed/2)
+			threshold = sc.nr_dirty * 10;
+			if (laptop_mode)
+				threshold = sc.nr_dirty * 2;
+			if (total_scanned > SWAP_CLUSTER_MAX
+					&& total_scanned < threshold) {
+				wakeup_bdflush(laptop_mode ? 0 : total_scanned);
 				sc.may_writepage = 1;
+			}
 		}
 		if (nr_pages && to_free > total_reclaimed)
 			continue;	/* swsusp: need to do more work */
@@ -1078,11 +1112,6 @@ scan:
 			blk_congestion_wait(WRITE, HZ/10);
 	}
 out:
-	for (i = 0; i < pgdat->nr_zones; i++) {
-		struct zone *zone = pgdat->node_zones + i;
-
-		zone->prev_priority = zone->temp_priority;
-	}
 	return total_reclaimed;
 }
 
@@ -1211,7 +1240,6 @@ static int __init kswapd_init(void)
 	for_each_pgdat(pgdat)
 		pgdat->kswapd
 		= find_task_by_pid(kernel_thread(kswapd, pgdat, CLONE_KERNEL));
-	total_memory = nr_free_pagecache_pages();
 	hotcpu_notifier(cpu_callback, 0);
 	return 0;
 }

_
