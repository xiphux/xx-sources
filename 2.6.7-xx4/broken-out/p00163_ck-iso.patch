---

 linux-2.6.7-xx4-xiphux/include/linux/sched.h |    4 ++
 linux-2.6.7-xx4-xiphux/kernel/sched.c        |   39 ++++++++++++++++++++++-----
 2 files changed, 35 insertions(+), 8 deletions(-)

diff -puN include/linux/sched.h~ck-iso include/linux/sched.h
--- linux-2.6.7-xx4/include/linux/sched.h~ck-iso	2004-06-28 12:25:32.342163928 -0400
+++ linux-2.6.7-xx4-xiphux/include/linux/sched.h	2004-06-28 12:27:23.071330536 -0400
@@ -129,8 +129,9 @@ extern unsigned long nr_iowait(void);
 
 #ifdef CONFIG_STAIRCASE
 #define SCHED_BATCH		3
+#define SCHED_ISO		4
 #define SCHED_MIN		0
-#define SCHED_MAX		3
+#define SCHED_MAX		4
 
 #define SCHED_RANGE(policy)	((policy) >= SCHED_MIN && \
 					(policy) <= SCHED_MAX)
@@ -328,6 +329,7 @@ struct signal_struct {
 #endif
 #ifdef CONFIG_STAIRCASE
 #define batch_task(p)		((p)->policy == SCHED_BATCH)
+#define iso_task(p)		((p)->policy == SCHED_ISO)
 #endif
 
 /*
diff -puN kernel/sched.c~ck-iso kernel/sched.c
--- linux-2.6.7-xx4/kernel/sched.c~ck-iso	2004-06-28 12:25:35.158735744 -0400
+++ linux-2.6.7-xx4-xiphux/kernel/sched.c	2004-06-28 12:34:05.055219696 -0400
@@ -469,6 +469,18 @@ static inline void rq_unlock(runqueue_t 
 
 #if defined(CONFIG_SPA) || defined(CONFIG_STAIRCASE)
 #ifdef CONFIG_STAIRCASE
+static int rr_interval(task_t * p)
+{
+	int rr_interval = RR_INTERVAL;
+	if (batch_task(p))
+		rr_interval *= 10;
+	else if (iso_task(p))
+		rr_interval /= 2;
+	if (!rr_interval)
+		rr_interval = 1;
+	return rr_interval;
+}
+
 static int task_preempts_curr(struct task_struct *p, runqueue_t *rq)
 {
 	if (p->prio >= rq->curr->prio)
@@ -699,6 +711,8 @@ static unsigned int burst(task_t *p)
 	if (rt_task(p))
 		return p->burst;
 	task_user_prio = TASK_USER_PRIO(p);
+	if (iso_task(p))
+		task_user_prio /= 2;
 	if (likely(task_user_prio < 40))
 		return 39 - task_user_prio;
 	else
@@ -745,7 +759,7 @@ int interactive = 1;
  */
 static int effective_prio(task_t *p)
 {
-	int prio;
+	int prio, rr;
 	unsigned int full_slice, used_slice, first_slice;
 	unsigned int best_burst;
 	if (rt_task(p))
@@ -755,17 +769,18 @@ static int effective_prio(task_t *p)
 
 	best_burst = burst(p);
 	full_slice = slice(p);
+	rr = rr_interval(p);
 	used_slice = full_slice - p->slice;
 	if (p->burst > best_burst)
 		p->burst = best_burst;
-	first_slice = RR_INTERVAL;
+	first_slice = rr;
 	if (interactive && !compute)
 		first_slice *= (p->burst + 1);
 	prio = MAX_PRIO - 2 - best_burst;
 
 	if (used_slice < first_slice)
 		return prio;
-	prio += 1 + (used_slice - first_slice) / RR_INTERVAL;
+	prio += 1 + (used_slice - first_slice) / rr;
 	if (prio > MAX_PRIO - 2)
 		prio = MAX_PRIO - 2;
 	return prio;
@@ -777,7 +792,7 @@ static void recalc_task_prio(task_t *p, 
 	unsigned long run_time = NS_TO_JIFFIES(p->runtime);
 	unsigned long total_run = NS_TO_JIFFIES(p->totalrun) + run_time;
 	if (((!run_time && NS_TO_JIFFIES(p->runtime + sleep_time) <
-		RR_INTERVAL) || p->flags & PF_FORKED) && !batch_task(p)) {
+		rr_interval(p)) || p->flags & PF_FORKED) && !batch_task(p)) {
 			p->flags &= ~PF_FORKED;
 			if (p->slice - total_run < 1) {
 				p->totalrun = 0;
@@ -1026,7 +1041,7 @@ activate_task(task_t *p, runqueue_t *rq,
 	p->slice = slice(p);
 	recalc_task_prio(p, now);
 	p->prio = effective_prio(p);
-	p->time_slice = RR_INTERVAL;
+	p->time_slice = rr_interval(p);
 	if (batch_task(p))
 		p->time_slice *= 10;
 #elif !defined(CONFIG_SPA)
@@ -2876,7 +2891,7 @@ void scheduler_tick(int user_ticks, int 
 		dec_burst(p);
 		p->slice = slice(p);
 		p->prio = effective_prio(p);
-		p->time_slice = RR_INTERVAL;
+		p->time_slice = rr_interval(p);
 		enqueue_task(p, rq);
 		goto out_unlock;
 #else
@@ -2935,7 +2950,7 @@ void scheduler_tick(int user_ticks, int 
 		p->time_slice = task_timeslice(p);
 #endif
 #ifdef CONFIG_STAIRCASE
-		p->time_slice = RR_INTERVAL;
+		p->time_slice = rr_interval(p);
 		enqueue_task(p, rq);
 		goto out_unlock;
 	}
@@ -3884,7 +3899,15 @@ static int setscheduler(pid_t pid, int p
 	if ((policy == SCHED_FIFO || policy == SCHED_RR) &&
 	    !capable(CAP_SYS_NICE))
 #endif
+#ifdef CONFIG_STAIRCASE
+		/*
+		 * If the caller requested an RT policy without having the
+		 * necessary rights, we downgrade the policy to SCHED_ISO.
+		 */
+		policy = SCHED_ISO;
+#else
 		goto out_unlock;
+#endif
 	if ((current->euid != p->euid) && (current->euid != p->uid) &&
 	    !capable(CAP_SYS_NICE))
 		goto out_unlock;
@@ -4380,6 +4403,7 @@ asmlinkage long sys_sched_get_priority_m
 	case SCHED_NORMAL:
 #ifdef CONFIG_STAIRCASE
 	case SCHED_BATCH:
+	case SCHED_ISO:
 #endif
 		ret = 0;
 		break;
@@ -4406,6 +4430,7 @@ asmlinkage long sys_sched_get_priority_m
 	case SCHED_NORMAL:
 #ifdef CONFIG_STAIRCASE
 	case SCHED_BATCH:
+	case SCHED_ISO:
 #endif
 		ret = 0;
 	}

_
