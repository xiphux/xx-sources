---

 linux-2.6.7-xx4-xiphux/fs/exec.c                  |    3 
 linux-2.6.7-xx4-xiphux/fs/hfs/inode.c             |   23 -
 linux-2.6.7-xx4-xiphux/fs/hfsplus/inode.c         |   23 -
 linux-2.6.7-xx4-xiphux/include/linux/mm_inline.h  |   34 +
 linux-2.6.7-xx4-xiphux/include/linux/mmzone.h     |   29 -
 linux-2.6.7-xx4-xiphux/include/linux/page-flags.h |   61 ++-
 linux-2.6.7-xx4-xiphux/include/linux/rmap.h       |    2 
 linux-2.6.7-xx4-xiphux/include/linux/swap.h       |   11 
 linux-2.6.7-xx4-xiphux/kernel/sysctl.c            |    9 
 linux-2.6.7-xx4-xiphux/mm/filemap.c               |    6 
 linux-2.6.7-xx4-xiphux/mm/hugetlb.c               |    9 
 linux-2.6.7-xx4-xiphux/mm/memory.c                |    7 
 linux-2.6.7-xx4-xiphux/mm/page-writeback.c        |    8 
 linux-2.6.7-xx4-xiphux/mm/page_alloc.c            |  144 +++----
 linux-2.6.7-xx4-xiphux/mm/rmap.c                  |   45 +-
 linux-2.6.7-xx4-xiphux/mm/shmem.c                 |    6 
 linux-2.6.7-xx4-xiphux/mm/swap.c                  |   89 ----
 linux-2.6.7-xx4-xiphux/mm/swap_state.c            |    3 
 linux-2.6.7-xx4-xiphux/mm/swapfile.c              |    6 
 linux-2.6.7-xx4-xiphux/mm/vmscan.c                |  405 ++++++++++++----------
 20 files changed, 458 insertions(+), 465 deletions(-)

diff -puN fs/hfs/inode.c~split-active-pages-np2 fs/hfs/inode.c
--- linux-2.6.7-xx4/fs/hfs/inode.c~split-active-pages-np2	2004-06-29 00:51:39.848468504 -0400
+++ linux-2.6.7-xx4-xiphux/fs/hfs/inode.c	2004-06-29 00:51:39.919457712 -0400
@@ -67,19 +67,20 @@ int hfs_releasepage(struct page *page, i
 		nidx = page->index >> (tree->node_size_shift - PAGE_CACHE_SHIFT);
 		spin_lock(&tree->hash_lock);
 		node = hfs_bnode_findhash(tree, nidx);
-		if (!node)
-			;
-		else if (atomic_read(&node->refcnt))
-			res = 0;
-		else for (i = 0; i < tree->pages_per_bnode; i++) {
-			if (PageActive(node->page[i])) {
+		if (node) {
+			if (atomic_read(&node->refcnt))
 				res = 0;
-				break;
+			else for (i = 0; i < tree->pages_per_bnode; i++) {
+				if (PageActiveMapped(node->page[i]) ||
+					PageActiveUnmapped(node->page[i])) {
+					res = 0;
+					break;
+				}
+			}
+			if (res) {
+				hfs_bnode_unhash(node);
+				hfs_bnode_free(node);
 			}
-		}
-		if (res && node) {
-			hfs_bnode_unhash(node);
-			hfs_bnode_free(node);
 		}
 		spin_unlock(&tree->hash_lock);
 	} else {
diff -puN fs/hfsplus/inode.c~split-active-pages-np2 fs/hfsplus/inode.c
--- linux-2.6.7-xx4/fs/hfsplus/inode.c~split-active-pages-np2	2004-06-29 00:51:39.851468048 -0400
+++ linux-2.6.7-xx4-xiphux/fs/hfsplus/inode.c	2004-06-29 00:51:39.922457256 -0400
@@ -67,19 +67,20 @@ int hfsplus_releasepage(struct page *pag
 		nidx = page->index >> (tree->node_size_shift - PAGE_CACHE_SHIFT);
 		spin_lock(&tree->hash_lock);
 		node = hfs_bnode_findhash(tree, nidx);
-		if (!node)
-			;
-		else if (atomic_read(&node->refcnt))
-			res = 0;
-		else for (i = 0; i < tree->pages_per_bnode; i++) {
-			if (PageActive(node->page[i])) {
+		if (node) {
+			if (atomic_read(&node->refcnt))
 				res = 0;
-				break;
+			else for (i = 0; i < tree->pages_per_bnode; i++) {
+				if (PageActiveMapped(node->page[i]) ||
+					PageActiveUnmapped(node->page[i])) {
+					res = 0;
+					break;
+				}
+			}
+			if (res) {
+				hfs_bnode_unhash(node);
+				hfs_bnode_free(node);
 			}
-		}
-		if (res && node) {
-			hfs_bnode_unhash(node);
-			hfs_bnode_free(node);
 		}
 		spin_unlock(&tree->hash_lock);
 	} else {
diff -puN include/linux/mm_inline.h~split-active-pages-np2 include/linux/mm_inline.h
--- linux-2.6.7-xx4/include/linux/mm_inline.h~split-active-pages-np2	2004-06-29 00:51:39.856467288 -0400
+++ linux-2.6.7-xx4-xiphux/include/linux/mm_inline.h	2004-06-29 00:51:39.923457104 -0400
@@ -1,9 +1,16 @@
 
 static inline void
-add_page_to_active_list(struct zone *zone, struct page *page)
+add_page_to_active_mapped_list(struct zone *zone, struct page *page)
 {
-	list_add(&page->lru, &zone->active_list);
-	zone->nr_active++;
+	list_add(&page->lru, &zone->active_mapped_list);
+	zone->nr_active_mapped++;
+}
+
+static inline void
+add_page_to_active_unmapped_list(struct zone *zone, struct page *page)
+{
+	list_add(&page->lru, &zone->active_unmapped_list);
+	zone->nr_active_unmapped++;
 }
 
 static inline void
@@ -14,10 +21,17 @@ add_page_to_inactive_list(struct zone *z
 }
 
 static inline void
-del_page_from_active_list(struct zone *zone, struct page *page)
+del_page_from_active_mapped_list(struct zone *zone, struct page *page)
+{
+	list_del(&page->lru);
+	zone->nr_active_mapped--;
+}
+
+static inline void
+del_page_from_active_unmapped_list(struct zone *zone, struct page *page)
 {
 	list_del(&page->lru);
-	zone->nr_active--;
+	zone->nr_active_unmapped--;
 }
 
 static inline void
@@ -31,10 +45,14 @@ static inline void
 del_page_from_lru(struct zone *zone, struct page *page)
 {
 	list_del(&page->lru);
-	if (PageActive(page)) {
-		ClearPageActive(page);
-		zone->nr_active--;
+	if (PageActiveMapped(page)) {
+		ClearPageActiveMapped(page);
+		zone->nr_active_mapped--;
+	} else if (PageActiveUnmapped(page)) {
+		ClearPageActiveUnmapped(page);
+		zone->nr_active_unmapped--;
 	} else {
+		ClearPageUsedOnce(page);
 		zone->nr_inactive--;
 	}
 }
diff -puN include/linux/mmzone.h~split-active-pages-np2 include/linux/mmzone.h
--- linux-2.6.7-xx4/include/linux/mmzone.h~split-active-pages-np2	2004-06-29 00:51:39.859466832 -0400
+++ linux-2.6.7-xx4-xiphux/include/linux/mmzone.h	2004-06-29 00:51:39.933455584 -0400
@@ -143,11 +143,15 @@ struct zone {
 	ZONE_PADDING(_pad1_)
 
 	spinlock_t		lru_lock;	
-	struct list_head	active_list;
+	struct list_head	active_mapped_list;
+	struct list_head	active_unmapped_list;
 	struct list_head	inactive_list;
-	unsigned long		nr_scan_active;
+	unsigned long		nr_scan_active_mapped;
+	unsigned long		nr_scan_active_unmapped;
 	unsigned long		nr_scan_inactive;
-	unsigned long		nr_active;
+	unsigned long		nr_dirty_inactive;
+	unsigned long		nr_active_mapped;
+	unsigned long		nr_active_unmapped;
 	unsigned long		nr_inactive;
 	int			all_unreclaimable; /* All pages pinned */
 	unsigned long		pages_scanned;	   /* since last reclaim */
@@ -155,25 +159,6 @@ struct zone {
 	ZONE_PADDING(_pad2_)
 
 	/*
-	 * prev_priority holds the scanning priority for this zone.  It is
-	 * defined as the scanning priority at which we achieved our reclaim
-	 * target at the previous try_to_free_pages() or balance_pgdat()
-	 * invokation.
-	 *
-	 * We use prev_priority as a measure of how much stress page reclaim is
-	 * under - it drives the swappiness decision: whether to unmap mapped
-	 * pages.
-	 *
-	 * temp_priority is used to remember the scanning priority at which
-	 * this zone was successfully refilled to free_pages == pages_high.
-	 *
-	 * Access to both these fields is quite racy even on uniprocessor.  But
-	 * it is expected to average out OK.
-	 */
-	int temp_priority;
-	int prev_priority;
-
-	/*
 	 * free areas of different sizes
 	 */
 	struct free_area	free_area[MAX_ORDER];
diff -puN include/linux/page-flags.h~split-active-pages-np2 include/linux/page-flags.h
--- linux-2.6.7-xx4/include/linux/page-flags.h~split-active-pages-np2	2004-06-29 00:51:39.862466376 -0400
+++ linux-2.6.7-xx4-xiphux/include/linux/page-flags.h	2004-06-29 00:51:39.968450264 -0400
@@ -58,26 +58,27 @@
 
 #define PG_dirty	 	 4
 #define PG_lru			 5
-#define PG_active		 6
-#define PG_slab			 7	/* slab debug (Suparna wants this) */
-
-#define PG_highmem		 8
-#define PG_checked		 9	/* kill me in 2.5.<early>. */
-#define PG_arch_1		10
-#define PG_reserved		11
-
-#define PG_private		12	/* Has something at ->private */
-#define PG_writeback		13	/* Page is under writeback */
-#define PG_nosave		14	/* Used for system suspend/resume */
-#define PG_maplock		15	/* Lock bit for rmap to ptes */
-
-#define PG_swapcache		16	/* Swap page: swp_entry_t in private */
-#define PG_mappedtodisk		17	/* Has blocks allocated on-disk */
-#define PG_reclaim		18	/* To be reclaimed asap */
-#define PG_compound		19	/* Part of a compound page */
-
-#define PG_anon			20	/* Anonymous: anon_vma in mapping */
+#define PG_active_mapped	 6
+#define PG_active_unmapped	 7
 
+#define PG_slab			 8	/* slab debug (Suparna wants this) */
+#define PG_highmem		 9
+#define PG_checked		10	/* kill me in 2.5.<early>. */
+#define PG_arch_1		11
+
+#define PG_reserved		12
+#define PG_private		13	/* Has something at ->private */
+#define PG_writeback		14	/* Page is under writeback */
+#define PG_nosave		15	/* Used for system suspend/resume */
+
+#define PG_maplock		16	/* Lock bit for rmap to ptes */
+#define PG_swapcache		17	/* Swap page: swp_entry_t in private */
+#define PG_mappedtodisk		18	/* Has blocks allocated on-disk */
+#define PG_reclaim		19	/* To be reclaimed asap */
+
+#define PG_compound		20	/* Part of a compound page */
+#define PG_anon			21	/* Anonymous: anon_vma in mapping */
+#define PG_usedonce		22	/* LRU page has been touched once */
 
 /*
  * Global page accounting.  One instance per CPU.  Only unsigned longs are
@@ -211,11 +212,17 @@ extern unsigned long __read_page_state(u
 #define TestSetPageLRU(page)	test_and_set_bit(PG_lru, &(page)->flags)
 #define TestClearPageLRU(page)	test_and_clear_bit(PG_lru, &(page)->flags)
 
-#define PageActive(page)	test_bit(PG_active, &(page)->flags)
-#define SetPageActive(page)	set_bit(PG_active, &(page)->flags)
-#define ClearPageActive(page)	clear_bit(PG_active, &(page)->flags)
-#define TestClearPageActive(page) test_and_clear_bit(PG_active, &(page)->flags)
-#define TestSetPageActive(page) test_and_set_bit(PG_active, &(page)->flags)
+#define PageActiveMapped(page)		test_bit(PG_active_mapped, &(page)->flags)
+#define SetPageActiveMapped(page)	set_bit(PG_active_mapped, &(page)->flags)
+#define ClearPageActiveMapped(page)	clear_bit(PG_active_mapped, &(page)->flags)
+#define TestClearPageActiveMapped(page) test_and_clear_bit(PG_active_mapped, &(page)->flags)
+#define TestSetPageActiveMapped(page) test_and_set_bit(PG_active_mapped, &(page)->flags)
+
+#define PageActiveUnmapped(page)	test_bit(PG_active_unmapped, &(page)->flags)
+#define SetPageActiveUnmapped(page)	set_bit(PG_active_unmapped, &(page)->flags)
+#define ClearPageActiveUnmapped(page)	clear_bit(PG_active_unmapped, &(page)->flags)
+#define TestClearPageActiveUnmapped(page) test_and_clear_bit(PG_active_unmapped, &(page)->flags)
+#define TestSetPageActiveUnmapped(page) test_and_set_bit(PG_active_unmapped, &(page)->flags)
 
 #define PageSlab(page)		test_bit(PG_slab, &(page)->flags)
 #define SetPageSlab(page)	set_bit(PG_slab, &(page)->flags)
@@ -296,6 +303,12 @@ extern unsigned long __read_page_state(u
 #define SetPageAnon(page)	set_bit(PG_anon, &(page)->flags)
 #define ClearPageAnon(page)	clear_bit(PG_anon, &(page)->flags)
 
+#define PageUsedOnce(page)	test_bit(PG_usedonce, &(page)->flags)
+#define SetPageUsedOnce(page)	set_bit(PG_usedonce, &(page)->flags)
+#define TestSetPageUsedOnce(page) test_and_set_bit(PG_usedonce, &(page)->flags)
+#define ClearPageUsedOnce(page)	clear_bit(PG_usedonce, &(page)->flags)
+#define TestClearPageUsedOnce(page) test_and_clear_bit(PG_usedonce, &(page)->flags)
+
 #ifdef CONFIG_SWAP
 #define PageSwapCache(page)	test_bit(PG_swapcache, &(page)->flags)
 #define SetPageSwapCache(page)	set_bit(PG_swapcache, &(page)->flags)
diff -puN include/linux/rmap.h~split-active-pages-np2 include/linux/rmap.h
--- linux-2.6.7-xx4/include/linux/rmap.h~split-active-pages-np2	2004-06-29 00:51:39.865465920 -0400
+++ linux-2.6.7-xx4-xiphux/include/linux/rmap.h	2004-06-29 00:51:39.968450264 -0400
@@ -95,7 +95,7 @@ static inline void page_dup_rmap(struct 
 /*
  * Called from mm/vmscan.c to handle paging out
  */
-int page_referenced(struct page *);
+void page_gather(struct page *, int *, int *);
 int try_to_unmap(struct page *);
 
 #else	/* !CONFIG_MMU */
diff -puN include/linux/swap.h~split-active-pages-np2 include/linux/swap.h
--- linux-2.6.7-xx4/include/linux/swap.h~split-active-pages-np2	2004-06-29 00:51:39.868465464 -0400
+++ linux-2.6.7-xx4-xiphux/include/linux/swap.h	2004-06-29 00:51:39.984447832 -0400
@@ -164,17 +164,20 @@ extern unsigned int nr_free_pagecache_pa
 
 /* linux/mm/swap.c */
 extern void FASTCALL(lru_cache_add(struct page *));
-extern void FASTCALL(lru_cache_add_active(struct page *));
-extern void FASTCALL(activate_page(struct page *));
-extern void FASTCALL(mark_page_accessed(struct page *));
 extern void lru_add_drain(void);
 extern int rotate_reclaimable_page(struct page *page);
 extern void swap_setup(void);
 
+/* Mark a page as having seen activity. */
+#define mark_page_accessed(page)	\
+do {					\
+	SetPageReferenced(page);	\
+} while (0)
+
 /* linux/mm/vmscan.c */
 extern int try_to_free_pages(struct zone **, unsigned int, unsigned int);
 extern int shrink_all_memory(int);
-extern int vm_swappiness;
+extern int vm_mapped_page_cost;
 
 #ifdef CONFIG_MMU
 /* linux/mm/shmem.c */
diff -puN kernel/sysctl.c~split-active-pages-np2 kernel/sysctl.c
--- linux-2.6.7-xx4/kernel/sysctl.c~split-active-pages-np2	2004-06-29 00:51:39.871465008 -0400
+++ linux-2.6.7-xx4-xiphux/kernel/sysctl.c	2004-06-29 00:51:40.031440688 -0400
@@ -658,6 +658,7 @@ static ctl_table kern_table[] = {
 /* Constants for minimum and maximum testing in vm_table.
    We use these as one-element integer vectors. */
 static int zero;
+static int one = 1;
 static int one_hundred = 100;
 
 
@@ -734,13 +735,13 @@ static ctl_table vm_table[] = {
 	},
 	{
 		.ctl_name	= VM_SWAPPINESS,
-		.procname	= "swappiness",
-		.data		= &vm_swappiness,
-		.maxlen		= sizeof(vm_swappiness),
+		.procname	= "mapped_page_cost",
+		.data		= &vm_mapped_page_cost,
+		.maxlen		= sizeof(vm_mapped_page_cost),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
 		.strategy	= &sysctl_intvec,
-		.extra1		= &zero,
+		.extra1		= &one,
 		.extra2		= &one_hundred,
 	},
 #ifdef CONFIG_HUGETLB_PAGE
diff -puN mm/filemap.c~split-active-pages-np2 mm/filemap.c
--- linux-2.6.7-xx4/mm/filemap.c~split-active-pages-np2	2004-06-29 00:51:39.875464400 -0400
+++ linux-2.6.7-xx4-xiphux/mm/filemap.c	2004-06-29 00:51:40.033440384 -0400
@@ -707,11 +707,7 @@ page_ok:
 		if (mapping_writably_mapped(mapping))
 			flush_dcache_page(page);
 
-		/*
-		 * Mark the page accessed if we read the beginning.
-		 */
-		if (!offset)
-			mark_page_accessed(page);
+		mark_page_accessed(page);
 
 		/*
 		 * Ok, we have the page, and it's up-to-date, so
diff -puN mm/hugetlb.c~split-active-pages-np2 mm/hugetlb.c
--- linux-2.6.7-xx4/mm/hugetlb.c~split-active-pages-np2	2004-06-29 00:51:39.879463792 -0400
+++ linux-2.6.7-xx4-xiphux/mm/hugetlb.c	2004-06-29 00:51:40.034440232 -0400
@@ -129,9 +129,12 @@ static void update_and_free_page(struct 
 	nr_huge_pages--;
 	nr_huge_pages_node[page_zone(page)->zone_pgdat->node_id]--;
 	for (i = 0; i < (HPAGE_SIZE / PAGE_SIZE); i++) {
-		page[i].flags &= ~(1 << PG_locked | 1 << PG_error | 1 << PG_referenced |
-				1 << PG_dirty | 1 << PG_active | 1 << PG_reserved |
-				1 << PG_private | 1<< PG_writeback);
+		page[i].flags &= ~(
+			1 << PG_locked		| 1 << PG_error		|
+			1 << PG_referenced	| 1 << PG_dirty		|
+			1 << PG_active_mapped	| 1 << PG_active_unmapped |
+			1 << PG_reserved	| 1 << PG_private	|
+			1 << PG_writeback);
 		set_page_count(&page[i], 0);
 	}
 	set_page_count(page, 1);
diff -puN mm/page-writeback.c~split-active-pages-np2 mm/page-writeback.c
--- linux-2.6.7-xx4/mm/page-writeback.c~split-active-pages-np2	2004-06-29 00:51:39.881463488 -0400
+++ linux-2.6.7-xx4-xiphux/mm/page-writeback.c	2004-06-29 00:51:40.035440080 -0400
@@ -153,9 +153,11 @@ get_dirty_limits(struct writeback_state 
 	if (dirty_ratio < 5)
 		dirty_ratio = 5;
 
-	background_ratio = dirty_background_ratio;
-	if (background_ratio >= dirty_ratio)
-		background_ratio = dirty_ratio / 2;
+	/*
+	 * Keep the ratio between dirty_ratio and background_ratio roughly
+	 * what the sysctls are after dirty_ratio has been scaled (above).
+	 */
+	background_ratio = dirty_background_ratio * dirty_ratio/vm_dirty_ratio;
 
 	background = (background_ratio * total_pages) / 100;
 	dirty = (dirty_ratio * total_pages) / 100;
diff -puN mm/page_alloc.c~split-active-pages-np2 mm/page_alloc.c
--- linux-2.6.7-xx4/mm/page_alloc.c~split-active-pages-np2	2004-06-29 00:51:39.884463032 -0400
+++ linux-2.6.7-xx4-xiphux/mm/page_alloc.c	2004-06-29 00:51:40.038439624 -0400
@@ -85,7 +85,7 @@ static void bad_page(const char *functio
 	page->flags &= ~(1 << PG_private	|
 			1 << PG_locked	|
 			1 << PG_lru	|
-			1 << PG_active	|
+			1 << PG_active_mapped	|
 			1 << PG_dirty	|
 			1 << PG_maplock |
 			1 << PG_anon    |
@@ -226,7 +226,8 @@ static inline void free_pages_check(cons
 			1 << PG_lru	|
 			1 << PG_private |
 			1 << PG_locked	|
-			1 << PG_active	|
+			1 << PG_active_mapped	|
+			1 << PG_active_unmapped	|
 			1 << PG_reclaim	|
 			1 << PG_slab	|
 			1 << PG_maplock |
@@ -261,8 +262,6 @@ free_pages_bulk(struct zone *zone, int c
 	base = zone->zone_mem_map;
 	area = zone->free_area + order;
 	spin_lock_irqsave(&zone->lock, flags);
-	zone->all_unreclaimable = 0;
-	zone->pages_scanned = 0;
 	while (!list_empty(list) && count--) {
 		page = list_entry(list->prev, struct page, lru);
 		/* have to delete it as __free_pages_bulk list manipulates */
@@ -347,7 +346,8 @@ static void prep_new_page(struct page *p
 			1 << PG_private	|
 			1 << PG_locked	|
 			1 << PG_lru	|
-			1 << PG_active	|
+			1 << PG_active_mapped	|
+			1 << PG_active_unmapped	|
 			1 << PG_dirty	|
 			1 << PG_reclaim	|
 			1 << PG_maplock |
@@ -600,7 +600,7 @@ __alloc_pages(unsigned int gfp_mask, uns
 {
 	const int wait = gfp_mask & __GFP_WAIT;
 	unsigned long min;
-	struct zone **zones;
+	struct zone **zones, *z;
 	struct page *page;
 	struct reclaim_state reclaim_state;
 	struct task_struct *p = current;
@@ -611,72 +611,56 @@ __alloc_pages(unsigned int gfp_mask, uns
 	might_sleep_if(wait);
 
 	zones = zonelist->zones;  /* the list of zones suitable for gfp_mask */
-	if (zones[0] == NULL)     /* no zones in the zonelist */
+
+	if (unlikely(zones[0] == NULL)) {
+		/* Should this ever happen?? */
 		return NULL;
+	}
 
 	alloc_type = zone_idx(zones[0]);
 
 	/* Go through the zonelist once, looking for a zone with enough free */
-	for (i = 0; zones[i] != NULL; i++) {
-		struct zone *z = zones[i];
+	for (i = 0; (z = zones[i]) != NULL; i++) {
+		min = z->pages_low + (1<<order) + z->protection[alloc_type];
 
-		min = (1<<order) + z->protection[alloc_type];
+		if (z->free_pages < min)
+			continue;
 
-		/*
-		 * We let real-time tasks dip their real-time paws a little
-		 * deeper into reserves.
-		 */
-		if (rt_task(p))
-			min -= z->pages_low >> 1;
+		page = buffered_rmqueue(z, order, gfp_mask);
+		if (page)
+			goto got_pg;
+ 	}
 
-		if (z->free_pages >= min ||
-				(!wait && z->free_pages >= z->pages_high)) {
-			page = buffered_rmqueue(z, order, gfp_mask);
-			if (page) {
-				zone_statistics(zonelist, z);
-				goto got_pg;
-			}
-		}
-	}
-
-	/* we're somewhat low on memory, failed to find what we needed */
-	for (i = 0; zones[i] != NULL; i++)
+	for (i = 0; (z = zones[i]) != NULL; i++)
 		wakeup_kswapd(zones[i]);
 
-	/* Go through the zonelist again, taking __GFP_HIGH into account */
-	for (i = 0; zones[i] != NULL; i++) {
-		struct zone *z = zones[i];
-
-		min = (1<<order) + z->protection[alloc_type];
-
+	/*
+	 * Go through the zonelist again. Let __GFP_HIGH and allocations
+	 * coming from realtime tasks to go deeper into reserves
+	 */
+	for (i = 0; (z = zones[i]) != NULL; i++) {
+		min = z->pages_min;
 		if (gfp_mask & __GFP_HIGH)
-			min -= z->pages_low >> 2;
-		if (rt_task(p))
-			min -= z->pages_low >> 1;
+			min -= min>>2;
+		if (unlikely(rt_task(p)))
+			min -= min>>2;
+		min += (1<<order) + z->protection[alloc_type];
 
-		if (z->free_pages >= min ||
-				(!wait && z->free_pages >= z->pages_high)) {
-			page = buffered_rmqueue(z, order, gfp_mask);
-			if (page) {
-				zone_statistics(zonelist, z);
-				goto got_pg;
-			}
-		}
-	}
+		if (z->free_pages < min)
+			continue;
 
-	/* here we're in the low on memory slow path */
+		page = buffered_rmqueue(z, order, gfp_mask);
+		if (page)
+			goto got_pg;
+	}
 
-rebalance:
+	/* This allocation should allow future memory freeing. */
 	if ((p->flags & (PF_MEMALLOC | PF_MEMDIE)) && !in_interrupt()) {
 		/* go through the zonelist yet again, ignoring mins */
-		for (i = 0; zones[i] != NULL; i++) {
-			struct zone *z = zones[i];
-
+		for (i = 0; (z = zones[i]) != NULL; i++) {
 			page = buffered_rmqueue(z, order, gfp_mask);
-			if (page) {
-				zone_statistics(zonelist, z);
+			if (page)
 				goto got_pg;
-			}
 		}
 		goto nopage;
 	}
@@ -685,6 +669,8 @@ rebalance:
 	if (!wait)
 		goto nopage;
 
+rebalance:
+	/* We now go into synchronous reclaim */
 	p->flags |= PF_MEMALLOC;
 	reclaim_state.reclaimed_slab = 0;
 	p->reclaim_state = &reclaim_state;
@@ -695,27 +681,28 @@ rebalance:
 	p->flags &= ~PF_MEMALLOC;
 
 	/* go through the zonelist yet one more time */
-	for (i = 0; zones[i] != NULL; i++) {
-		struct zone *z = zones[i];
+	for (i = 0; (z = zones[i]) != NULL; i++) {
+		min = z->pages_min;
+		if (gfp_mask & __GFP_HIGH)
+			min -= min>>2;
+		if (unlikely(rt_task(p)))
+			min -= min>>2;
+		min += (1<<order) + z->protection[alloc_type];
 
-		min = (1UL << order) + z->protection[alloc_type];
+		if (z->free_pages < min)
+			continue;
 
-		if (z->free_pages >= min ||
-				(!wait && z->free_pages >= z->pages_high)) {
-			page = buffered_rmqueue(z, order, gfp_mask);
-			if (page) {
- 				zone_statistics(zonelist, z);
-				goto got_pg;
-			}
-		}
+		page = buffered_rmqueue(z, order, gfp_mask);
+		if (page)
+			goto got_pg;
 	}
 
 	/*
 	 * Don't let big-order allocations loop unless the caller explicitly
 	 * requests that.  Wait for some write requests to complete then retry.
 	 *
-	 * In this implementation, __GFP_REPEAT means __GFP_NOFAIL, but that
-	 * may not be true in other implementations.
+	 * In this implementation, __GFP_REPEAT means __GFP_NOFAIL for order
+	 * <= 3, but that may not be true in other implementations.
 	 */
 	do_retry = 0;
 	if (!(gfp_mask & __GFP_NORETRY)) {
@@ -738,6 +725,7 @@ nopage:
 	}
 	return NULL;
 got_pg:
+	zone_statistics(zonelist, z);
 	kernel_map_pages(page, 1 << order, 1);
 	return page;
 }
@@ -831,7 +819,8 @@ unsigned int nr_used_zone_pages(void)
 	struct zone *zone;
 
 	for_each_zone(zone)
-		pages += zone->nr_active + zone->nr_inactive;
+		pages += zone->nr_active_mapped + zone->nr_active_unmapped
+			+ zone->nr_inactive;
 
 	return pages;
 }
@@ -985,7 +974,7 @@ void get_zone_counts(unsigned long *acti
 	*inactive = 0;
 	*free = 0;
 	for_each_zone(zone) {
-		*active += zone->nr_active;
+		*active += zone->nr_active_mapped + zone->nr_active_unmapped;
 		*inactive += zone->nr_inactive;
 		*free += zone->free_pages;
 	}
@@ -1103,7 +1092,7 @@ void show_free_areas(void)
 			K(zone->pages_min),
 			K(zone->pages_low),
 			K(zone->pages_high),
-			K(zone->nr_active),
+			K(zone->nr_active_mapped + zone->nr_active_unmapped),
 			K(zone->nr_inactive),
 			K(zone->present_pages)
 			);
@@ -1454,8 +1443,6 @@ static void __init free_area_init_core(s
 		zone->zone_pgdat = pgdat;
 		zone->free_pages = 0;
 
-		zone->temp_priority = zone->prev_priority = DEF_PRIORITY;
-
 		/*
 		 * The per-cpu-pages pools are set to around 1000th of the
 		 * size of the zone.  But no more than 1/4 of a meg - there's
@@ -1489,11 +1476,15 @@ static void __init free_area_init_core(s
 		}
 		printk("  %s zone: %lu pages, LIFO batch:%lu\n",
 				zone_names[j], realsize, batch);
-		INIT_LIST_HEAD(&zone->active_list);
+		INIT_LIST_HEAD(&zone->active_mapped_list);
+		INIT_LIST_HEAD(&zone->active_unmapped_list);
 		INIT_LIST_HEAD(&zone->inactive_list);
-		zone->nr_scan_active = 0;
+		zone->nr_scan_active_mapped = 0;
+		zone->nr_scan_active_unmapped = 0;
 		zone->nr_scan_inactive = 0;
-		zone->nr_active = 0;
+		zone->nr_dirty_inactive = 0;
+		zone->nr_active_mapped = 0;
+		zone->nr_active_unmapped = 0;
 		zone->nr_inactive = 0;
 		if (!size)
 			continue;
@@ -1856,7 +1847,7 @@ static void setup_per_zone_protection(vo
 				 * zero because the lower zones take
 				 * contributions from the higher zones.
 				 */
-				if (j > max_zone || j > i) {
+				if (j > max_zone || j >= i) {
 					zone->protection[i] = 0;
 					continue;
 				}
@@ -1865,7 +1856,6 @@ static void setup_per_zone_protection(vo
 				 */
 				zone->protection[i] = higherzone_val(zone,
 								max_zone, i);
-				zone->protection[i] += zone->pages_low;
 			}
 		}
 	}
diff -puN mm/rmap.c~split-active-pages-np2 mm/rmap.c
--- linux-2.6.7-xx4/mm/rmap.c~split-active-pages-np2	2004-06-29 00:51:39.888462424 -0400
+++ linux-2.6.7-xx4-xiphux/mm/rmap.c	2004-06-29 00:51:40.039439472 -0400
@@ -193,15 +193,15 @@ vma_address(struct page *page, struct vm
  * Subfunctions of page_referenced: page_referenced_one called
  * repeatedly from either page_referenced_anon or page_referenced_file.
  */
-static int page_referenced_one(struct page *page,
-	struct vm_area_struct *vma, unsigned int *mapcount)
+static void page_gather_one(struct page *page,
+		struct vm_area_struct *vma, unsigned int *mapcount,
+		int *referenced, int *dirty)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long address;
 	pgd_t *pgd;
 	pmd_t *pmd;
 	pte_t *pte;
-	int referenced = 0;
 
 	if (!mm->rss)
 		goto out;
@@ -228,7 +228,10 @@ static int page_referenced_one(struct pa
 		goto out_unmap;
 
 	if (ptep_clear_flush_young(vma, address, pte))
-		referenced++;
+		(*referenced)++;
+
+	if (pte_dirty(*pte))
+		(*dirty)++;
 
 	(*mapcount)--;
 
@@ -237,25 +240,24 @@ out_unmap:
 out_unlock:
 	spin_unlock(&mm->page_table_lock);
 out:
-	return referenced;
+	;
 }
 
-static inline int page_referenced_anon(struct page *page)
+static inline void
+page_gather_anon(struct page *page, int *referenced, int *dirty)
 {
 	unsigned int mapcount = page->mapcount;
 	struct anon_vma *anon_vma = (struct anon_vma *) page->mapping;
 	struct vm_area_struct *vma;
-	int referenced = 0;
 
 	spin_lock(&anon_vma->lock);
 	BUG_ON(list_empty(&anon_vma->head));
 	list_for_each_entry(vma, &anon_vma->head, anon_vma_node) {
-		referenced += page_referenced_one(page, vma, &mapcount);
+		page_gather_one(page, vma, &mapcount, referenced, dirty);
 		if (!mapcount)
 			break;
 	}
 	spin_unlock(&anon_vma->lock);
-	return referenced;
 }
 
 /**
@@ -272,32 +274,31 @@ static inline int page_referenced_anon(s
  * The spinlock address_space->i_mmap_lock is tried.  If it can't be gotten,
  * assume a reference count of 0, so try_to_unmap will then have a go.
  */
-static inline int page_referenced_file(struct page *page)
+static inline void
+page_gather_file(struct page *page, int *referenced, int *dirty)
 {
 	unsigned int mapcount = page->mapcount;
 	struct address_space *mapping = page->mapping;
 	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
 	struct vm_area_struct *vma = NULL;
 	struct prio_tree_iter iter;
-	int referenced = 0;
 
 	if (!spin_trylock(&mapping->i_mmap_lock))
-		return 0;
+		return;
 
 	while ((vma = vma_prio_tree_next(vma, &mapping->i_mmap,
 					&iter, pgoff, pgoff)) != NULL) {
 		if ((vma->vm_flags & (VM_LOCKED|VM_MAYSHARE))
 				  == (VM_LOCKED|VM_MAYSHARE)) {
-			referenced++;
+			(*referenced)++;
 			break;
 		}
-		referenced += page_referenced_one(page, vma, &mapcount);
+		page_gather_one(page, vma, &mapcount, referenced, dirty);
 		if (!mapcount)
 			break;
 	}
 
 	spin_unlock(&mapping->i_mmap_lock);
-	return referenced;
 }
 
 /**
@@ -308,23 +309,23 @@ static inline int page_referenced_file(s
  * returns the number of ptes which referenced the page.
  * Caller needs to hold the rmap lock.
  */
-int page_referenced(struct page *page)
+void page_gather(struct page *page, int *referenced, int *dirty)
 {
-	int referenced = 0;
+	*referenced = 0;
+	*dirty = 0;
 
 	if (page_test_and_clear_young(page))
-		referenced++;
+		(*referenced)++;
 
 	if (TestClearPageReferenced(page))
-		referenced++;
+		(*referenced)++;
 
 	if (page->mapcount && page->mapping) {
 		if (PageAnon(page))
-			referenced += page_referenced_anon(page);
+			page_gather_anon(page, referenced, dirty);
 		else
-			referenced += page_referenced_file(page);
+			page_gather_file(page, referenced, dirty);
 	}
-	return referenced;
 }
 
 /**
diff -puN mm/shmem.c~split-active-pages-np2 mm/shmem.c
--- linux-2.6.7-xx4/mm/shmem.c~split-active-pages-np2	2004-06-29 00:51:39.891461968 -0400
+++ linux-2.6.7-xx4-xiphux/mm/shmem.c	2004-06-29 00:51:40.041439168 -0400
@@ -1432,11 +1432,7 @@ static void do_shmem_file_read(struct fi
 			 */
 			if (mapping_writably_mapped(mapping))
 				flush_dcache_page(page);
-			/*
-			 * Mark the page accessed if we read the beginning.
-			 */
-			if (!offset)
-				mark_page_accessed(page);
+			mark_page_accessed(page);
 		}
 
 		/*
diff -puN mm/swap.c~split-active-pages-np2 mm/swap.c
--- linux-2.6.7-xx4/mm/swap.c~split-active-pages-np2	2004-06-29 00:51:39.896461208 -0400
+++ linux-2.6.7-xx4-xiphux/mm/swap.c	2004-06-29 00:51:40.042439016 -0400
@@ -78,14 +78,18 @@ int rotate_reclaimable_page(struct page 
 		return 1;
 	if (PageDirty(page))
 		return 1;
-	if (PageActive(page))
+	if (PageActiveMapped(page))
+		return 1;
+	if (PageActiveUnmapped(page))
 		return 1;
 	if (!PageLRU(page))
 		return 1;
 
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lru_lock, flags);
-	if (PageLRU(page) && !PageActive(page)) {
+	if (PageLRU(page)
+		&& !PageActiveMapped(page) && !PageActiveUnmapped(page)) {
+
 		list_del(&page->lru);
 		list_add_tail(&page->lru, &zone->inactive_list);
 		inc_page_state(pgrotated);
@@ -96,48 +100,11 @@ int rotate_reclaimable_page(struct page 
 	return 0;
 }
 
-/*
- * FIXME: speed this up?
- */
-void fastcall activate_page(struct page *page)
-{
-	struct zone *zone = page_zone(page);
-
-	spin_lock_irq(&zone->lru_lock);
-	if (PageLRU(page) && !PageActive(page)) {
-		del_page_from_inactive_list(zone, page);
-		SetPageActive(page);
-		add_page_to_active_list(zone, page);
-		inc_page_state(pgactivate);
-	}
-	spin_unlock_irq(&zone->lru_lock);
-}
-
-/*
- * Mark a page as having seen activity.
- *
- * inactive,unreferenced	->	inactive,referenced
- * inactive,referenced		->	active,unreferenced
- * active,unreferenced		->	active,referenced
- */
-void fastcall mark_page_accessed(struct page *page)
-{
-	if (!PageActive(page) && PageReferenced(page) && PageLRU(page)) {
-		activate_page(page);
-		ClearPageReferenced(page);
-	} else if (!PageReferenced(page)) {
-		SetPageReferenced(page);
-	}
-}
-
-EXPORT_SYMBOL(mark_page_accessed);
-
 /**
  * lru_cache_add: add a page to the page lists
  * @page: the page to add
  */
 static DEFINE_PER_CPU(struct pagevec, lru_add_pvecs) = { 0, };
-static DEFINE_PER_CPU(struct pagevec, lru_add_active_pvecs) = { 0, };
 
 void fastcall lru_cache_add(struct page *page)
 {
@@ -149,25 +116,12 @@ void fastcall lru_cache_add(struct page 
 	put_cpu_var(lru_add_pvecs);
 }
 
-void fastcall lru_cache_add_active(struct page *page)
-{
-	struct pagevec *pvec = &get_cpu_var(lru_add_active_pvecs);
-
-	page_cache_get(page);
-	if (!pagevec_add(pvec, page))
-		__pagevec_lru_add_active(pvec);
-	put_cpu_var(lru_add_active_pvecs);
-}
-
 void lru_add_drain(void)
 {
 	struct pagevec *pvec = &get_cpu_var(lru_add_pvecs);
 
 	if (pagevec_count(pvec))
 		__pagevec_lru_add(pvec);
-	pvec = &__get_cpu_var(lru_add_active_pvecs);
-	if (pagevec_count(pvec))
-		__pagevec_lru_add_active(pvec);
 	put_cpu_var(lru_add_pvecs);
 }
 
@@ -303,6 +257,7 @@ void __pagevec_lru_add(struct pagevec *p
 		}
 		if (TestSetPageLRU(page))
 			BUG();
+		ClearPageUsedOnce(page);
 		add_page_to_inactive_list(zone, page);
 	}
 	if (zone)
@@ -313,33 +268,6 @@ void __pagevec_lru_add(struct pagevec *p
 
 EXPORT_SYMBOL(__pagevec_lru_add);
 
-void __pagevec_lru_add_active(struct pagevec *pvec)
-{
-	int i;
-	struct zone *zone = NULL;
-
-	for (i = 0; i < pagevec_count(pvec); i++) {
-		struct page *page = pvec->pages[i];
-		struct zone *pagezone = page_zone(page);
-
-		if (pagezone != zone) {
-			if (zone)
-				spin_unlock_irq(&zone->lru_lock);
-			zone = pagezone;
-			spin_lock_irq(&zone->lru_lock);
-		}
-		if (TestSetPageLRU(page))
-			BUG();
-		if (TestSetPageActive(page))
-			BUG();
-		add_page_to_active_list(zone, page);
-	}
-	if (zone)
-		spin_unlock_irq(&zone->lru_lock);
-	release_pages(pvec->pages, pvec->nr, pvec->cold);
-	pagevec_reinit(pvec);
-}
-
 /*
  * Try to drop buffers from the pages in a pagevec
  */
@@ -421,9 +349,6 @@ static void lru_drain_cache(unsigned int
 	/* CPU is dead, so no locking needed. */
 	if (pagevec_count(pvec))
 		__pagevec_lru_add(pvec);
-	pvec = &per_cpu(lru_add_active_pvecs, cpu);
-	if (pagevec_count(pvec))
-		__pagevec_lru_add_active(pvec);
 }
 
 /* Drop the CPU's cached committed space back into the central pool. */
diff -puN mm/swapfile.c~split-active-pages-np2 mm/swapfile.c
--- linux-2.6.7-xx4/mm/swapfile.c~split-active-pages-np2	2004-06-29 00:51:39.900460600 -0400
+++ linux-2.6.7-xx4-xiphux/mm/swapfile.c	2004-06-29 00:51:40.044438712 -0400
@@ -469,10 +469,10 @@ static unsigned long unuse_pmd(struct vm
 			pte_unmap(pte);
 
 			/*
-			 * Move the page to the active list so it is not
-			 * immediately swapped out again after swapon.
+			 * Touch the page so it is not immediately swapped
+			 * out again after swapon.
 			 */
-			activate_page(page);
+			mark_page_accessed(page);
 
 			/* add 1 since address may be 0 */
 			return 1 + offset + address;
diff -puN mm/vmscan.c~split-active-pages-np2 mm/vmscan.c
--- linux-2.6.7-xx4/mm/vmscan.c~split-active-pages-np2	2004-06-29 00:51:39.904459992 -0400
+++ linux-2.6.7-xx4-xiphux/mm/vmscan.c	2004-06-29 00:51:40.046438408 -0400
@@ -57,6 +57,12 @@ struct scan_control {
 	/* Incremented by the number of inactive pages that were scanned */
 	unsigned long nr_scanned;
 
+	/* Number of dirty pages found on the end of the inactive list */
+	unsigned long nr_dirty;
+
+	/* Number of dirty pages we're putting on the inactive list */
+	unsigned long nr_dirty_inactive;
+
 	/* Incremented by the number of pages reclaimed */
 	unsigned long nr_reclaimed;
 
@@ -116,10 +122,9 @@ struct shrinker {
 #endif
 
 /*
- * From 0 .. 100.  Higher means more swappy.
+ * From 1 .. 100.  Higher means less swappy.
  */
-int vm_swappiness = 60;
-static long total_memory;
+int vm_mapped_page_cost = 32;
 
 static LIST_HEAD(shrinker_list);
 static DECLARE_MUTEX(shrinker_sem);
@@ -209,27 +214,6 @@ static int shrink_slab(unsigned long sca
 	return 0;
 }
 
-/* Must be called with page's rmap lock held. */
-static inline int page_mapping_inuse(struct page *page)
-{
-	struct address_space *mapping;
-
-	/* Page is in somebody's page tables. */
-	if (page_mapped(page))
-		return 1;
-
-	/* Be more reluctant to reclaim swapcache than pagecache */
-	if (PageSwapCache(page))
-		return 1;
-
-	mapping = page_mapping(page);
-	if (!mapping)
-		return 0;
-
-	/* File is mmap'd by somebody? */
-	return mapping_mapped(mapping);
-}
-
 static inline int is_page_cache_freeable(struct page *page)
 {
 	return page_count(page) - !!PagePrivate(page) == 2;
@@ -349,7 +333,7 @@ static int shrink_list(struct list_head 
 		struct address_space *mapping;
 		struct page *page;
 		int may_enter_fs;
-		int referenced;
+		int referenced, dirty, mapped;
 
 		page = lru_to_page(page_list);
 		list_del(&page->lru);
@@ -357,22 +341,41 @@ static int shrink_list(struct list_head 
 		if (TestSetPageLocked(page))
 			goto keep;
 
-		BUG_ON(PageActive(page));
+		BUG_ON(PageActiveMapped(page) || PageActiveUnmapped(page));
 
-		if (PageWriteback(page))
-			goto keep_locked;
+		if (PageWriteback(page)) {
+			SetPageReclaim(page);
+			if (likely(PageWriteback(page)))
+				goto keep_locked;
+			/* We've raced. Can fall through but must clear
+			 * PageReclaim */
+			ClearPageReclaim(page);
+		}
+
+		page_map_lock(page);
+		mapped = page_mapped(page);
 
 		sc->nr_scanned++;
-		/* Double the slab pressure for mapped and swapcache pages */
-		if (page_mapped(page) || PageSwapCache(page))
-			sc->nr_scanned++;
+		/* Increase the slab pressure for mapped and swapcache pages */
+		if (mapped || PageSwapCache(page))
+			sc->nr_scanned += vm_mapped_page_cost;
 
-		page_map_lock(page);
-		referenced = page_referenced(page);
-		if (referenced && page_mapping_inuse(page)) {
-			/* In active use or really unfreeable.  Activate it. */
+		page_gather(page, &referenced, &dirty);
+		if (referenced) {
+			/*
+			 * Has been referenced.  Activate used twice or
+			 * mapped pages, otherwise give it another chance
+			 * on the inactive list
+			 */
 			page_map_unlock(page);
-			goto activate_locked;
+			if (TestSetPageUsedOnce(page) || mapped)
+				goto activate_locked;
+
+			if (dirty) {
+				set_page_dirty(page);
+				sc->nr_dirty_inactive++;
+			}
+			goto keep_locked;
 		}
 
 #ifdef CONFIG_SWAP
@@ -413,8 +416,7 @@ static int shrink_list(struct list_head 
 		page_map_unlock(page);
 
 		if (PageDirty(page)) {
-			if (referenced)
-				goto keep_locked;
+			sc->nr_dirty++;
 			if (!may_enter_fs)
 				goto keep_locked;
 			if (laptop_mode && !sc->may_writepage)
@@ -509,7 +511,10 @@ free_it:
 		continue;
 
 activate_locked:
-		SetPageActive(page);
+		if (page_mapped(page))
+			SetPageActiveMapped(page);
+		else
+			SetPageActiveUnmapped(page);
 		pgactivate++;
 keep_locked:
 		unlock_page(page);
@@ -574,7 +579,6 @@ static void shrink_cache(struct zone *zo
 			nr_taken++;
 		}
 		zone->nr_inactive -= nr_taken;
-		zone->pages_scanned += nr_taken;
 		spin_unlock_irq(&zone->lru_lock);
 
 		if (nr_taken == 0)
@@ -600,9 +604,13 @@ static void shrink_cache(struct zone *zo
 			if (TestSetPageLRU(page))
 				BUG();
 			list_del(&page->lru);
-			if (PageActive(page))
-				add_page_to_active_list(zone, page);
-			else
+			if (PageActiveMapped(page)) {
+				ClearPageUsedOnce(page);
+				add_page_to_active_mapped_list(zone, page);
+			} else if (PageActiveUnmapped(page)) {
+				ClearPageUsedOnce(page);
+				add_page_to_active_unmapped_list(zone, page);
+			} else
 				add_page_to_inactive_list(zone, page);
 			if (!pagevec_add(&pvec, page)) {
 				spin_unlock_irq(&zone->lru_lock);
@@ -634,9 +642,9 @@ done:
  * But we had to alter page->flags anyway.
  */
 static void
-refill_inactive_zone(struct zone *zone, struct scan_control *sc)
+shrink_active_list(struct zone *zone, struct list_head *list, unsigned long *nr_list_pages, struct scan_control *sc)
 {
-	int pgmoved;
+	int pgmoved, pgmoved_unmapped;
 	int pgdeactivate = 0;
 	int pgscanned = 0;
 	int nr_pages = sc->nr_to_scan;
@@ -645,17 +653,14 @@ refill_inactive_zone(struct zone *zone, 
 	LIST_HEAD(l_active);	/* Pages to go onto the active_list */
 	struct page *page;
 	struct pagevec pvec;
-	int reclaim_mapped = 0;
-	long mapped_ratio;
-	long distress;
-	long swap_tendency;
 
 	lru_add_drain();
 	pgmoved = 0;
+
 	spin_lock_irq(&zone->lru_lock);
-	while (pgscanned < nr_pages && !list_empty(&zone->active_list)) {
-		page = lru_to_page(&zone->active_list);
-		prefetchw_prev_lru_page(page, &zone->active_list, flags);
+	while (pgscanned < nr_pages && !list_empty(list)) {
+		page = lru_to_page(list);
+		prefetchw_prev_lru_page(page, list, flags);
 		if (!TestClearPageLRU(page))
 			BUG();
 		list_del(&page->lru);
@@ -668,63 +673,23 @@ refill_inactive_zone(struct zone *zone, 
 			 */
 			__put_page(page);
 			SetPageLRU(page);
-			list_add(&page->lru, &zone->active_list);
+			list_add(&page->lru, list);
 		} else {
 			list_add(&page->lru, &l_hold);
 			pgmoved++;
 		}
 		pgscanned++;
 	}
-	zone->nr_active -= pgmoved;
+	*nr_list_pages -= pgmoved;
+	zone->pages_scanned += pgmoved;
 	spin_unlock_irq(&zone->lru_lock);
 
-	/*
-	 * `distress' is a measure of how much trouble we're having reclaiming
-	 * pages.  0 -> no problems.  100 -> great trouble.
-	 */
-	distress = 100 >> zone->prev_priority;
-
-	/*
-	 * The point of this algorithm is to decide when to start reclaiming
-	 * mapped memory instead of just pagecache.  Work out how much memory
-	 * is mapped.
-	 */
-	mapped_ratio = (sc->nr_mapped * 100) / total_memory;
-
-	/*
-	 * Now decide how much we really want to unmap some pages.  The mapped
-	 * ratio is downgraded - just because there's a lot of mapped memory
-	 * doesn't necessarily mean that page reclaim isn't succeeding.
-	 *
-	 * The distress ratio is important - we don't want to start going oom.
-	 *
-	 * A 100% value of vm_swappiness overrides this algorithm altogether.
-	 */
-	swap_tendency = mapped_ratio / 2 + distress + vm_swappiness;
-
-	/*
-	 * Now use this metric to decide whether to start moving mapped memory
-	 * onto the inactive list.
-	 */
-	if (swap_tendency >= 100)
-		reclaim_mapped = 1;
-
 	while (!list_empty(&l_hold)) {
+		int referenced, dirty;
+
 		page = lru_to_page(&l_hold);
 		list_del(&page->lru);
-		if (page_mapped(page)) {
-			if (!reclaim_mapped) {
-				list_add(&page->lru, &l_active);
-				continue;
-			}
-			page_map_lock(page);
-			if (page_referenced(page)) {
-				page_map_unlock(page);
-				list_add(&page->lru, &l_active);
-				continue;
-			}
-			page_map_unlock(page);
-		}
+
 		/*
 		 * FIXME: need to consider page_count(page) here if/when we
 		 * reap orphaned pages via the LRU (Daniel's locking stuff)
@@ -733,6 +698,20 @@ refill_inactive_zone(struct zone *zone, 
 			list_add(&page->lru, &l_active);
 			continue;
 		}
+
+		page_map_lock(page);
+		page_gather(page, &referenced, &dirty);
+		page_map_unlock(page);
+
+		if (referenced) {
+			list_add(&page->lru, &l_active);
+			continue;
+		}
+		if (dirty) {
+			set_page_dirty(page);
+			sc->nr_dirty_inactive++;
+		}
+
 		list_add(&page->lru, &l_inactive);
 	}
 
@@ -744,7 +723,8 @@ refill_inactive_zone(struct zone *zone, 
 		prefetchw_prev_lru_page(page, &l_inactive, flags);
 		if (TestSetPageLRU(page))
 			BUG();
-		if (!TestClearPageActive(page))
+		if (!TestClearPageActiveMapped(page)
+				&& !TestClearPageActiveUnmapped(page))
 			BUG();
 		list_move(&page->lru, &zone->inactive_list);
 		pgmoved++;
@@ -768,23 +748,37 @@ refill_inactive_zone(struct zone *zone, 
 	}
 
 	pgmoved = 0;
+	pgmoved_unmapped = 0;
 	while (!list_empty(&l_active)) {
 		page = lru_to_page(&l_active);
 		prefetchw_prev_lru_page(page, &l_active, flags);
 		if (TestSetPageLRU(page))
 			BUG();
-		BUG_ON(!PageActive(page));
-		list_move(&page->lru, &zone->active_list);
-		pgmoved++;
+		if(!TestClearPageActiveMapped(page)
+				&& !TestClearPageActiveUnmapped(page))
+			BUG();
+		if (page_mapped(page)) {
+			SetPageActiveMapped(page);
+			list_move(&page->lru, &zone->active_mapped_list);
+			pgmoved++;
+		} else {
+			SetPageActiveUnmapped(page);
+			list_move(&page->lru, &zone->active_unmapped_list);
+			pgmoved_unmapped++;
+		}
+
 		if (!pagevec_add(&pvec, page)) {
-			zone->nr_active += pgmoved;
+			zone->nr_active_mapped += pgmoved;
 			pgmoved = 0;
+			zone->nr_active_unmapped += pgmoved_unmapped;
+			pgmoved_unmapped = 0;
 			spin_unlock_irq(&zone->lru_lock);
 			__pagevec_release(&pvec);
 			spin_lock_irq(&zone->lru_lock);
 		}
 	}
-	zone->nr_active += pgmoved;
+	zone->nr_active_mapped += pgmoved;
+	zone->nr_active_unmapped += pgmoved_unmapped;
 	spin_unlock_irq(&zone->lru_lock);
 	pagevec_release(&pvec);
 
@@ -798,46 +792,121 @@ refill_inactive_zone(struct zone *zone, 
 static void
 shrink_zone(struct zone *zone, struct scan_control *sc)
 {
+	unsigned long long tmp;
+	unsigned long scan_active, scan_active_mapped, scan_active_unmapped;
+	unsigned long scan_inactive;
 	unsigned long nr_active;
-	unsigned long nr_inactive;
+	int count;
 
-	/*
-	 * Add one to `nr_to_scan' just to make sure that the kernel will
-	 * slowly sift through the active list.
-	 */
-	zone->nr_scan_active += (zone->nr_active >> sc->priority) + 1;
-	nr_active = zone->nr_scan_active;
-	if (nr_active >= SWAP_CLUSTER_MAX)
-		zone->nr_scan_active = 0;
+	nr_active = zone->nr_active_mapped + zone->nr_active_unmapped;
+	scan_inactive = (nr_active + zone->nr_inactive) >> sc->priority;
+
+	if (nr_active >= (zone->nr_inactive*2 + 1)) {
+		/*
+		 * Add one to `nr_to_scan' just to make sure that the kernel
+		 * will slowly sift through the active list.
+		 */
+		if (nr_active >= 4*(zone->nr_inactive*2 + 1)) {
+			/* Don't scan more than 4 times inactive list scan */
+			scan_active = 4*scan_inactive;
+		} else {
+			/* Cast to long long so the multiply doesn't overflow */
+			tmp = (unsigned long long)scan_inactive * nr_active;
+			do_div(tmp, zone->nr_inactive*2 + 1);
+			scan_active = (unsigned long)tmp;
+		}
+
+		tmp = scan_active * zone->nr_active_mapped;
+		do_div(tmp, nr_active + 1);
+		scan_active_mapped = ((unsigned long)tmp + 1)
+						/ vm_mapped_page_cost;
+		scan_active_unmapped = scan_active - tmp + 1;
+	} else {
+		/* Don't scan the active list if the inactive list is large */
+		scan_active_mapped = scan_active_unmapped = 0;
+		if (sc->priority == DEF_PRIORITY) {
+			/* Keep things ticking */
+			scan_active_mapped++;
+			scan_active_unmapped++;
+		}
+	}
+
+	zone->nr_scan_active_mapped += scan_active_mapped;
+	scan_active_mapped = zone->nr_scan_active_mapped;
+	if (scan_active_mapped >= SWAP_CLUSTER_MAX)
+		zone->nr_scan_active_mapped = 0;
 	else
-		nr_active = 0;
+		scan_active_mapped = 0;
 
-	zone->nr_scan_inactive += (zone->nr_inactive >> sc->priority) + 1;
-	nr_inactive = zone->nr_scan_inactive;
-	if (nr_inactive >= SWAP_CLUSTER_MAX)
+	zone->nr_scan_active_unmapped += scan_active_unmapped;
+	scan_active_unmapped = zone->nr_scan_active_unmapped;
+	if (scan_active_unmapped >= SWAP_CLUSTER_MAX)
+		zone->nr_scan_active_unmapped = 0;
+	else
+		scan_active_unmapped = 0;
+
+	zone->nr_scan_inactive += scan_inactive;
+	scan_inactive = zone->nr_scan_inactive;
+	if (scan_inactive >= SWAP_CLUSTER_MAX)
 		zone->nr_scan_inactive = 0;
 	else
-		nr_inactive = 0;
+		scan_inactive = 0;
 
-	sc->nr_to_reclaim = SWAP_CLUSTER_MAX;
+	sc->nr_dirty_inactive = 0;
 
-	while (nr_active || nr_inactive) {
-		if (nr_active) {
-			sc->nr_to_scan = min(nr_active,
+	while (scan_active_mapped || scan_active_unmapped || scan_inactive) {
+		if (scan_active_mapped) {
+			sc->nr_to_scan = min(scan_active_mapped,
+					(unsigned long)SWAP_CLUSTER_MAX);
+			scan_active_mapped -= sc->nr_to_scan;
+			shrink_active_list(zone, &zone->active_mapped_list,
+					&zone->nr_active_mapped, sc);
+		}
+		if (scan_active_unmapped) {
+			sc->nr_to_scan = min(scan_active_unmapped,
 					(unsigned long)SWAP_CLUSTER_MAX);
-			nr_active -= sc->nr_to_scan;
-			refill_inactive_zone(zone, sc);
+			scan_active_unmapped -= sc->nr_to_scan;
+			shrink_active_list(zone, &zone->active_unmapped_list,
+					&zone->nr_active_unmapped, sc);
 		}
 
-		if (nr_inactive) {
-			sc->nr_to_scan = min(nr_inactive,
+		if (scan_inactive) {
+			sc->nr_to_scan = min(scan_inactive,
 					(unsigned long)SWAP_CLUSTER_MAX);
-			nr_inactive -= sc->nr_to_scan;
+			scan_inactive -= sc->nr_to_scan;
 			shrink_cache(zone, sc);
 			if (sc->nr_to_reclaim <= 0)
 				break;
 		}
 	}
+	zone->nr_scan_active_mapped += scan_active_mapped;
+	zone->nr_scan_active_unmapped += scan_active_unmapped;
+	zone->nr_scan_inactive += scan_inactive;
+
+	/*
+	 * Try to write back as many pages as the number of dirty ones
+	 * we're adding to the inactive list.  This tends to cause slow
+	 * streaming writers to write data to the disk smoothly, at the
+	 * dirtying rate, which is nice.   But that's undesirable in
+	 * laptop mode, where we *want* lumpy writeout.  So in laptop
+	 * mode, write out the whole world.
+	 */
+	zone->nr_dirty_inactive += sc->nr_dirty_inactive;
+	count = zone->nr_dirty_inactive;
+	if (count > zone->nr_inactive / 2
+		|| (!(laptop_mode && !sc->may_writepage)
+			&& count > SWAP_CLUSTER_MAX)) {
+		zone->nr_dirty_inactive = 0;
+		wakeup_bdflush(laptop_mode ? 0 : count*2);
+		sc->may_writepage = 1;
+	}
+
+	if (sc->nr_reclaimed) {
+		zone->all_unreclaimable = 0;
+		zone->pages_scanned = 0;
+	}
+	if (zone->pages_scanned > zone->present_pages)
+		zone->all_unreclaimable = 1;
 }
 
 /*
@@ -864,10 +933,6 @@ shrink_caches(struct zone **zones, struc
 	for (i = 0; zones[i] != NULL; i++) {
 		struct zone *zone = zones[i];
 
-		zone->temp_priority = sc->priority;
-		if (zone->prev_priority > sc->priority)
-			zone->prev_priority = sc->priority;
-
 		if (zone->all_unreclaimable && sc->priority != DEF_PRIORITY)
 			continue;	/* Let kswapd poll it */
 
@@ -896,17 +961,17 @@ int try_to_free_pages(struct zone **zone
 	int total_scanned = 0, total_reclaimed = 0;
 	struct reclaim_state *reclaim_state = current->reclaim_state;
 	struct scan_control sc;
-	int i;
 
+	sc.nr_to_reclaim = SWAP_CLUSTER_MAX;
 	sc.gfp_mask = gfp_mask;
 	sc.may_writepage = 0;
+	sc.nr_dirty = 0;
 
 	inc_page_state(allocstall);
 
-	for (i = 0; zones[i] != 0; i++)
-		zones[i]->temp_priority = DEF_PRIORITY;
-
 	for (priority = DEF_PRIORITY; priority >= 0; priority--) {
+		int threshold;
+
 		sc.nr_mapped = read_page_state(nr_mapped);
 		sc.nr_scanned = 0;
 		sc.nr_reclaimed = 0;
@@ -917,21 +982,22 @@ int try_to_free_pages(struct zone **zone
 			sc.nr_reclaimed += reclaim_state->reclaimed_slab;
 			reclaim_state->reclaimed_slab = 0;
 		}
-		if (sc.nr_reclaimed >= SWAP_CLUSTER_MAX) {
+		total_scanned += sc.nr_scanned;
+		total_reclaimed += sc.nr_reclaimed;
+		if (total_reclaimed >= SWAP_CLUSTER_MAX) {
 			ret = 1;
 			goto out;
 		}
-		total_scanned += sc.nr_scanned;
-		total_reclaimed += sc.nr_reclaimed;
 
 		/*
-		 * Try to write back as many pages as we just scanned.  This
-		 * tends to cause slow streaming writers to write data to the
-		 * disk smoothly, at the dirtying rate, which is nice.   But
-		 * that's undesirable in laptop mode, where we *want* lumpy
-		 * writeout.  So in laptop mode, write out the whole world.
+		 * If we're in laptop mode, and more than a quarter of
+		 * the pages we're scanning are dirty, start writing.
 		 */
-		if (total_scanned > SWAP_CLUSTER_MAX + SWAP_CLUSTER_MAX/2) {
+		threshold = sc.nr_dirty * 10;
+		if (laptop_mode)
+			threshold = sc.nr_dirty * 2;
+		if (total_scanned > SWAP_CLUSTER_MAX
+				&& total_scanned < threshold) {
 			wakeup_bdflush(laptop_mode ? 0 : total_scanned);
 			sc.may_writepage = 1;
 		}
@@ -943,8 +1009,6 @@ int try_to_free_pages(struct zone **zone
 	if ((gfp_mask & __GFP_FS) && !(gfp_mask & __GFP_NORETRY))
 		out_of_memory();
 out:
-	for (i = 0; zones[i] != 0; i++)
-		zones[i]->prev_priority = zones[i]->temp_priority;
 	return ret;
 }
 
@@ -975,6 +1039,7 @@ out:
  */
 static int balance_pgdat(pg_data_t *pgdat, int nr_pages)
 {
+	int all_zones_ok;
 	int to_free = nr_pages;
 	int priority;
 	int i;
@@ -984,20 +1049,15 @@ static int balance_pgdat(pg_data_t *pgda
 
 	sc.gfp_mask = GFP_KERNEL;
 	sc.may_writepage = 0;
+	sc.nr_dirty = 0;
 	sc.nr_mapped = read_page_state(nr_mapped);
 
 	inc_page_state(pageoutrun);
 
-	for (i = 0; i < pgdat->nr_zones; i++) {
-		struct zone *zone = pgdat->node_zones + i;
-
-		zone->temp_priority = DEF_PRIORITY;
-	}
-
 	for (priority = DEF_PRIORITY; priority >= 0; priority--) {
-		int all_zones_ok = 1;
 		int end_zone = 0;	/* Inclusive.  0 = ZONE_DMA */
 
+		all_zones_ok = 1;
 
 		if (nr_pages == 0) {
 			/*
@@ -1031,18 +1091,20 @@ scan:
 		 * cause too much scanning of the lower zones.
 		 */
 		for (i = 0; i <= end_zone; i++) {
+			int threshold;
 			struct zone *zone = pgdat->node_zones + i;
 
 			if (zone->all_unreclaimable && priority != DEF_PRIORITY)
 				continue;
 
 			if (nr_pages == 0) {	/* Not software suspend */
-				if (zone->free_pages <= zone->pages_high)
+				if (zone->free_pages < zone->pages_high)
 					all_zones_ok = 0;
-			}
-			zone->temp_priority = priority;
-			if (zone->prev_priority > priority)
-				zone->prev_priority = priority;
+				sc.nr_to_reclaim = zone->pages_high -
+							zone->free_pages;
+			} else
+				sc.nr_to_reclaim = INT_MAX;
+
 			sc.nr_scanned = 0;
 			sc.nr_reclaimed = 0;
 			sc.priority = priority;
@@ -1051,18 +1113,18 @@ scan:
 			shrink_slab(sc.nr_scanned, GFP_KERNEL);
 			sc.nr_reclaimed += reclaim_state->reclaimed_slab;
 			total_reclaimed += sc.nr_reclaimed;
-			if (zone->all_unreclaimable)
-				continue;
-			if (zone->pages_scanned > zone->present_pages * 2)
-				zone->all_unreclaimable = 1;
+
 			/*
-			 * If we've done a decent amount of scanning and
-			 * the reclaim ratio is low, start doing writepage
-			 * even in laptop mode
+			 * If we're in laptop mode, and more than a quarter of
+			 * the pages we're scanning are dirty, start writing.
 			 */
-			if (total_scanned > SWAP_CLUSTER_MAX * 2 &&
-			    total_scanned > total_reclaimed+total_reclaimed/2)
+			if (laptop_mode)
+				threshold = sc.nr_dirty * 2;
+			if (total_scanned > SWAP_CLUSTER_MAX
+					&& total_scanned < threshold) {
+				wakeup_bdflush(laptop_mode ? 0 : total_scanned);
 				sc.may_writepage = 1;
+			}
 		}
 		if (nr_pages && to_free > total_reclaimed)
 			continue;	/* swsusp: need to do more work */
@@ -1075,12 +1137,8 @@ scan:
 		if (total_scanned && priority < DEF_PRIORITY - 2)
 			blk_congestion_wait(WRITE, HZ/10);
 	}
-out:
-	for (i = 0; i < pgdat->nr_zones; i++) {
-		struct zone *zone = pgdat->node_zones + i;
 
-		zone->prev_priority = zone->temp_priority;
-	}
+out:
 	return total_reclaimed;
 }
 
@@ -1144,8 +1202,6 @@ static int kswapd(void *p)
  */
 void wakeup_kswapd(struct zone *zone)
 {
-	if (zone->free_pages > zone->pages_low)
-		return;
 	if (!waitqueue_active(&zone->zone_pgdat->kswapd_wait))
 		return;
 	wake_up_interruptible(&zone->zone_pgdat->kswapd_wait);
@@ -1210,7 +1266,6 @@ static int __init kswapd_init(void)
 	for_each_pgdat(pgdat)
 		pgdat->kswapd
 		= find_task_by_pid(kernel_thread(kswapd, pgdat, CLONE_KERNEL));
-	total_memory = nr_free_pagecache_pages();
 	hotcpu_notifier(cpu_callback, 0);
 	return 0;
 }
diff -puN fs/exec.c~split-active-pages-np2 fs/exec.c
--- linux-2.6.7-xx4/fs/exec.c~split-active-pages-np2	2004-06-29 00:51:39.907459536 -0400
+++ linux-2.6.7-xx4-xiphux/fs/exec.c	2004-06-29 00:51:40.048438104 -0400
@@ -320,7 +320,8 @@ void install_arg_page(struct vm_area_str
 		goto out;
 	}
 	mm->rss++;
-	lru_cache_add_active(page);
+	lru_cache_add(page);
+	mark_page_accessed(page);
 	set_pte(pte, pte_mkdirty(pte_mkwrite(mk_pte(
 					page, vma->vm_page_prot))));
 	page_add_anon_rmap(page, vma, address);
diff -puN mm/memory.c~split-active-pages-np2 mm/memory.c
--- linux-2.6.7-xx4/mm/memory.c~split-active-pages-np2	2004-06-29 00:51:39.910459080 -0400
+++ linux-2.6.7-xx4-xiphux/mm/memory.c	2004-06-29 00:51:40.050437800 -0400
@@ -1110,7 +1110,8 @@ static int do_wp_page(struct mm_struct *
 		else
 			page_remove_rmap(old_page);
 		break_cow(vma, new_page, address, page_table);
-		lru_cache_add_active(new_page);
+		lru_cache_add(new_page);
+		mark_page_accessed(new_page);
 		page_add_anon_rmap(new_page, vma, address);
 
 		/* Free the old page.. */
@@ -1474,7 +1475,7 @@ do_anonymous_page(struct mm_struct *mm, 
 		entry = maybe_mkwrite(pte_mkdirty(mk_pte(page,
 							 vma->vm_page_prot)),
 				      vma);
-		lru_cache_add_active(page);
+		lru_cache_add(page);
 		mark_page_accessed(page);
 		page_add_anon_rmap(page, vma, addr);
 	}
@@ -1586,7 +1587,7 @@ retry:
 			entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 		set_pte(page_table, entry);
 		if (anon) {
-			lru_cache_add_active(new_page);
+			lru_cache_add(new_page);
 			page_add_anon_rmap(new_page, vma, address);
 		} else
 			page_add_file_rmap(new_page);
diff -puN mm/swap_state.c~split-active-pages-np2 mm/swap_state.c
--- linux-2.6.7-xx4/mm/swap_state.c~split-active-pages-np2	2004-06-29 00:51:45.720575808 -0400
+++ linux-2.6.7-xx4-xiphux/mm/swap_state.c	2004-06-29 00:52:13.923288344 -0400
@@ -375,7 +375,8 @@ struct page *read_swap_cache_async(swp_e
 			/*
 			 * Initiate read into locked page and return.
 			 */
-			lru_cache_add_active(new_page);
+			lru_cache_add(new_page);
+			mark_page_accessed(new_page);
 			swap_readpage(NULL, new_page);
 			return new_page;
 		}

_
