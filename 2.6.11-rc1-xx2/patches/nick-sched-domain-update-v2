Index: xx-sources/include/linux/sched.h
===================================================================
--- xx-sources.orig/include/linux/sched.h	2005-01-14 23:50:38.388848224 +0000
+++ xx-sources/include/linux/sched.h	2005-01-14 23:55:50.507398992 +0000
@@ -415,7 +415,6 @@
 #define SD_WAKE_IDLE		8	/* Wake to idle CPU on task wakeup */
 #define SD_WAKE_AFFINE		16	/* Wake task to waking CPU */
 #define SD_WAKE_BALANCE		32	/* Perform balancing at task wakeup */
-#define SD_SHARE_CPUPOWER	64	/* Domain members share cpu power */
 
 struct sched_group {
 	struct sched_group *next;	/* Must be a circular list */
@@ -439,7 +438,9 @@
 	unsigned int imbalance_pct;	/* No balance until over watermark */
 	unsigned long long cache_hot_time; /* Task considered cache hot (ns) */
 	unsigned int cache_nice_tries;	/* Leave cache hot tasks for # tries */
-	unsigned int per_cpu_gain;	/* CPU % gained by adding domain cpus */
+	unsigned int load_idx;
+	unsigned int idle_load_idx;
+	unsigned int wake_balance_load_idx;
 	int flags;			/* See SD_* */
 
 	/* Runtime fields. */
@@ -451,17 +452,26 @@
 	/* load_balance() stats */
 	unsigned long lb_cnt[MAX_IDLE_TYPES];
 	unsigned long lb_failed[MAX_IDLE_TYPES];
+	unsigned long lb_balanced[MAX_IDLE_TYPES];
 	unsigned long lb_imbalance[MAX_IDLE_TYPES];
+	unsigned long lb_gained[MAX_IDLE_TYPES];
+	unsigned long lb_hot_gained[MAX_IDLE_TYPES];
 	unsigned long lb_nobusyg[MAX_IDLE_TYPES];
 	unsigned long lb_nobusyq[MAX_IDLE_TYPES];
 
+	/* Active load balancing */
+	unsigned long alb_cnt;
+	unsigned long alb_failed;
+	unsigned long alb_pushed;
+
 	/* sched_balance_exec() stats */
 	unsigned long sbe_attempts;
 	unsigned long sbe_pushed;
 
 	/* try_to_wake_up() stats */
-	unsigned long ttwu_wake_affine;
-	unsigned long ttwu_wake_balance;
+	unsigned long ttwu_wake_remote;
+	unsigned long ttwu_move_affine;
+	unsigned long ttwu_move_balance;
 #endif
 };
 
Index: xx-sources/include/linux/schedstats.h
===================================================================
--- xx-sources.orig/include/linux/schedstats.h	2005-01-14 23:49:42.319372080 +0000
+++ xx-sources/include/linux/schedstats.h	2005-01-14 23:50:38.673804904 +0000
@@ -26,35 +26,13 @@
 	unsigned long yld_cnt;
 
 	/* schedule() stats */
-	unsigned long sched_noswitch;
 	unsigned long sched_switch;
 	unsigned long sched_cnt;
 	unsigned long sched_goidle;
 
-	/* pull_task() stats */
-	unsigned long pt_gained[MAX_IDLE_TYPES];
-	unsigned long pt_lost[MAX_IDLE_TYPES];
-
-	/* active_load_balance() stats */
-	unsigned long alb_cnt;
-	unsigned long alb_lost;
-	unsigned long alb_gained;
-	unsigned long alb_failed;
-
 	/* try_to_wake_up() stats */
 	unsigned long ttwu_cnt;
-	unsigned long ttwu_attempts;
-	unsigned long ttwu_moved;
-
-	/* wake_up_new_task() stats */
-	unsigned long wunt_cnt;
-	unsigned long wunt_moved;
-
-	/* sched_migrate_task() stats */
-	unsigned long smt_cnt;
-
-	/* sched_balance_exec() stats */
-	unsigned long sbe_cnt;
+	unsigned long ttwu_local;
 };
 
 extern struct file_operations proc_schedstat_operations;
@@ -66,11 +44,11 @@
 extern void sched_info_switch(task_t *prev, task_t *next);
 extern void sched_info_queued(task_t *t);
 
-# define schedstat_inc(sspcd, field)	sspcd->field++;
-# define schedstat_add(sspcd, field, amt)	sspcd->field += amt;
+# define schedstat_inc(sspcd, field)	do { sspcd->field++; } while (0)
+# define schedstat_add(sspcd, field, amt)	do { sspcd->field += amt; } while (0)
 #else /* !CONFIG_SCHEDSTATS */
-# define schedstat_inc(sspcd, field)	do { } while (0);
-# define schedstat_add(sspcd, field, amt)	do { } while (0);
+# define schedstat_inc(sspcd, field)	do { } while (0)
+# define schedstat_add(sspcd, field, amt)	do { } while (0)
 # define sched_info_queued(t)		do { } while (0)
 # define sched_info_switch(t, next)	do { } while (0)
 #endif
Index: xx-sources/include/linux/topology.h
===================================================================
--- xx-sources.orig/include/linux/topology.h	2005-01-14 23:49:02.229466672 +0000
+++ xx-sources/include/linux/topology.h	2005-01-14 23:50:38.673804904 +0000
@@ -85,13 +85,13 @@
 	.imbalance_pct		= 110,			\
 	.cache_hot_time		= 0,			\
 	.cache_nice_tries	= 0,			\
-	.per_cpu_gain		= 25,			\
+	.load_idx		= 0,			\
+	.idle_load_idx		= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
 				| SD_WAKE_AFFINE	\
-				| SD_WAKE_IDLE		\
-				| SD_SHARE_CPUPOWER,	\
+				| SD_WAKE_IDLE,		\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 	.nr_balance_failed	= 0,			\
@@ -111,7 +111,9 @@
 	.imbalance_pct		= 125,			\
 	.cache_hot_time		= (5*1000000/2),	\
 	.cache_nice_tries	= 1,			\
-	.per_cpu_gain		= 100,			\
+	.load_idx		= 2,			\
+	.idle_load_idx		= 1,			\
+	.wake_balance_load_idx	= 0,			\
 	.flags			= SD_LOAD_BALANCE	\
 				| SD_BALANCE_NEWIDLE	\
 				| SD_BALANCE_EXEC	\
Index: xx-sources/kernel/nicksched.c
===================================================================
--- xx-sources.orig/kernel/nicksched.c	2005-01-14 23:50:38.390847920 +0000
+++ xx-sources/kernel/nicksched.c	2005-01-14 23:55:50.506399144 +0000
@@ -155,7 +155,7 @@
 	 */
 	unsigned long nr_running;
 #ifdef CONFIG_SMP
-	unsigned long cpu_load;
+	unsigned long cpu_load[4];
 #endif
 	unsigned long array_sequence;
 
@@ -773,23 +773,23 @@
  * We want to under-estimate the load of migration sources, to
  * balance conservatively.
  */
-static inline unsigned long source_load(int cpu)
+static inline unsigned long source_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return min(rq->cpu_load, load_now);
+	return min(rq->cpu_load[type], load_now);
 }
 
 /*
  * Return a high guess at the load of a migration-target cpu
  */
-static inline unsigned long target_load(int cpu)
+static inline unsigned long target_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return max(rq->cpu_load, load_now);
+	return max(rq->cpu_load[type], load_now);
 }
 
 #endif
@@ -854,12 +854,11 @@
 	runqueue_t *rq;
 #ifdef CONFIG_SMP
 	unsigned long load, this_load;
-	struct sched_domain *sd;
+	struct sched_domain *sd, *this_sd = NULL;
 	int new_cpu;
 #endif
 
 	rq = task_rq_lock(p, &flags);
-	schedstat_inc(rq->sspcd, ttwu_cnt);
 	old_state = p->state;
 	if (!(old_state & state))
 		goto out;
@@ -874,68 +873,78 @@
 	if (unlikely(task_running(rq, p)))
 		goto out_activate;
 
+	schedstat_inc(rq->sspcd, ttwu_cnt);
+	if (cpu == this_cpu) {
+		schedstat_inc(rq->sspcd, ttwu_local);
+	} else {
+		for_each_domain(this_cpu, sd) {
+			if (cpu_isset(cpu, sd->span)) {
+				schedstat_inc(sd, ttwu_wake_remote);
+				this_sd = sd;
+				break;
+			}
+		}
+	}
+ 
 	new_cpu = cpu;
-
 	if (cpu == this_cpu || unlikely(!cpu_isset(this_cpu, p->cpus_allowed)))
 		goto out_set_cpu;
 
-	load = source_load(cpu);
-	this_load = target_load(this_cpu);
-
 	/*
-	 * If sync wakeup then subtract the (maximum possible) effect of
-	 * the currently running task from the load of the current CPU:
+	 * Check for affine wakeup and passive balancing possibilities.
 	 */
-	if (sync)
-		this_load -= SCHED_LOAD_SCALE;
+	if (this_sd) {
+		unsigned int imbalance;
 
-	/* Don't pull the task off an idle CPU to a busy one */
-	if (load < SCHED_LOAD_SCALE/2 && this_load > SCHED_LOAD_SCALE/2)
-		goto out_set_cpu;
+		load = source_load(cpu, this_sd->wake_balance_load_idx);
+		this_load = target_load(this_cpu, this_sd->wake_balance_load_idx);
 
-	new_cpu = this_cpu; /* Wake to this CPU if we can */
+		/* Don't pull the task off an idle CPU to a busy one */
+		if (load < SCHED_LOAD_SCALE
+				&& load + this_load > SCHED_LOAD_SCALE
+				&& this_load > load)
+			goto out_set_cpu;
+
+		new_cpu = this_cpu; /* Wake to this CPU if we can */
 
-	/*
-	 * Scan domains for affine wakeup and passive balancing
-	 * possibilities.
-	 */
-	for_each_domain(this_cpu, sd) {
-		unsigned int imbalance;
 		/*
 		 * Start passive balancing when half the imbalance_pct
 		 * limit is reached.
 		 */
-		imbalance = sd->imbalance_pct + (sd->imbalance_pct - 100) / 2;
+		imbalance = 100 + (this_sd->imbalance_pct - 100) / 2;
 
-		if ((sd->flags & SD_WAKE_AFFINE) &&
-				!task_hot(p, rq->timestamp_last_tick, sd)) {
-			/*
-			 * This domain has SD_WAKE_AFFINE and p is cache cold
-			 * in this domain.
-			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_affine);
+		if ((this_sd->flags & SD_WAKE_AFFINE) &&
+			!task_hot(p, rq->timestamp_last_tick, this_sd)) {
+			unsigned long load, this_load;
+			load = target_load(cpu, this_sd->wake_balance_load_idx);
+			this_load = source_load(this_cpu, this_sd->wake_balance_load_idx);
+
+			if (imbalance*load > 100*this_load) {
+				/*
+				 * This domain has SD_WAKE_AFFINE and
+				 * p is cache cold in this domain, and
+				 * there is no bad imbalance.
+				 */
+				schedstat_inc(this_sd, ttwu_move_affine);
 				goto out_set_cpu;
 			}
-		} else if ((sd->flags & SD_WAKE_BALANCE) &&
+		}
+
+		if ((this_sd->flags & SD_WAKE_BALANCE) &&
 				imbalance*this_load <= 100*load) {
 			/*
 			 * This domain has SD_WAKE_BALANCE and there is
 			 * an imbalance.
 			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_balance);
-				goto out_set_cpu;
-			}
+			schedstat_inc(this_sd, ttwu_move_balance);
+			goto out_set_cpu;
 		}
 	}
 
 	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
 out_set_cpu:
-	schedstat_inc(rq->sspcd, ttwu_attempts);
 	new_cpu = wake_idle(new_cpu, p);
 	if (new_cpu != cpu) {
-		schedstat_inc(rq->sspcd, ttwu_moved);
 		set_task_cpu(p, new_cpu);
 		task_rq_unlock(rq, &flags);
 		/* might preempt at this point */
@@ -1348,7 +1357,7 @@
 	cpus_and(mask, sd->span, p->cpus_allowed);
 
 	for_each_cpu_mask(i, mask) {
-		load = target_load(i);
+		load = target_load(i, sd->wake_balance_load_idx);
 
 		if (load < min_load) {
 			min_cpu = i;
@@ -1361,7 +1370,7 @@
 	}
 
 	/* add +1 to account for the new task */
-	this_load = source_load(this_cpu) + SCHED_LOAD_SCALE;
+	this_load = source_load(this_cpu, sd->wake_balance_load_idx) + SCHED_LOAD_SCALE;
 
 	/*
 	 * Would with the addition of the new task to the
@@ -1569,13 +1578,10 @@
 		goto skip_bitmap;
 	}
 
-	/*
-	 * Right now, this is the only place pull_task() is called,
-	 * so we can safely collect pull_task() stats here rather than
-	 * inside pull_task().
-	 */
-	schedstat_inc(this_rq->sspcd, pt_gained[idle]);
-	schedstat_inc(busiest->sspcd, pt_lost[idle]);
+#ifdef CONFIG_SCHEDSTATS
+	if (task_hot(tmp, busiest->timestamp_last_tick, sd))
+		schedstat_inc(sd, lb_hot_gained[idle]);
+#endif
 
 	pull_task(busiest, array, tmp, this_rq, dst_array, this_cpu);
 	pulled++;
@@ -1588,6 +1594,13 @@
 		goto skip_bitmap;
 	}
 out:
+	/*
+	 * Right now, this is the only place pull_task() is called,
+	 * so we can safely collect pull_task() stats here rather than
+	 * inside pull_task().
+	 */
+	schedstat_add(sd, lb_gained[idle], pulled);
+
 	return pulled;
 }
 
@@ -1602,8 +1615,11 @@
 {
 	struct sched_group *busiest = NULL, *this = NULL, *group = sd->groups;
 	unsigned long max_load, avg_load, total_load, this_load, total_pwr;
+	int load_idx = sd->idle_load_idx;
 
 	max_load = this_load = total_load = total_pwr = 0;
+	if (idle == NOT_IDLE)
+		load_idx = sd->load_idx;
 
 	do {
 		unsigned long load;
@@ -1618,9 +1634,9 @@
 		for_each_cpu_mask(i, group->cpumask) {
 			/* Bias balancing toward cpus of our domain */
 			if (local_group)
-				load = target_load(i);
+				load = target_load(i, load_idx);
 			else
-				load = source_load(i);
+				load = source_load(i, load_idx);
 
 			nr_cpus++;
 			avg_load += load;
@@ -1673,7 +1689,7 @@
 	*imbalance = (*imbalance * min(busiest->cpu_power, this->cpu_power))
 				/ SCHED_LOAD_SCALE;
 
-	if (*imbalance < SCHED_LOAD_SCALE - 1) {
+	if (*imbalance < SCHED_LOAD_SCALE) {
 		unsigned long pwr_now = 0, pwr_move = 0;
 		unsigned long tmp;
 
@@ -1714,13 +1730,12 @@
 	}
 
 	/* Get rid of the scaling factor, rounding down as we divide */
-	*imbalance = (*imbalance + 1) / SCHED_LOAD_SCALE;
+	*imbalance = *imbalance / SCHED_LOAD_SCALE;
 
 	return busiest;
 
 out_balanced:
-	if (busiest && (idle == NEWLY_IDLE ||
-			(idle == SCHED_IDLE && max_load > SCHED_LOAD_SCALE)) ) {
+	if (busiest && idle != NOT_IDLE && max_load > SCHED_LOAD_SCALE) {
 		*imbalance = 1;
 		return busiest;
 	}
@@ -1739,7 +1754,7 @@
 	int i;
 
 	for_each_cpu_mask(i, group->cpumask) {
-		load = source_load(i);
+		load = source_load(i, 0);
 
 		if (load > max_load) {
 			max_load = load;
@@ -1844,6 +1859,8 @@
 	return nr_moved;
 
 out_balanced:
+	schedstat_inc(sd, lb_balanced[idle]);
+
 	/* tune up the balancing interval */
 	if (sd->balance_interval < sd->max_interval)
 		sd->balance_interval *= 2;
@@ -1887,6 +1904,8 @@
 					imbalance, sd, NEWLY_IDLE);
 	if (!nr_moved)
 		schedstat_inc(sd, lb_failed[NEWLY_IDLE]);
+	else
+		schedstat_inc(sd, lb_balanced[NEWLY_IDLE]);
 
 	spin_unlock(&busiest->lock);
 
@@ -1963,10 +1982,9 @@
 				double_lock_balance(busiest_rq, target_rq);
 				if (move_tasks(target_rq, cpu, busiest_rq,
 						1, sd, SCHED_IDLE)) {
-					schedstat_inc(busiest_rq->sspcd, alb_lost);
-					schedstat_inc(target_rq->sspcd, alb_gained);
+					schedstat_inc(sd, alb_pushed);
 				} else {
-					schedstat_inc(busiest_rq->sspcd, alb_failed);
+					schedstat_inc(sd, alb_failed);
 				}
 				spin_unlock(&target_rq->lock);
 			}
@@ -1993,18 +2011,23 @@
 	unsigned long old_load, this_load;
 	unsigned long j = jiffies + CPU_OFFSET(this_cpu);
 	struct sched_domain *sd;
+	int i;
 
-	/* Update our load */
-	old_load = this_rq->cpu_load;
 	this_load = this_rq->nr_running * SCHED_LOAD_SCALE;
-	/*
-	 * Round up the averaging division if load is increasing. This
-	 * prevents us from getting stuck on 9 if the load is 10, for
-	 * example.
-	 */
-	if (this_load > old_load)
-		old_load++;
-	this_rq->cpu_load = (old_load + this_load) / 2;
+	/* Update our load */
+	for (i = 0; i < 4; i++) {
+		unsigned long new_load = this_load;
+		int scale = 1<<i;
+		old_load = this_rq->cpu_load[i];
+		/*
+		 * Round up the averaging division if load is increasing. This
+		 * prevents us from getting stuck on 9 if the load is 10, for
+		 * example.
+		 */
+		if (new_load > old_load)
+			new_load += scale-1;
+		this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) / scale;
+	}
 
 	for_each_domain(this_cpu, sd) {
 		unsigned long interval;
@@ -2042,21 +2065,6 @@
 }
 #endif
 
-static inline int wake_priority_sleeper(runqueue_t *rq)
-{
-#ifdef CONFIG_SCHED_SMT
-	/*
-	 * If an SMT sibling task has been put to sleep for priority
-	 * reasons reschedule the idle task to see if it can now run.
-	 */
-	if (rq->nr_running) {
-		resched_task(rq->idle);
-		return 1;
-	}
-#endif
-	return 0;
-}
-
 /*
  * This function gets called by the timer code, with HZ frequency.
  * We call it with interrupts disabled.
@@ -2077,8 +2085,6 @@
 #endif
 
 	if (p == rq->idle) {
-		if (wake_priority_sleeper(rq))
-			goto out;
 		cpu_status = SCHED_IDLE;
 		goto out;
 	}
@@ -2103,129 +2109,6 @@
 	rebalance_tick(cpu, rq, cpu_status);
 }
 
-#ifdef CONFIG_SCHED_SMT
-static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	int i;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return;
-
-	/*
-	 * Unlock the current runqueue because we have to lock in
-	 * CPU order to avoid deadlocks. Caller knows that we might
-	 * unlock. We keep IRQs disabled.
-	 */
-	spin_unlock(&this_rq->lock);
-
-	sibling_map = sd->span;
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	/*
-	 * We clear this CPU from the mask. This both simplifies the
-	 * inner loop and keps this_rq locked when we exit:
-	 */
-	cpu_clear(this_cpu, sibling_map);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-
-		/*
-		 * If an SMT sibling task is sleeping due to priority
-		 * reasons wake it up now.
-		 */
-		if (smt_rq->curr == smt_rq->idle && smt_rq->nr_running)
-			resched_task(smt_rq->idle);
-	}
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	/*
-	 * We exit with this_cpu's rq still held and IRQs
-	 * still disabled:
-	 */
-}
-
-static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	prio_array_t *array;
-	int ret = 0, i;
-	task_t *p;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return 0;
-
-	/*
-	 * The same locking rules and details apply as for
-	 * wake_sleeping_dependent():
-	 */
-	spin_unlock(&this_rq->lock);
-	sibling_map = sd->span;
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	cpu_clear(this_cpu, sibling_map);
-
-	/*
-	 * Establish next task to be run - it might have gone away because
-	 * we released the runqueue lock above:
-	 */
-	if (!this_rq->nr_running)
-		goto out_unlock;
-	array = this_rq->active;
-	if (!array->nr_active)
-		array = this_rq->expired;
-	BUG_ON(!array->nr_active);
-
-	p = list_entry(array->queue[sched_find_first_bit(array->bitmap)].next,
-		task_t, u.nicksched.run_list);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-		task_t *smt_curr = smt_rq->curr;
-
-		/*
-		 * If a user task with lower static priority than the
-		 * running task on the SMT sibling is trying to schedule,
-		 * delay it till there is proportionately less timeslice
-		 * left of the sibling task to prevent a lower priority
-		 * task from using an unfair proportion of the
-		 * physical cpu's resources. -ck
-		 */
-		if ((smt_curr->static_prio + 5 < p->static_prio) &&
-			p->mm && smt_curr->mm && !rt_task(p))
-				ret = 1;
-
-		/*
-		 * Reschedule a lower priority task on the SMT sibling,
-		 * or wake it up if it has been put to sleep for priority
-		 * reasons.
-		 */
-		if ((p->static_prio + 5 < smt_curr->static_prio &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
-			(smt_curr == smt_rq->idle && smt_rq->nr_running))
-				resched_task(smt_curr);
-	}
-out_unlock:
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	return ret;
-}
-#else
-static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-}
-
-static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	return 0;
-}
-#endif
-
 /*
  * schedule() is the main scheduler function.
  */
@@ -2316,34 +2199,14 @@
 
 	cpu = smp_processor_id();
 	if (unlikely(!rq->nr_running)) {
-go_idle:
 		rq->array_sequence++;
 		idle_balance(cpu, rq);
 		if (!rq->nr_running) {
 			rq->arrays[0].min_prio = MAX_PRIO;
 			rq->arrays[1].min_prio = MAX_PRIO;
 			next = rq->idle;
-			wake_sleeping_dependent(cpu, rq);
-			/*
-			 * wake_sleeping_dependent() might have released
-			 * the runqueue, so break out if we got new
-			 * tasks meanwhile:
-			 */
-			if (!rq->nr_running)
-				goto switch_tasks;
-		}
-	} else {
-		if (dependent_sleeper(cpu, rq)) {
-			next = rq->idle;
 			goto switch_tasks;
 		}
-		/*
-		 * dependent_sleeper() releases and reacquires the runqueue
-		 * lock, hence go into the idle loop if the rq went
-		 * empty meanwhile:
-		 */
-		if (unlikely(!rq->nr_running))
-			goto go_idle;
 	}
 
 	array = rq->active;
@@ -2357,8 +2220,7 @@
 		rq->expired = array;
 		rq->expired->min_prio = MAX_PRIO;
 		array = rq->active;
-	} else
-		schedstat_inc(rq->sspcd, sched_noswitch);
+	}
 
 	idx = sched_find_first_bit(array->bitmap);
 	queue = array->queue + idx;
@@ -3157,16 +3019,20 @@
 		cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
 		seq_printf(seq, "domain%d %s", dcnt++, mask_str);
 		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++) {
-			seq_printf(seq, " %lu %lu %lu %lu %lu",
+			seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu",
 				sd->lb_cnt[itype],
+			        sd->lb_balanced[itype],
 				sd->lb_failed[itype],
 				sd->lb_imbalance[itype],
+			        sd->lb_gained[itype],
+			        sd->lb_hot_gained[itype],
 				sd->lb_nobusyq[itype],
 				sd->lb_nobusyg[itype]);
 		}
-		seq_printf(seq, " %lu %lu %lu %lu\n",
+		seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu\n",
+		        sd->alb_cnt, sd->alb_failed, sd->alb_pushed,
 			sd->sbe_pushed, sd->sbe_attempts,
-			sd->ttwu_wake_affine, sd->ttwu_wake_balance);
+		        sd->ttwu_wake_remote, sd->ttwu_move_affine, sd->ttwu_move_balance);
 	}
 }
 #endif
@@ -3578,7 +3444,8 @@
 
 #ifdef CONFIG_SMP
 		rq->sd = &sched_domain_dummy;
-		rq->cpu_load = 0;
+		for (j = 0; j < 4; j++)
+			rq->cpu_load[j] = 0;
 		rq->active_balance = 0;
 		rq->push_cpu = 0;
 		rq->migration_thread = NULL;
@@ -3660,7 +3527,7 @@
 sd_alloc_ctl_domain_table(struct sched_domain *sd)
 {
 	struct ctl_table *table;
-	table = sd_alloc_ctl_entry(9);
+	table = sd_alloc_ctl_entry(11);
 
 	set_table_entry(&table[0], 1, "min_interval", &sd->min_interval,
 			sizeof(long), 0644, proc_doulongvec_minmax);
@@ -3674,9 +3541,13 @@
 			sizeof(long long), 0644, proc_doulonglongvec_minmax);
 	set_table_entry(&table[5], 6, "cache_nice_tries", &sd->cache_nice_tries,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[6], 7, "per_cpu_gain", &sd->per_cpu_gain,
+	set_table_entry(&table[6], 7, "load_idx", &sd->load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[7], 8, "idle_load_idx", &sd->idle_load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[8], 9, "wake_balance_load_idx", &sd->wake_balance_load_idx,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[7], 8, "flags", &sd->flags,
+	set_table_entry(&table[9], 10, "flags", &sd->flags,
 			sizeof(int), 0644, proc_dointvec_minmax);
 	return table;
 }
Index: xx-sources/kernel/sched.c
===================================================================
--- xx-sources.orig/kernel/sched.c	2005-01-14 23:50:38.391847768 +0000
+++ xx-sources/kernel/sched.c	2005-01-14 23:55:50.504399448 +0000
@@ -217,7 +217,7 @@
 	 */
 	unsigned long nr_running;
 #ifdef CONFIG_SMP
-	unsigned long cpu_load;
+	unsigned long cpu_load[4];
 #endif
 	unsigned long long nr_switches;
 
@@ -363,21 +363,6 @@
 	spin_unlock_irqrestore(&rq->lock, *flags);
 }
 
-/*
- * rq_lock - lock a given runqueue and disable interrupts.
- */
-static runqueue_t *this_rq_lock(void)
-	__acquires(rq->lock)
-{
-	runqueue_t *rq;
-
-	local_irq_disable();
-	rq = this_rq();
-	spin_lock(&rq->lock);
-
-	return rq;
-}
-
 #ifdef CONFIG_SCHED_SMT
 static int cpu_and_siblings_are_idle(int cpu)
 {
@@ -724,23 +709,23 @@
  * We want to under-estimate the load of migration sources, to
  * balance conservatively.
  */
-static inline unsigned long source_load(int cpu)
+static inline unsigned long source_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return min(rq->cpu_load, load_now);
+	return min(rq->cpu_load[type], load_now);
 }
 
 /*
  * Return a high guess at the load of a migration-target cpu
  */
-static inline unsigned long target_load(int cpu)
+static inline unsigned long target_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return max(rq->cpu_load, load_now);
+	return max(rq->cpu_load[type], load_now);
 }
 
 #endif
@@ -805,7 +790,7 @@
 	runqueue_t *rq;
 #ifdef CONFIG_SMP
 	unsigned long load, this_load;
-	struct sched_domain *sd;
+	struct sched_domain *sd, *this_sd = NULL;
 	int new_cpu;
 #endif
 
@@ -825,68 +810,79 @@
 	if (unlikely(task_running(rq, p)))
 		goto out_activate;
 
-	new_cpu = cpu;
+	schedstat_inc(rq->sspcd, ttwu_cnt);
+	if (cpu == this_cpu) {
+		schedstat_inc(rq->sspcd, ttwu_local);
+	} else {
+		for_each_domain(this_cpu, sd) {
+			if (cpu_isset(cpu, sd->span)) {
+				schedstat_inc(sd, ttwu_wake_remote);
+				this_sd = sd;
+				break;
+			}
+		}
+	}
 
+	new_cpu = cpu;
 	if (cpu == this_cpu || unlikely(!cpu_isset(this_cpu, p->cpus_allowed)))
 		goto out_set_cpu;
 
-	load = source_load(cpu);
-	this_load = target_load(this_cpu);
 
 	/*
-	 * If sync wakeup then subtract the (maximum possible) effect of
-	 * the currently running task from the load of the current CPU:
+	 * Check for affine wakeup and passive balancing possibilities.
 	 */
-	if (sync)
-		this_load -= SCHED_LOAD_SCALE;
+	if (this_sd) {
+		unsigned int imbalance;
 
-	/* Don't pull the task off an idle CPU to a busy one */
-	if (load < SCHED_LOAD_SCALE/2 && this_load > SCHED_LOAD_SCALE/2)
-		goto out_set_cpu;
+		load = source_load(cpu, this_sd->wake_balance_load_idx);
+		this_load = target_load(this_cpu, this_sd->wake_balance_load_idx);
 
-	new_cpu = this_cpu; /* Wake to this CPU if we can */
+		/* Don't pull the task off an idle CPU to a busy one */
+		if (load < SCHED_LOAD_SCALE
+				&& load + this_load > SCHED_LOAD_SCALE
+				&& this_load > load)
+			goto out_set_cpu;
+
+		new_cpu = this_cpu; /* Wake to this CPU if we can */
 
-	/*
-	 * Scan domains for affine wakeup and passive balancing
-	 * possibilities.
-	 */
-	for_each_domain(this_cpu, sd) {
-		unsigned int imbalance;
 		/*
 		 * Start passive balancing when half the imbalance_pct
 		 * limit is reached.
 		 */
-		imbalance = sd->imbalance_pct + (sd->imbalance_pct - 100) / 2;
+		imbalance = 100 + (this_sd->imbalance_pct - 100) / 2;
 
-		if ((sd->flags & SD_WAKE_AFFINE) &&
-				!task_hot(p, rq->timestamp_last_tick, sd)) {
-			/*
-			 * This domain has SD_WAKE_AFFINE and p is cache cold
-			 * in this domain.
-			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_affine);
+		if ((this_sd->flags & SD_WAKE_AFFINE) &&
+			!task_hot(p, rq->timestamp_last_tick, this_sd)) {
+			unsigned long load, this_load;
+			load = target_load(cpu, this_sd->wake_balance_load_idx);
+			this_load = source_load(this_cpu, this_sd->wake_balance_load_idx);
+
+			if (imbalance*load > 100*this_load) {
+				/*
+				 * This domain has SD_WAKE_AFFINE and
+				 * p is cache cold in this domain, and
+				 * there is no bad imbalance.
+				 */
+				schedstat_inc(this_sd, ttwu_move_affine);
 				goto out_set_cpu;
 			}
-		} else if ((sd->flags & SD_WAKE_BALANCE) &&
+		}
+
+		if ((this_sd->flags & SD_WAKE_BALANCE) &&
 				imbalance*this_load <= 100*load) {
 			/*
 			 * This domain has SD_WAKE_BALANCE and there is
 			 * an imbalance.
 			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_balance);
-				goto out_set_cpu;
-			}
+			schedstat_inc(this_sd, ttwu_move_balance);
+			goto out_set_cpu;
 		}
 	}
 
 	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
 out_set_cpu:
-	schedstat_inc(rq->sspcd, ttwu_attempts);
 	new_cpu = wake_idle(new_cpu, p);
 	if (new_cpu != cpu) {
-		schedstat_inc(rq->sspcd, ttwu_moved);
 		set_task_cpu(p, new_cpu);
 		task_rq_unlock(rq, &flags);
 		/* might preempt at this point */
@@ -1332,7 +1328,7 @@
 	cpus_and(mask, sd->span, p->cpus_allowed);
 
 	for_each_cpu_mask(i, mask) {
-		load = target_load(i);
+		load = target_load(i, sd->wake_balance_load_idx);
 
 		if (load < min_load) {
 			min_cpu = i;
@@ -1345,7 +1341,7 @@
 	}
 
 	/* add +1 to account for the new task */
-	this_load = source_load(this_cpu) + SCHED_LOAD_SCALE;
+	this_load = source_load(this_cpu, sd->wake_balance_load_idx) + SCHED_LOAD_SCALE;
 
 	/*
 	 * Would with the addition of the new task to the
@@ -1549,13 +1545,10 @@
 		goto skip_bitmap;
 	}
 
-	/*
-	 * Right now, this is the only place pull_task() is called,
-	 * so we can safely collect pull_task() stats here rather than
-	 * inside pull_task().
-	 */
-	schedstat_inc(this_rq->sspcd, pt_gained[idle]);
-	schedstat_inc(busiest->sspcd, pt_lost[idle]);
+#ifdef CONFIG_SCHEDSTATS
+	if (task_hot(tmp, busiest->timestamp_last_tick, sd))
+		schedstat_inc(sd, lb_hot_gained[idle]);
+#endif
 
 	pull_task(busiest, array, tmp, this_rq, dst_array, this_cpu);
 	pulled++;
@@ -1568,6 +1561,13 @@
 		goto skip_bitmap;
 	}
 out:
+	/*
+	 * Right now, this is the only place pull_task() is called,
+	 * so we can safely collect pull_task() stats here rather than
+	 * inside pull_task().
+	 */
+	schedstat_add(sd, lb_gained[idle], pulled);
+
 	return pulled;
 }
 
@@ -1582,8 +1582,11 @@
 {
 	struct sched_group *busiest = NULL, *this = NULL, *group = sd->groups;
 	unsigned long max_load, avg_load, total_load, this_load, total_pwr;
+	int load_idx = sd->idle_load_idx;
 
 	max_load = this_load = total_load = total_pwr = 0;
+	if (idle == NOT_IDLE)
+		load_idx = sd->load_idx;
 
 	do {
 		unsigned long load;
@@ -1598,9 +1601,9 @@
 		for_each_cpu_mask(i, group->cpumask) {
 			/* Bias balancing toward cpus of our domain */
 			if (local_group)
-				load = target_load(i);
+				load = target_load(i, load_idx);
 			else
-				load = source_load(i);
+				load = source_load(i, load_idx);
 
 			nr_cpus++;
 			avg_load += load;
@@ -1653,7 +1656,7 @@
 	*imbalance = (*imbalance * min(busiest->cpu_power, this->cpu_power))
 				/ SCHED_LOAD_SCALE;
 
-	if (*imbalance < SCHED_LOAD_SCALE - 1) {
+	if (*imbalance < SCHED_LOAD_SCALE) {
 		unsigned long pwr_now = 0, pwr_move = 0;
 		unsigned long tmp;
 
@@ -1694,13 +1697,12 @@
 	}
 
 	/* Get rid of the scaling factor, rounding down as we divide */
-	*imbalance = (*imbalance + 1) / SCHED_LOAD_SCALE;
+	*imbalance = *imbalance / SCHED_LOAD_SCALE;
 
 	return busiest;
 
 out_balanced:
-	if (busiest && (idle == NEWLY_IDLE ||
-			(idle == SCHED_IDLE && max_load > SCHED_LOAD_SCALE)) ) {
+	if (busiest && idle != NOT_IDLE && max_load > SCHED_LOAD_SCALE) {
 		*imbalance = 1;
 		return busiest;
 	}
@@ -1719,7 +1721,7 @@
 	int i;
 
 	for_each_cpu_mask(i, group->cpumask) {
-		load = source_load(i);
+		load = source_load(i, 0);
 
 		if (load > max_load) {
 			max_load = load;
@@ -1744,7 +1746,6 @@
 	unsigned long imbalance;
 	int nr_moved;
 
-	spin_lock(&this_rq->lock);
 	schedstat_inc(sd, lb_cnt[idle]);
 
 	group = find_busiest_group(sd, this_cpu, &imbalance, idle);
@@ -1779,12 +1780,11 @@
 		 * still unbalanced. nr_moved simply stays zero, so it is
 		 * correctly treated as an imbalance.
 		 */
-		double_lock_balance(this_rq, busiest);
+		double_rq_lock(this_rq, busiest);
 		nr_moved = move_tasks(this_rq, this_cpu, busiest,
 						imbalance, sd, idle);
-		spin_unlock(&busiest->lock);
+		double_rq_unlock(this_rq, busiest);
 	}
-	spin_unlock(&this_rq->lock);
 
 	if (!nr_moved) {
 		schedstat_inc(sd, lb_failed[idle]);
@@ -1826,7 +1826,7 @@
 	return nr_moved;
 
 out_balanced:
-	spin_unlock(&this_rq->lock);
+	schedstat_inc(sd, lb_balanced[idle]);
 
 	/* tune up the balancing interval */
 	if (sd->balance_interval < sd->max_interval)
@@ -1871,6 +1871,8 @@
 					imbalance, sd, NEWLY_IDLE);
 	if (!nr_moved)
 		schedstat_inc(sd, lb_failed[NEWLY_IDLE]);
+	else
+		schedstat_inc(sd, lb_balanced[NEWLY_IDLE]);
 
 	spin_unlock(&busiest->lock);
 
@@ -1947,10 +1949,9 @@
 				double_lock_balance(busiest_rq, target_rq);
 				if (move_tasks(target_rq, cpu, busiest_rq,
 						1, sd, SCHED_IDLE)) {
-					schedstat_inc(busiest_rq->sspcd, alb_lost);
-					schedstat_inc(target_rq->sspcd, alb_gained);
+		 			schedstat_inc(sd, alb_pushed);
 				} else {
-					schedstat_inc(busiest_rq->sspcd, alb_failed);
+					schedstat_inc(sd, alb_failed);
 				}
 				spin_unlock(&target_rq->lock);
 			}
@@ -1977,18 +1978,23 @@
 	unsigned long old_load, this_load;
 	unsigned long j = jiffies + CPU_OFFSET(this_cpu);
 	struct sched_domain *sd;
+	int i;
 
-	/* Update our load */
-	old_load = this_rq->cpu_load;
 	this_load = this_rq->nr_running * SCHED_LOAD_SCALE;
-	/*
-	 * Round up the averaging division if load is increasing. This
-	 * prevents us from getting stuck on 9 if the load is 10, for
-	 * example.
-	 */
-	if (this_load > old_load)
-		old_load++;
-	this_rq->cpu_load = (old_load + this_load) / 2;
+	/* Update our load */
+	for (i = 0; i < 4; i++) {
+		unsigned long new_load = this_load;
+		int scale = 1<<i;
+		old_load = this_rq->cpu_load[i];
+		/*
+		 * Round up the averaging division if load is increasing. This
+		 * prevents us from getting stuck on 9 if the load is 10, for
+		 * example.
+		 */
+		if (new_load > old_load)
+			new_load += scale-1;
+		this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) / scale;
+	}
 
 	for_each_domain(this_cpu, sd) {
 		unsigned long interval;
@@ -2026,24 +2032,6 @@
 }
 #endif
 
-static inline int wake_priority_sleeper(runqueue_t *rq)
-{
-	int ret = 0;
-#ifdef CONFIG_SCHED_SMT
-	spin_lock(&rq->lock);
-	/*
-	 * If an SMT sibling task has been put to sleep for priority
-	 * reasons reschedule the idle task to see if it can now run.
-	 */
-	if (rq->nr_running) {
-		resched_task(rq->idle);
-		ret = 1;
-	}
-	spin_unlock(&rq->lock);
-#endif
-	return ret;
-}
-
 /*
  * We place interactive tasks back into the active array, if possible.
  *
@@ -2076,8 +2064,6 @@
 	rq->timestamp_last_tick = sched_clock();
 
 	if (p == rq->idle) {
-		if (wake_priority_sleeper(rq))
-			goto out;
 		rebalance_tick(cpu, rq, SCHED_IDLE);
 		return;
 	}
@@ -2157,131 +2143,6 @@
 	rebalance_tick(cpu, rq, NOT_IDLE);
 }
 
-#ifdef CONFIG_SCHED_SMT
-static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	int i;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return;
-
-	/*
-	 * Unlock the current runqueue because we have to lock in
-	 * CPU order to avoid deadlocks. Caller knows that we might
-	 * unlock. We keep IRQs disabled.
-	 */
-	spin_unlock(&this_rq->lock);
-
-	sibling_map = sd->span;
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	/*
-	 * We clear this CPU from the mask. This both simplifies the
-	 * inner loop and keps this_rq locked when we exit:
-	 */
-	cpu_clear(this_cpu, sibling_map);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-
-		/*
-		 * If an SMT sibling task is sleeping due to priority
-		 * reasons wake it up now.
-		 */
-		if (smt_rq->curr == smt_rq->idle && smt_rq->nr_running)
-			resched_task(smt_rq->idle);
-	}
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	/*
-	 * We exit with this_cpu's rq still held and IRQs
-	 * still disabled:
-	 */
-}
-
-static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	prio_array_t *array;
-	int ret = 0, i;
-	task_t *p;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return 0;
-
-	/*
-	 * The same locking rules and details apply as for
-	 * wake_sleeping_dependent():
-	 */
-	spin_unlock(&this_rq->lock);
-	sibling_map = sd->span;
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	cpu_clear(this_cpu, sibling_map);
-
-	/*
-	 * Establish next task to be run - it might have gone away because
-	 * we released the runqueue lock above:
-	 */
-	if (!this_rq->nr_running)
-		goto out_unlock;
-	array = this_rq->active;
-	if (!array->nr_active)
-		array = this_rq->expired;
-	BUG_ON(!array->nr_active);
-
-	p = list_entry(array->queue[sched_find_first_bit(array->bitmap)].next,
-		task_t, u.ingosched.run_list);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-		task_t *smt_curr = smt_rq->curr;
-
-		/*
-		 * If a user task with lower static priority than the
-		 * running task on the SMT sibling is trying to schedule,
-		 * delay it till there is proportionately less timeslice
-		 * left of the sibling task to prevent a lower priority
-		 * task from using an unfair proportion of the
-		 * physical cpu's resources. -ck
-		 */
-		if (((smt_curr->u.ingosched.time_slice * (100 - sd->per_cpu_gain) / 100) >
-			task_timeslice(p) || rt_task(smt_curr)) &&
-			p->mm && smt_curr->mm && !rt_task(p))
-				ret = 1;
-
-		/*
-		 * Reschedule a lower priority task on the SMT sibling,
-		 * or wake it up if it has been put to sleep for priority
-		 * reasons.
-		 */
-		if ((((p->u.ingosched.time_slice * (100 - sd->per_cpu_gain) / 100) >
-			task_timeslice(smt_curr) || rt_task(p)) &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
-			(smt_curr == smt_rq->idle && smt_rq->nr_running))
-				resched_task(smt_curr);
-	}
-out_unlock:
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	return ret;
-}
-#else
-static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-}
-
-static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	return 0;
-}
-#endif
-
 /*
  * schedule() is the main scheduler function.
  */
@@ -2360,32 +2221,12 @@
 
 	cpu = smp_processor_id();
 	if (unlikely(!rq->nr_running)) {
-go_idle:
 		idle_balance(cpu, rq);
 		if (!rq->nr_running) {
 			next = rq->idle;
 			rq->expired_timestamp = 0;
-			wake_sleeping_dependent(cpu, rq);
-			/*
-			 * wake_sleeping_dependent() might have released
-			 * the runqueue, so break out if we got new
-			 * tasks meanwhile:
-			 */
-			if (!rq->nr_running)
-				goto switch_tasks;
-		}
-	} else {
-		if (dependent_sleeper(cpu, rq)) {
-			next = rq->idle;
 			goto switch_tasks;
 		}
-		/*
-		 * dependent_sleeper() releases and reacquires the runqueue
-		 * lock, hence go into the idle loop if the rq went
-		 * empty meanwhile:
-		 */
-		if (unlikely(!rq->nr_running))
-			goto go_idle;
 	}
 
 	array = rq->active;
@@ -2399,8 +2240,7 @@
 		array = rq->active;
 		rq->expired_timestamp = 0;
 		rq->best_expired_prio = MAX_PRIO;
-	} else
-		schedstat_inc(rq->sspcd, sched_noswitch);
+	}
 
 	idx = sched_find_first_bit(array->bitmap);
 	queue = array->queue + idx;
@@ -2680,11 +2520,12 @@
  */
 static long ingo_sys_sched_yield(void)
 {
-	runqueue_t *rq = this_rq_lock();
+	runqueue_t *rq = this_rq();
 	prio_array_t *array = current->u.ingosched.array;
 	prio_array_t *target = rq->expired;
 
 	schedstat_inc(rq->sspcd, yld_cnt);
+	spin_lock(&rq->lock);
 	/*
 	 * We implement yielding by moving the task into the expired
 	 * queue.
@@ -2831,16 +2672,20 @@
 		cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
 		seq_printf(seq, "domain%d %s", dcnt++, mask_str);
 		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++) {
-			seq_printf(seq, " %lu %lu %lu %lu %lu",
+			seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu",
 				sd->lb_cnt[itype],
+			        sd->lb_balanced[itype],
 				sd->lb_failed[itype],
 				sd->lb_imbalance[itype],
+			        sd->lb_gained[itype],
+			        sd->lb_hot_gained[itype],
 				sd->lb_nobusyq[itype],
 				sd->lb_nobusyg[itype]);
 		}
-		seq_printf(seq, " %lu %lu %lu %lu\n",
+		seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu\n",
+		        sd->alb_cnt, sd->alb_failed, sd->alb_pushed,
 			sd->sbe_pushed, sd->sbe_attempts,
-			sd->ttwu_wake_affine, sd->ttwu_wake_balance);
+		        sd->ttwu_wake_remote, sd->ttwu_move_affine, sd->ttwu_move_balance);
 	}
 }
 #endif
@@ -3674,7 +3519,8 @@
 
 #ifdef CONFIG_SMP
 		rq->sd = &sched_domain_dummy;
-		rq->cpu_load = 0;
+		for (j = 0; j < 4; j++)
+			rq->cpu_load[j] = 0;
 		rq->active_balance = 0;
 		rq->push_cpu = 0;
 		rq->migration_thread = NULL;
@@ -3755,7 +3601,7 @@
 sd_alloc_ctl_domain_table(struct sched_domain *sd)
 {
 	struct ctl_table *table;
-	table = sd_alloc_ctl_entry(9);
+	table = sd_alloc_ctl_entry(11);
 
 	set_table_entry(&table[0], 1, "min_interval", &sd->min_interval,
 			sizeof(long), 0644, proc_doulongvec_minmax);
@@ -3769,9 +3615,13 @@
 			sizeof(long long), 0644, proc_doulonglongvec_minmax);
 	set_table_entry(&table[5], 6, "cache_nice_tries", &sd->cache_nice_tries,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[6], 7, "per_cpu_gain", &sd->per_cpu_gain,
+	set_table_entry(&table[6], 7, "load_idx", &sd->load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[7], 8, "idle_load_idx", &sd->idle_load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[8], 9, "wake_balance_load_idx", &sd->wake_balance_load_idx,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[7], 8, "flags", &sd->flags,
+	set_table_entry(&table[9], 10, "flags", &sd->flags,
 			sizeof(int), 0644, proc_dointvec_minmax);
 	return table;
 }
Index: xx-sources/kernel/scheduler.c
===================================================================
--- xx-sources.orig/kernel/scheduler.c	2005-01-14 23:50:37.714950672 +0000
+++ xx-sources/kernel/scheduler.c	2005-01-14 23:55:50.507398992 +0000
@@ -1146,7 +1146,7 @@
  * bump this up when changing the output format or the meaning of an existing
  * format, so that tools can adapt (or abort)
  */
-#define SCHEDSTAT_VERSION 10
+#define SCHEDSTAT_VERSION 11
 
 DEFINE_PER_CPU(struct schedstat_per_cpu_data, schedstat_pcd_data);
 
@@ -1170,22 +1170,14 @@
 
 		/* schedstat_per_cpu-specific stats */
 		seq_printf(seq,
-		    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu "
-		    "%lu %lu %lu %lu %lu %lu %lu %lu %lu %lu",
+		    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu",
 		    cpu, sspcd->yld_both_empty,
-		    sspcd->yld_act_empty, sspcd->yld_exp_empty,
-		    sspcd->yld_cnt, sspcd->sched_noswitch,
+		    sspcd->yld_act_empty, sspcd->yld_exp_empty, sspcd->yld_cnt,
 		    sspcd->sched_switch, sspcd->sched_cnt, sspcd->sched_goidle,
-		    sspcd->alb_cnt, sspcd->alb_gained, sspcd->alb_lost,
-		    sspcd->alb_failed,
-		    sspcd->ttwu_cnt, sspcd->ttwu_moved, sspcd->ttwu_attempts,
-		    sspcd->wunt_cnt, sspcd->wunt_moved,
-		    sspcd->smt_cnt, sspcd->sbe_cnt, sspcd->rq_sched_info.cpu_time,
+		    sspcd->ttwu_cnt, sspcd->ttwu_local,
+		    sspcd->rq_sched_info.cpu_time,
 		    sspcd->rq_sched_info.run_delay, sspcd->rq_sched_info.pcnt);
 
-		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++)
-			seq_printf(seq, " %lu %lu", sspcd->pt_gained[itype],
-						    sspcd->pt_lost[itype]);
 		seq_printf(seq, "\n");
 
 		/* domain-specific stats */
Index: xx-sources/kernel/staircase.c
===================================================================
--- xx-sources.orig/kernel/staircase.c	2005-01-14 23:50:38.393847464 +0000
+++ xx-sources/kernel/staircase.c	2005-01-14 23:55:50.505399296 +0000
@@ -128,7 +128,7 @@
 	 */
 	unsigned long nr_running;
 #ifdef CONFIG_SMP
-	unsigned long cpu_load;
+	unsigned long cpu_load[4];
 #endif
 	unsigned long long nr_switches;
 
@@ -272,21 +272,6 @@
 	spin_unlock_irqrestore(&rq->lock, *flags);
 }
 
-/*
- * rq_lock - lock a given runqueue and disable interrupts.
- */
-static runqueue_t *this_rq_lock(void)
-	__acquires(rq->lock)
-{
-	runqueue_t *rq;
-
-	local_irq_disable();
-	rq = this_rq();
-	spin_lock(&rq->lock);
-
-	return rq;
-}
-
 static inline void rq_unlock(runqueue_t *rq)
 	__releases(rq->lock)
 {
@@ -690,23 +675,23 @@
  * We want to under-estimate the load of migration sources, to
  * balance conservatively.
  */
-static inline unsigned long source_load(int cpu)
+static inline unsigned long source_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return min(rq->cpu_load, load_now);
+	return min(rq->cpu_load[type], load_now);
 }
 
 /*
  * Return a high guess at the load of a migration-target cpu
  */
-static inline unsigned long target_load(int cpu)
+static inline unsigned long target_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return max(rq->cpu_load, load_now);
+	return max(rq->cpu_load[type], load_now);
 }
 
 #endif
@@ -795,12 +780,11 @@
 	runqueue_t *rq;
 #ifdef CONFIG_SMP
 	unsigned long load, this_load;
-	struct sched_domain *sd;
+	struct sched_domain *sd, *this_sd = NULL;
 	int new_cpu;
 #endif
 
 	rq = task_rq_lock(p, &flags);
-	schedstat_inc(rq->sspcd, ttwu_cnt);
 	old_state = p->state;
 	if (!(old_state & state))
 		goto out;
@@ -815,68 +799,78 @@
 	if (unlikely(task_running(rq, p)))
 		goto out_activate;
 
-	new_cpu = cpu;
+	schedstat_inc(rq->sspcd, ttwu_cnt);
+	if (cpu == this_cpu) {
+		schedstat_inc(rq->sspcd, ttwu_local);
+ 	} else {
+ 		for_each_domain(this_cpu, sd) {
+			if (cpu_isset(cpu, sd->span)) {
+				schedstat_inc(sd, ttwu_wake_remote);
+				this_sd = sd;
+				break;
+			}
+		}
+	}
 
+	new_cpu = cpu;
 	if (cpu == this_cpu || unlikely(!cpu_isset(this_cpu, p->cpus_allowed)))
 		goto out_set_cpu;
 
-	load = source_load(cpu);
-	this_load = target_load(this_cpu);
-
 	/*
-	 * If sync wakeup then subtract the (maximum possible) effect of
-	 * the currently running task from the load of the current CPU:
+	 * Check for affine wakeup and passive balancing possibilities.
 	 */
-	if (sync)
-		this_load -= SCHED_LOAD_SCALE;
+	if (this_sd) {
+		unsigned int imbalance;
 
-	/* Don't pull the task off an idle CPU to a busy one */
-	if (load < SCHED_LOAD_SCALE/2 && this_load > SCHED_LOAD_SCALE/2)
-		goto out_set_cpu;
+		load = source_load(cpu, this_sd->wake_balance_load_idx);
+		this_load = target_load(this_cpu, this_sd->wake_balance_load_idx);
 
-	new_cpu = this_cpu; /* Wake to this CPU if we can */
+		/* Don't pull the task off an idle CPU to a busy one */
+		if (load < SCHED_LOAD_SCALE
+				&& load + this_load > SCHED_LOAD_SCALE
+				&& this_load > load)
+			goto out_set_cpu;
+
+		new_cpu = this_cpu; /* Wake to this CPU if we can */
 
-	/*
-	 * Scan domains for affine wakeup and passive balancing
-	 * possibilities.
-	 */
-	for_each_domain(this_cpu, sd) {
-		unsigned int imbalance;
 		/*
 		 * Start passive balancing when half the imbalance_pct
 		 * limit is reached.
 		 */
-		imbalance = sd->imbalance_pct + (sd->imbalance_pct - 100) / 2;
+		imbalance = 100 + (this_sd->imbalance_pct - 100) / 2;
 
-		if ((sd->flags & SD_WAKE_AFFINE) &&
-				!task_hot(p, rq->timestamp_last_tick, sd)) {
-			/*
-			 * This domain has SD_WAKE_AFFINE and p is cache cold
-			 * in this domain.
-			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_affine);
+		if ((this_sd->flags & SD_WAKE_AFFINE) &&
+			!task_hot(p, rq->timestamp_last_tick, this_sd)) {
+			unsigned long load, this_load;
+			load = target_load(cpu, this_sd->wake_balance_load_idx);
+			this_load = source_load(this_cpu, this_sd->wake_balance_load_idx);
+
+			if (imbalance*load > 100*this_load) {
+				/*
+				 * This domain has SD_WAKE_AFFINE and
+				 * p is cache cold in this domain, and
+				 * there is no bad imbalance.
+				 */
+				schedstat_inc(this_sd, ttwu_move_affine);
 				goto out_set_cpu;
 			}
-		} else if ((sd->flags & SD_WAKE_BALANCE) &&
+		}
+
+		if ((this_sd->flags & SD_WAKE_BALANCE) &&
 				imbalance*this_load <= 100*load) {
 			/*
 			 * This domain has SD_WAKE_BALANCE and there is
 			 * an imbalance.
 			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_balance);
-				goto out_set_cpu;
-			}
+			schedstat_inc(this_sd, ttwu_move_balance);
+			goto out_set_cpu;
 		}
 	}
 
 	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
 out_set_cpu:
-	schedstat_inc(rq->sspcd, ttwu_attempts);
 	new_cpu = wake_idle(new_cpu, p);
 	if (new_cpu != cpu) {
-		schedstat_inc(rq->sspcd, ttwu_moved);
 		set_task_cpu(p, new_cpu);
 		task_rq_unlock(rq, &flags);
 		/* might preempt at this point */
@@ -1235,7 +1229,7 @@
 	cpus_and(mask, sd->span, p->cpus_allowed);
 
 	for_each_cpu_mask(i, mask) {
-		load = target_load(i);
+		load = target_load(i, sd->wake_balance_load_idx);
 
 		if (load < min_load) {
 			min_cpu = i;
@@ -1248,7 +1242,7 @@
 	}
 
 	/* add +1 to account for the new task */
-	this_load = source_load(this_cpu) + SCHED_LOAD_SCALE;
+	this_load = source_load(this_cpu, sd->wake_balance_load_idx) + SCHED_LOAD_SCALE;
 
 	/*
 	 * Would with the addition of the new task to the
@@ -1427,13 +1421,10 @@
 		goto skip_bitmap;
 	}
 
-	/*
-	 * Right now, this is the only place pull_task() is called,
-	 * so we can safely collect pull_task() stats here rather than
-	 * inside pull_task().
-	 */
-	schedstat_inc(this_rq->sspcd, pt_gained[idle]);
-	schedstat_inc(busiest->sspcd, pt_lost[idle]);
+#ifdef CONFIG_SCHEDSTATS
+	if (task_hot(tmp, busiest->timestamp_last_tick, sd))
+		schedstat_inc(sd, lb_hot_gained[idle]);
+#endif
 
 	pull_task(busiest, tmp, this_rq, this_cpu);
 	pulled++;
@@ -1446,6 +1437,13 @@
 		goto skip_bitmap;
 	}
 out:
+	/*
+	 * Right now, this is the only place pull_task() is called,
+	 * so we can safely collect pull_task() stats here rather than
+	 * inside pull_task().
+	 */
+	schedstat_add(sd, lb_gained[idle], pulled);
+
 	return pulled;
 }
 
@@ -1460,8 +1458,11 @@
 {
 	struct sched_group *busiest = NULL, *this = NULL, *group = sd->groups;
 	unsigned long max_load, avg_load, total_load, this_load, total_pwr;
+	int load_idx = sd->idle_load_idx;
 
 	max_load = this_load = total_load = total_pwr = 0;
+	if (idle == NOT_IDLE)
+		load_idx = sd->load_idx;
 
 	do {
 		unsigned long load;
@@ -1476,9 +1477,9 @@
 		for_each_cpu_mask(i, group->cpumask) {
 			/* Bias balancing toward cpus of our domain */
 			if (local_group)
-				load = target_load(i);
+				load = target_load(i, load_idx);
 			else
-				load = source_load(i);
+				load = source_load(i, load_idx);
 
 			nr_cpus++;
 			avg_load += load;
@@ -1531,7 +1532,7 @@
 	*imbalance = (*imbalance * min(busiest->cpu_power, this->cpu_power))
 				/ SCHED_LOAD_SCALE;
 
-	if (*imbalance < SCHED_LOAD_SCALE - 1) {
+	if (*imbalance < SCHED_LOAD_SCALE) {
 		unsigned long pwr_now = 0, pwr_move = 0;
 		unsigned long tmp;
 
@@ -1572,13 +1573,12 @@
 	}
 
 	/* Get rid of the scaling factor, rounding down as we divide */
-	*imbalance = (*imbalance + 1) / SCHED_LOAD_SCALE;
+	*imbalance = *imbalance / SCHED_LOAD_SCALE;
 
 	return busiest;
 
 out_balanced:
-	if (busiest && (idle == NEWLY_IDLE ||
-			(idle == SCHED_IDLE && max_load > SCHED_LOAD_SCALE)) ) {
+	if (busiest && idle != NOT_IDLE && max_load > SCHED_LOAD_SCALE) {
 		*imbalance = 1;
 		return busiest;
 	}
@@ -1597,7 +1597,7 @@
 	int i;
 
 	for_each_cpu_mask(i, group->cpumask) {
-		load = source_load(i);
+		load = source_load(i, 0);
 
 		if (load > max_load) {
 			max_load = load;
@@ -1622,7 +1622,6 @@
 	unsigned long imbalance;
 	int nr_moved;
 
-	spin_lock(&this_rq->lock);
 	schedstat_inc(sd, lb_cnt[idle]);
 
 	group = find_busiest_group(sd, this_cpu, &imbalance, idle);
@@ -1657,12 +1656,11 @@
 		 * still unbalanced. nr_moved simply stays zero, so it is
 		 * correctly treated as an imbalance.
 		 */
-		double_lock_balance(this_rq, busiest);
+		double_rq_lock(this_rq, busiest);
 		nr_moved = move_tasks(this_rq, this_cpu, busiest,
 						imbalance, sd, idle);
-		spin_unlock(&busiest->lock);
+		double_rq_unlock(this_rq, busiest);
 	}
-	spin_unlock(&this_rq->lock);
 
 	if (!nr_moved) {
 		schedstat_inc(sd, lb_failed[idle]);
@@ -1704,7 +1702,7 @@
 	return nr_moved;
 
 out_balanced:
-	spin_unlock(&this_rq->lock);
+	schedstat_inc(sd, lb_balanced[idle]);
 
 	/* tune up the balancing interval */
 	if (sd->balance_interval < sd->max_interval)
@@ -1749,6 +1747,8 @@
 					imbalance, sd, NEWLY_IDLE);
 	if (!nr_moved)
 		schedstat_inc(sd, lb_failed[NEWLY_IDLE]);
+	else
+		schedstat_inc(sd, lb_balanced[NEWLY_IDLE]);
 
 	spin_unlock(&busiest->lock);
 
@@ -1825,10 +1825,9 @@
 				double_lock_balance(busiest_rq, target_rq);
 				if (move_tasks(target_rq, cpu, busiest_rq,
 						1, sd, SCHED_IDLE)) {
-					schedstat_inc(busiest_rq->sspcd, alb_lost);
-					schedstat_inc(target_rq->sspcd, alb_gained);
+					schedstat_inc(sd, alb_pushed);
 				} else {
-					schedstat_inc(busiest_rq->sspcd, alb_failed);
+					schedstat_inc(sd, alb_failed);
 				}
 				spin_unlock(&target_rq->lock);
 			}
@@ -1855,18 +1854,23 @@
 	unsigned long old_load, this_load;
 	unsigned long j = jiffies + CPU_OFFSET(this_cpu);
 	struct sched_domain *sd;
+	int i;
 
-	/* Update our load */
-	old_load = this_rq->cpu_load;
 	this_load = this_rq->nr_running * SCHED_LOAD_SCALE;
-	/*
-	 * Round up the averaging division if load is increasing. This
-	 * prevents us from getting stuck on 9 if the load is 10, for
-	 * example.
-	 */
-	if (this_load > old_load)
-		old_load++;
-	this_rq->cpu_load = (old_load + this_load) / 2;
+	/* Update our load */
+	for (i = 0; i < 4; i++) {
+		unsigned long new_load = this_load;
+		int scale = 1<<i;
+		old_load = this_rq->cpu_load[i];
+		/*
+		 * Round up the averaging division if load is increasing. This
+		 * prevents us from getting stuck on 9 if the load is 10, for
++		 * example.
+		 */
+		if (new_load > old_load)
+			new_load += scale-1;
+		this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) / scale;
+	}
 
 	for_each_domain(this_cpu, sd) {
 		unsigned long interval;
@@ -1904,24 +1908,6 @@
 }
 #endif
 
-static int wake_priority_sleeper(runqueue_t *rq)
-{
-	int ret = 0;
-#ifdef CONFIG_SCHED_SMT
-	spin_lock(&rq->lock);
-	/*
-	 * If an SMT sibling task has been put to sleep for priority
-	 * reasons reschedule the idle task to see if it can now run.
-	 */
-	if (rq->nr_running) {
-		resched_task(rq->idle);
-		ret = 1;
-	}
-	spin_unlock(&rq->lock);
-#endif
-	return ret;
-}
-
 static void time_slice_expired(task_t *p, runqueue_t *rq)
 {
 	set_tsk_need_resched(p);
@@ -1945,8 +1931,6 @@
 	rq->timestamp_last_tick = sched_clock();
 
 	if (p == rq->idle) {
-		if (wake_priority_sleeper(rq))
-			goto out;
 		rebalance_tick(cpu, rq, SCHED_IDLE);
 		return;
 	}
@@ -1995,126 +1979,6 @@
 	rebalance_tick(cpu, rq, NOT_IDLE);
 }
 
-#ifdef CONFIG_SCHED_SMT
-static void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	int i;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return;
-
-	/*
-	 * Unlock the current runqueue because we have to lock in
-	 * CPU order to avoid deadlocks. Caller knows that we might
-	 * unlock. We keep IRQs disabled.
-	 */
-	spin_unlock(&this_rq->lock);
-
-	sibling_map = sd->span;
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	/*
-	 * We clear this CPU from the mask. This both simplifies the
-	 * inner loop and keps this_rq locked when we exit:
-	 */
-	cpu_clear(this_cpu, sibling_map);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-
-		/*
-		 * If an SMT sibling task is sleeping due to priority
-		 * reasons wake it up now.
-		 */
-		if (smt_rq->curr == smt_rq->idle && smt_rq->nr_running)
-			resched_task(smt_rq->idle);
-	}
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	/*
-	 * We exit with this_cpu's rq still held and IRQs
-	 * still disabled:
-	 */
-}
-
-static int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	int ret = 0, i;
-	task_t *p;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return 0;
-
-	/*
-	 * The same locking rules and details apply as for
-	 * wake_sleeping_dependent():
-	 */
-	spin_unlock(&this_rq->lock);
-	sibling_map = sd->span;
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	cpu_clear(this_cpu, sibling_map);
-
-	/*
-	 * Establish next task to be run - it might have gone away because
-	 * we released the runqueue lock above:
-	 */
-	if (!this_rq->nr_running)
-		goto out_unlock;
-
-	p = list_entry(this_rq->queue[sched_find_first_bit(this_rq->bitmap)].next,
-		task_t, u.scsched.run_list);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-		task_t *smt_curr = smt_rq->curr;
-
-		/*
-		 * If a user task with lower static priority than the
-		 * running task on the SMT sibling is trying to schedule,
-		 * delay it till there is proportionately less timeslice
-		 * left of the sibling task to prevent a lower priority
-		 * task from using an unfair proportion of the
-		 * physical cpu's resources. -ck
-		 */
-		if (((smt_curr->u.scsched.slice * (100 - sd->per_cpu_gain) / 100) >
-			slice(p) || rt_task(smt_curr)) &&
-			p->mm && smt_curr->mm && !rt_task(p))
-				ret = 1;
-
-		/*
-		 * Reschedule a lower priority task on the SMT sibling,
-		 * or wake it up if it has been put to sleep for priority
-		 * reasons.
-		 */
-		if ((((p->u.scsched.slice * (100 - sd->per_cpu_gain) / 100) > 
-			slice(smt_curr) || rt_task(p)) && 
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
-			(smt_curr == smt_rq->idle && smt_rq->nr_running))
-				resched_task(smt_curr);
-	}
-out_unlock:
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	return ret;
-}
-#else
-static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-}
-
-static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	return 0;
-}
-#endif
-
 /*
  * schedule() is the main scheduler function.
  */
@@ -2190,31 +2054,11 @@
 
 	cpu = smp_processor_id();
 	if (unlikely(!rq->nr_running)) {
-go_idle:
 		idle_balance(cpu, rq);
 		if (!rq->nr_running) {
 			next = rq->idle;
-			wake_sleeping_dependent(cpu, rq);
-			/*
-			 * wake_sleeping_dependent() might have released
-			 * the runqueue, so break out if we got new
-			 * tasks meanwhile:
-			 */
-			if (!rq->nr_running)
-				goto switch_tasks;
-		}
-	} else {
-		if (dependent_sleeper(cpu, rq)) {
-			next = rq->idle;
 			goto switch_tasks;
 		}
-		/*
-		 * dependent_sleeper() releases and reacquires the runqueue
-		 * lock, hence go into the idle loop if the rq went
-		 * empty meanwhile:
-		 */
-		if (unlikely(!rq->nr_running))
-			goto go_idle;
 	}
 
 	idx = sched_find_first_bit(rq->bitmap);
@@ -2475,7 +2319,8 @@
 static long sc_sys_sched_yield(void)
 {
 	int newprio;
-	runqueue_t *rq = this_rq_lock();
+	runqueue_t *rq = this_rq();
+	spin_lock(&rq->lock);
 
 	newprio = current->u.scsched.prio;
 	schedstat_inc(rq->sspcd, yld_cnt);
@@ -3015,16 +2860,20 @@
 		cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
 		seq_printf(seq, "domain%d %s", dcnt++, mask_str);
 		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++) {
-			seq_printf(seq, " %lu %lu %lu %lu %lu",
+			seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu",
 				sd->lb_cnt[itype],
+			        sd->lb_balanced[itype],
 				sd->lb_failed[itype],
 				sd->lb_imbalance[itype],
+			        sd->lb_gained[itype],
+			        sd->lb_hot_gained[itype],
 				sd->lb_nobusyq[itype],
 				sd->lb_nobusyg[itype]);
 		}
-		seq_printf(seq, " %lu %lu %lu %lu\n",
+		seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu\n",
+		        sd->alb_cnt, sd->alb_failed, sd->alb_pushed,
 			sd->sbe_pushed, sd->sbe_attempts,
-			sd->ttwu_wake_affine, sd->ttwu_wake_balance);
+		        sd->ttwu_wake_remote, sd->ttwu_move_affine, sd->ttwu_move_balance);
 	}
 }
 #endif
@@ -3437,7 +3286,8 @@
 
 #ifdef CONFIG_SMP
 		rq->sd = &sched_domain_dummy;
-		rq->cpu_load = 0;
+		for (j = 0; j < 4; j++)
+			rq->cpu_load[j] = 0;
 		rq->active_balance = 0;
 		rq->push_cpu = 0;
 		rq->migration_thread = NULL;
@@ -3524,7 +3374,7 @@
 sd_alloc_ctl_domain_table(struct sched_domain *sd)
 {
 	struct ctl_table *table;
-	table = sd_alloc_ctl_entry(9);
+	table = sd_alloc_ctl_entry(11);
 
 	set_table_entry(&table[0], 1, "min_interval", &sd->min_interval,
 			sizeof(long), 0644, proc_doulongvec_minmax);
@@ -3538,9 +3388,13 @@
 			sizeof(long long), 0644, proc_doulonglongvec_minmax);
 	set_table_entry(&table[5], 6, "cache_nice_tries", &sd->cache_nice_tries,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[6], 7, "per_cpu_gain", &sd->per_cpu_gain,
+	set_table_entry(&table[6], 7, "load_idx", &sd->load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[7], 8, "idle_load_idx", &sd->idle_load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[8], 9, "wake_balance_load_idx", &sd->wake_balance_load_idx,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[7], 8, "flags", &sd->flags,
+	set_table_entry(&table[9], 10, "flags", &sd->flags,
 			sizeof(int), 0644, proc_dointvec_minmax);
 	return table;
 }
Index: xx-sources/kernel/xsched.c
===================================================================
--- xx-sources.orig/kernel/xsched.c	2005-01-14 23:50:38.652808096 +0000
+++ xx-sources/kernel/xsched.c	2005-01-14 23:55:50.506399144 +0000
@@ -173,7 +173,7 @@
 	 */
 	unsigned long nr_running;
 #ifdef CONFIG_SMP
-	unsigned long cpu_load;
+	unsigned long cpu_load[4];
 #endif
 	unsigned long sequence;
 
@@ -729,23 +729,23 @@
  * We want to under-estimate the load of migration sources, to
  * balance conservatively.
  */
-static inline unsigned long source_load(int cpu)
+static inline unsigned long source_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return min(rq->cpu_load, load_now);
+	return min(rq->cpu_load[type], load_now);
 }
 
 /*
  * Return a high guess at the load of a migration-target cpu
  */
-static inline unsigned long target_load(int cpu)
+static inline unsigned long target_load(int cpu, int type)
 {
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long load_now = rq->nr_running * SCHED_LOAD_SCALE;
 
-	return max(rq->cpu_load, load_now);
+	return max(rq->cpu_load[type], load_now);
 }
 
 #endif
@@ -810,12 +810,11 @@
 	runqueue_t *rq;
 #ifdef CONFIG_SMP
 	unsigned long load, this_load;
-	struct sched_domain *sd;
+	struct sched_domain *sd, *this_sd = NULL;
 	int new_cpu;
 #endif
 
 	rq = task_rq_lock(p, &flags);
-	schedstat_inc(rq->sspcd, ttwu_cnt);
 	old_state = p->state;
 	if (!(old_state & state))
 		goto out;
@@ -830,68 +829,78 @@
 	if (unlikely(task_running(rq, p)))
 		goto out_activate;
 
-	new_cpu = cpu;
+	schedstat_inc(rq->sspcd, ttwu_cnt);
+	if (cpu == this_cpu) {
+		schedstat_inc(rq->sspcd, ttwu_local);
+	} else {
+		for_each_domain(this_cpu, sd) {
+			if (cpu_isset(cpu, sd->span)) {
+				schedstat_inc(sd, ttwu_wake_remote);
+				this_sd = sd;
+				break;
+			}
+		}
+	}
 
+	new_cpu = cpu;
 	if (cpu == this_cpu || unlikely(!cpu_isset(this_cpu, p->cpus_allowed)))
 		goto out_set_cpu;
 
-	load = source_load(cpu);
-	this_load = target_load(this_cpu);
-
 	/*
-	 * If sync wakeup then subtract the (maximum possible) effect of
-	 * the currently running task from the load of the current CPU:
+	 * Check for affine wakeup and passive balancing possibilities.
 	 */
-	if (sync)
-		this_load -= SCHED_LOAD_SCALE;
+	if (this_sd) {
+		unsigned int imbalance;
 
-	/* Don't pull the task off an idle CPU to a busy one */
-	if (load < SCHED_LOAD_SCALE/2 && this_load > SCHED_LOAD_SCALE/2)
-		goto out_set_cpu;
+		load = source_load(cpu, this_sd->wake_balance_load_idx);
+		this_load = target_load(this_cpu, this_sd->wake_balance_load_idx);
 
-	new_cpu = this_cpu; /* Wake to this CPU if we can */
+		/* Don't pull the task off an idle CPU to a busy one */
+		if (load < SCHED_LOAD_SCALE
+				&& load + this_load > SCHED_LOAD_SCALE
+				&& this_load > load)
+			goto out_set_cpu;
+
+		new_cpu = this_cpu; /* Wake to this CPU if we can */
 
-	/*
-	 * Scan domains for affine wakeup and passive balancing
-	 * possibilities.
-	 */
-	for_each_domain(this_cpu, sd) {
-		unsigned int imbalance;
 		/*
 		 * Start passive balancing when half the imbalance_pct
 		 * limit is reached.
 		 */
-		imbalance = sd->imbalance_pct + (sd->imbalance_pct - 100) / 2;
+		imbalance = 100 + (this_sd->imbalance_pct - 100) / 2;
 
-		if ((sd->flags & SD_WAKE_AFFINE) &&
-				!task_hot(p, rq->timestamp_last_tick, sd)) {
-			/*
-			 * This domain has SD_WAKE_AFFINE and p is cache cold
-			 * in this domain.
-			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_affine);
+		if ((this_sd->flags & SD_WAKE_AFFINE) &&
+			!task_hot(p, rq->timestamp_last_tick, this_sd)) {
+			unsigned long load, this_load;
+			load = target_load(cpu, this_sd->wake_balance_load_idx);
+			this_load = source_load(this_cpu, this_sd->wake_balance_load_idx);
+
+			if (imbalance*load > 100*this_load) {
+				/*
+				 * This domain has SD_WAKE_AFFINE and
+				 * p is cache cold in this domain, and
+				 * there is no bad imbalance.
+				 */
+				schedstat_inc(this_sd, ttwu_move_affine);
 				goto out_set_cpu;
 			}
-		} else if ((sd->flags & SD_WAKE_BALANCE) &&
+		}
+
+		if ((this_sd->flags & SD_WAKE_BALANCE) &&
 				imbalance*this_load <= 100*load) {
 			/*
 			 * This domain has SD_WAKE_BALANCE and there is
 			 * an imbalance.
 			 */
-			if (cpu_isset(cpu, sd->span)) {
-				schedstat_inc(sd, ttwu_wake_balance);
-				goto out_set_cpu;
-			}
+			schedstat_inc(this_sd, ttwu_move_balance);
+			goto out_set_cpu;
 		}
 	}
 
 	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
 out_set_cpu:
-	schedstat_inc(rq->sspcd, ttwu_attempts);
 	new_cpu = wake_idle(new_cpu, p);
 	if (new_cpu != cpu) {
-		schedstat_inc(rq->sspcd, ttwu_moved);
 		set_task_cpu(p, new_cpu);
 		task_rq_unlock(rq, &flags);
 		/* might preempt at this point */
@@ -1284,7 +1293,7 @@
 	cpus_and(mask, sd->span, p->cpus_allowed);
 
 	for_each_cpu_mask(i, mask) {
-		load = target_load(i);
+		load = target_load(i, sd->wake_balance_load_idx);
 
 		if (load < min_load) {
 			min_cpu = i;
@@ -1297,7 +1306,7 @@
 	}
 
 	/* add +1 to account for the new task */
-	this_load = source_load(this_cpu) + SCHED_LOAD_SCALE;
+	this_load = source_load(this_cpu, sd->wake_balance_load_idx) + SCHED_LOAD_SCALE;
 
 	/*
 	 * Would with the addition of the new task to the
@@ -1480,13 +1489,10 @@
 		goto skip_bitmap;
 	}
 
-	/*
-	 * Right now, this is the only place pull_task() is called,
-	 * so we can safely collect pull_task() stats here rather than
-	 * inside pull_task().
-	 */
-	schedstat_inc(this_rq->sspcd, pt_gained[idle]);
-	schedstat_inc(busiest->sspcd, pt_lost[idle]);
+#ifdef CONFIG_SCHEDSTATS
+	if (task_hot(tmp, busiest->timestamp_last_tick, sd))
+		schedstat_inc(sd, lb_hot_gained[idle]);
+#endif
 
 	pull_task(busiest, tmp, this_rq, this_cpu);
 	pulled++;
@@ -1499,6 +1505,13 @@
 		goto skip_bitmap;
 	}
 out:
+	/*
+	 * Right now, this is the only place pull_task() is called,
+	 * so we can safely collect pull_task() stats here rather than
+	 * inside pull_task().
+	 */
+	schedstat_add(sd, lb_gained[idle], pulled);
+
 	return pulled;
 }
 
@@ -1513,8 +1526,11 @@
 {
 	struct sched_group *busiest = NULL, *this = NULL, *group = sd->groups;
 	unsigned long max_load, avg_load, total_load, this_load, total_pwr;
+	int load_idx = sd->idle_load_idx;
 
 	max_load = this_load = total_load = total_pwr = 0;
+	if (idle == NOT_IDLE)
+		load_idx = sd->load_idx;
 
 	do {
 		unsigned long load;
@@ -1529,9 +1545,9 @@
 		for_each_cpu_mask(i, group->cpumask) {
 			/* Bias balancing toward cpus of our domain */
 			if (local_group)
-				load = target_load(i);
+				load = target_load(i, load_idx);
 			else
-				load = source_load(i);
+				load = source_load(i, load_idx);
 
 			nr_cpus++;
 			avg_load += load;
@@ -1584,7 +1600,7 @@
 	*imbalance = (*imbalance * min(busiest->cpu_power, this->cpu_power))
 				/ SCHED_LOAD_SCALE;
 
-	if (*imbalance < SCHED_LOAD_SCALE - 1) {
+	if (*imbalance < SCHED_LOAD_SCALE) {
 		unsigned long pwr_now = 0, pwr_move = 0;
 		unsigned long tmp;
 
@@ -1625,13 +1641,12 @@
 	}
 
 	/* Get rid of the scaling factor, rounding down as we divide */
-	*imbalance = (*imbalance + 1) / SCHED_LOAD_SCALE;
+	*imbalance = *imbalance / SCHED_LOAD_SCALE;
 
 	return busiest;
 
 out_balanced:
-	if (busiest && (idle == NEWLY_IDLE ||
-			(idle == SCHED_IDLE && max_load > SCHED_LOAD_SCALE)) ) {
+	if (busiest && idle != NOT_IDLE && max_load > SCHED_LOAD_SCALE) {
 		*imbalance = 1;
 		return busiest;
 	}
@@ -1650,7 +1665,7 @@
 	int i;
 
 	for_each_cpu_mask(i, group->cpumask) {
-		load = source_load(i);
+		load = source_load(i, 0);
 
 		if (load > max_load) {
 			max_load = load;
@@ -1675,7 +1690,6 @@
 	unsigned long imbalance;
 	int nr_moved;
 
-	spin_lock(&this_rq->lock);
 	schedstat_inc(sd, lb_cnt[idle]);
 
 	group = find_busiest_group(sd, this_cpu, &imbalance, idle);
@@ -1710,12 +1724,11 @@
 		 * still unbalanced. nr_moved simply stays zero, so it is
 		 * correctly treated as an imbalance.
 		 */
-		double_lock_balance(this_rq, busiest);
+		double_rq_lock(this_rq, busiest);
 		nr_moved = move_tasks(this_rq, this_cpu, busiest,
 						imbalance, sd, idle);
-		spin_unlock(&busiest->lock);
+		double_rq_unlock(this_rq, busiest);
 	}
-	spin_unlock(&this_rq->lock);
 
 	if (!nr_moved) {
 		schedstat_inc(sd, lb_failed[idle]);
@@ -1757,7 +1770,7 @@
 	return nr_moved;
 
 out_balanced:
-	spin_unlock(&this_rq->lock);
+	schedstat_inc(sd, lb_balanced[idle]);
 
 	/* tune up the balancing interval */
 	if (sd->balance_interval < sd->max_interval)
@@ -1802,6 +1815,8 @@
 					imbalance, sd, NEWLY_IDLE);
 	if (!nr_moved)
 		schedstat_inc(sd, lb_failed[NEWLY_IDLE]);
+	else
+		schedstat_inc(sd, lb_balanced[NEWLY_IDLE]);
 
 	spin_unlock(&busiest->lock);
 
@@ -1878,10 +1893,9 @@
 				double_lock_balance(busiest_rq, target_rq);
 				if (move_tasks(target_rq, cpu, busiest_rq,
 						1, sd, SCHED_IDLE)) {
-					schedstat_inc(busiest_rq->sspcd, alb_lost);
-					schedstat_inc(target_rq->sspcd, alb_gained);
+					schedstat_inc(sd, alb_pushed);
 				} else {
-					schedstat_inc(busiest_rq->sspcd, alb_failed);
+		 			schedstat_inc(sd, alb_failed);
 				}
 				spin_unlock(&target_rq->lock);
 			}
@@ -1908,18 +1922,23 @@
 	unsigned long old_load, this_load;
 	unsigned long j = jiffies + CPU_OFFSET(this_cpu);
 	struct sched_domain *sd;
+	int i;
 
-	/* Update our load */
-	old_load = this_rq->cpu_load;
 	this_load = this_rq->nr_running * SCHED_LOAD_SCALE;
-	/*
-	 * Round up the averaging division if load is increasing. This
-	 * prevents us from getting stuck on 9 if the load is 10, for
-	 * example.
-	 */
-	if (this_load > old_load)
-		old_load++;
-	this_rq->cpu_load = (old_load + this_load) / 2;
+	/* Update our load */
+	for (i = 0; i < 4; i++) {
+		unsigned long new_load = this_load;
+		int scale = 1<<i;
+		old_load = this_rq->cpu_load[i];
+		/*
+		 * Round up the averaging division if load is increasing. This
+		 * prevents us from getting stuck on 9 if the load is 10, for
+		 * example.
+		 */
+		if (new_load > old_load)
+			new_load += scale-1;
+		this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) / scale;
+	}
 
 	for_each_domain(this_cpu, sd) {
 		unsigned long interval;
@@ -1957,21 +1976,6 @@
 }
 #endif
 
-static inline int wake_priority_sleeper(runqueue_t *rq)
-{
-#ifdef CONFIG_SCHED_SMT
-	/*
-	 * If an SMT sibling task has been put to sleep for priority
-	 * reasons reschedule the idle task to see if it can now run.
-	 */
-	if (rq->nr_running) {
-		resched_task(rq->idle);
-		return 1;
-	}
-#endif
-	return 0;
-}
-
 /*
  * This function gets called by the timer code, with HZ frequency.
  * We call it with interrupts disabled.
@@ -1992,8 +1996,6 @@
 #endif
 
 	if (p == rq->idle) {
-		if (wake_priority_sleeper(rq))
-			goto out;
  		cpu_status = SCHED_IDLE;
  		goto out;
 	}
@@ -2018,124 +2020,6 @@
 	rebalance_tick(cpu, rq, cpu_status);
 }
 
-#ifdef CONFIG_SCHED_SMT
-static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	int i;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return;
-
-	/*
-	 * Unlock the current runqueue because we have to lock in
-	 * CPU order to avoid deadlocks. Caller knows that we might
-	 * unlock. We keep IRQs disabled.
-	 */
-	spin_unlock(&this_rq->lock);
-
-	sibling_map = sd->span;
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	/*
-	 * We clear this CPU from the mask. This both simplifies the
-	 * inner loop and keps this_rq locked when we exit:
-	 */
-	cpu_clear(this_cpu, sibling_map);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-
-		/*
-		 * If an SMT sibling task is sleeping due to priority
-		 * reasons wake it up now.
-		 */
-		if (smt_rq->curr == smt_rq->idle && smt_rq->nr_running)
-			resched_task(smt_rq->idle);
-	}
-
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	/*
-	 * We exit with this_cpu's rq still held and IRQs
-	 * still disabled:
-	 */
-}
-
-static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	struct sched_domain *sd = this_rq->sd;
-	cpumask_t sibling_map;
-	int ret = 0, i;
-	task_t *p;
-
-	if (!(sd->flags & SD_SHARE_CPUPOWER))
-		return 0;
-
-	/*
-	 * The same locking rules and details apply as for
-	 * wake_sleeping_dependent():
-	 */
-	spin_unlock(&this_rq->lock);
-	sibling_map = sd->span;
-	for_each_cpu_mask(i, sibling_map)
-		spin_lock(&cpu_rq(i)->lock);
-	cpu_clear(this_cpu, sibling_map);
-
-	/*
-	 * Establish next task to be run - it might have gone away because
-	 * we released the runqueue lock above:
-	 */
-	if (!this_rq->nr_running)
-		goto out_unlock;
-
-	p = list_entry(this_rq->queue[sched_find_first_bit(this_rq->bitmap)].next,
-		task_t, u.xsched.run_list);
-
-	for_each_cpu_mask(i, sibling_map) {
-		runqueue_t *smt_rq = cpu_rq(i);
-		task_t *smt_curr = smt_rq->curr;
-
-		/*
-		 * If a user task with lower static priority than the
-		 * running task on the SMT sibling is trying to schedule,
-		 * delay it till there is proportionately less timeslice
-		 * left of the sibling task to prevent a lower priority
-		 * task from using an unfair proportion of the
-		 * physical cpu's resources. -ck
-		 */
-		if ((smt_curr->static_prio + 5 < p->static_prio) &&
-			p->mm && smt_curr->mm && !rt_task(p))
-				ret = 1;
-
-		/*
-		 * Reschedule a lower priority task on the SMT sibling,
-		 * or wake it up if it has been put to sleep for priority
-		 * reasons.
-		 */
-		if ((p->static_prio + 5 < smt_curr->static_prio &&
-			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
-			(smt_curr == smt_rq->idle && smt_rq->nr_running))
-				resched_task(smt_curr);
-	}
-out_unlock:
-	for_each_cpu_mask(i, sibling_map)
-		spin_unlock(&cpu_rq(i)->lock);
-	return ret;
-}
-#else
-static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
-{
-}
-
-static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
-{
-	return 0;
-}
-#endif
-
 /*
  * schedule() is the main scheduler function.
  */
@@ -2225,33 +2109,13 @@
 no_check_expired:
 	cpu = smp_processor_id();
 	if (unlikely(!rq->nr_running)) {
-go_idle:
 		rq->sequence++;
 		idle_balance(cpu, rq);
 		if (!rq->nr_running) {
 			rq->min_prio = MAX_PRIO;
  			next = rq->idle;
- 			wake_sleeping_dependent(cpu, rq);
-			/*
-			 * wake_sleeping_dependent() might have released
-			 * the runqueue, so break out if we got new
-			 * tasks meanwhile:
-			 */
-			if (!rq->nr_running)
-				goto switch_tasks;
-		}
-	} else {
-		if (dependent_sleeper(cpu, rq)) {
-			next = rq->idle;
  			goto switch_tasks;
 		}
-		/*
-		 * dependent_sleeper() releases and reacquires the runqueue
-		 * lock, hence go into the idle loop if the rq went
-		 * empty meanwhile:
-		 */
-		if (unlikely(!rq->nr_running))
-			goto go_idle;
 	}
 
 	if (unlikely(RUNQUEUE_IDLE(rq))) {
@@ -3054,16 +2918,20 @@
 		cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
 		seq_printf(seq, "domain%d %s", dcnt++, mask_str);
 		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++) {
-			seq_printf(seq, " %lu %lu %lu %lu %lu",
+			seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu",
 				sd->lb_cnt[itype],
+			        sd->lb_balanced[itype],
 				sd->lb_failed[itype],
 				sd->lb_imbalance[itype],
+			        sd->lb_gained[itype],
+			        sd->lb_hot_gained[itype],
 				sd->lb_nobusyq[itype],
 				sd->lb_nobusyg[itype]);
 		}
-		seq_printf(seq, " %lu %lu %lu %lu\n",
+		seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu\n",
+		        sd->alb_cnt, sd->alb_failed, sd->alb_pushed,
 			sd->sbe_pushed, sd->sbe_attempts,
-			sd->ttwu_wake_affine, sd->ttwu_wake_balance);
+		        sd->ttwu_wake_remote, sd->ttwu_move_affine, sd->ttwu_move_balance);
 	}
 }
 #endif
@@ -3471,7 +3339,8 @@
 
 #ifdef CONFIG_SMP
 		rq->sd = &sched_domain_dummy;
-		rq->cpu_load = 0;
+		for (j = 0; j < 4; j++)
+			rq->cpu_load[j] = 0;
 		rq->active_balance = 0;
 		rq->push_cpu = 0;
 		rq->migration_thread = NULL;
@@ -3551,7 +3420,7 @@
 sd_alloc_ctl_domain_table(struct sched_domain *sd)
 {
 	struct ctl_table *table;
-	table = sd_alloc_ctl_entry(9);
+	table = sd_alloc_ctl_entry(11);
 
 	set_table_entry(&table[0], 1, "min_interval", &sd->min_interval,
 			sizeof(long), 0644, proc_doulongvec_minmax);
@@ -3565,9 +3434,13 @@
 			sizeof(long long), 0644, proc_doulonglongvec_minmax);
 	set_table_entry(&table[5], 6, "cache_nice_tries", &sd->cache_nice_tries,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[6], 7, "per_cpu_gain", &sd->per_cpu_gain,
+	set_table_entry(&table[6], 7, "load_idx", &sd->load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[7], 8, "idle_load_idx", &sd->idle_load_idx,
+			sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[8], 9, "wake_balance_load_idx", &sd->wake_balance_load_idx,
 			sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[7], 8, "flags", &sd->flags,
+	set_table_entry(&table[9], 10, "flags", &sd->flags,
 			sizeof(int), 0644, proc_dointvec_minmax);
 	return table;
 }
