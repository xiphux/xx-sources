Put common scheduler domains code into scheduler.c

Signed-off-by: Con Kolivas <kernel@kolivas.org>


Index: linux-2.6.10-rc1-mm5/include/linux/sched.h
===================================================================
--- linux-2.6.10-rc1-mm5.orig/include/linux/sched.h	2004-11-11 21:52:40.107383801 +1100
+++ linux-2.6.10-rc1-mm5/include/linux/sched.h	2004-11-11 21:52:42.351038015 +1100
@@ -460,13 +460,11 @@ struct sched_domain {
 #endif
 };
 
-extern void cpu_attach_domain(struct sched_domain *sd, int cpu);
-#ifdef ARCH_HAS_SCHED_DOMAIN
 /* Useful helpers that arch setup code may use. Defined in kernel/sched.c */
-extern cpumask_t cpu_isolated_map;
+extern void cpu_attach_domain(struct sched_domain *sd, int cpu);
 extern void init_sched_build_groups(struct sched_group groups[],
 	                        cpumask_t span, int (*group_fn)(int cpu));
-#endif /* ARCH_HAS_SCHED_DOMAIN */
+extern cpumask_t cpu_isolated_map;
 #endif /* CONFIG_SMP */
 
 
Index: linux-2.6.10-rc1-mm5/kernel/sched.c
===================================================================
--- linux-2.6.10-rc1-mm5.orig/kernel/sched.c	2004-11-11 21:52:35.709061658 +1100
+++ linux-2.6.10-rc1-mm5/kernel/sched.c	2004-11-11 21:52:42.353037707 +1100
@@ -3563,9 +3563,6 @@ static void __devinit ingo_cpu_attach_do
 	}
 }
 
-/* cpus with isolated domains */
-cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
-
 /* Setup the mask of cpus configured for isolated domains */
 static int __init isolated_cpu_setup(char *str)
 {
@@ -3580,52 +3577,6 @@ static int __init isolated_cpu_setup(cha
 
 __setup ("isolcpus=", isolated_cpu_setup);
 
-/*
- * init_sched_build_groups takes an array of groups, the cpumask we wish
- * to span, and a pointer to a function which identifies what group a CPU
- * belongs to. The return value of group_fn must be a valid index into the
- * groups[] array, and must be >= 0 and < NR_CPUS (due to the fact that we
- * keep track of groups covered with a cpumask_t).
- *
- * init_sched_build_groups will build a circular linked list of the groups
- * covered by the given span, and will set each group's ->cpumask correctly,
- * and ->cpu_power to 0.
- */
-void __devinit init_sched_build_groups(struct sched_group groups[],
-			cpumask_t span, int (*group_fn)(int cpu))
-{
-	struct sched_group *first = NULL, *last = NULL;
-	cpumask_t covered = CPU_MASK_NONE;
-	int i;
-
-	for_each_cpu_mask(i, span) {
-		int group = group_fn(i);
-		struct sched_group *sg = &groups[group];
-		int j;
-
-		if (cpu_isset(i, covered))
-			continue;
-
-		sg->cpumask = CPU_MASK_NONE;
-		sg->cpu_power = 0;
-
-		for_each_cpu_mask(j, span) {
-			if (group_fn(j) != group)
-				continue;
-
-			cpu_set(j, covered);
-			cpu_set(j, sg->cpumask);
-		}
-		if (!first)
-			first = sg;
-		if (last)
-			last->next = sg;
-		last = sg;
-	}
-	last->next = first;
-}
-
-
 #ifdef ARCH_HAS_SCHED_DOMAIN
 extern void __devinit arch_init_sched_domains(void);
 extern void __devinit arch_destroy_sched_domains(void);
Index: linux-2.6.10-rc1-mm5/kernel/scheduler.c
===================================================================
--- linux-2.6.10-rc1-mm5.orig/kernel/scheduler.c	2004-11-11 21:52:40.109383493 +1100
+++ linux-2.6.10-rc1-mm5/kernel/scheduler.c	2004-11-11 21:52:42.354037553 +1100
@@ -878,6 +878,56 @@ void fastcall complete_all(struct comple
 }
 EXPORT_SYMBOL(complete_all);
 
+#ifdef CONFIG_SMP
+/* cpus with isolated domains */
+cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
+
+/*
+ * init_sched_build_groups takes an array of groups, the cpumask we wish
+ * to span, and a pointer to a function which identifies what group a CPU
+ * belongs to. The return value of group_fn must be a valid index into the
+ * groups[] array, and must be >= 0 and < NR_CPUS (due to the fact that we
+ * keep track of groups covered with a cpumask_t).
+ *
+ * init_sched_build_groups will build a circular linked list of the groups
+ * covered by the given span, and will set each group's ->cpumask correctly,
+ * and ->cpu_power to 0.
+ */
+void __devinit init_sched_build_groups(struct sched_group groups[],
+			cpumask_t span, int (*group_fn)(int cpu))
+{
+	struct sched_group *first = NULL, *last = NULL;
+	cpumask_t covered = CPU_MASK_NONE;
+	int i;
+
+	for_each_cpu_mask(i, span) {
+		int group = group_fn(i);
+		struct sched_group *sg = &groups[group];
+		int j;
+
+		if (cpu_isset(i, covered))
+			continue;
+
+		sg->cpumask = CPU_MASK_NONE;
+		sg->cpu_power = 0;
+
+		for_each_cpu_mask(j, span) {
+			if (group_fn(j) != group)
+				continue;
+
+			cpu_set(j, covered);
+			cpu_set(j, sg->cpumask);
+		}
+		if (!first)
+			first = sg;
+		if (last)
+			last->next = sg;
+		last = sg;
+	}
+	last->next = first;
+}
+#endif
+
 extern struct sched_drv ingo_sched_drv;
 
 struct sched_drv *scheduler =
