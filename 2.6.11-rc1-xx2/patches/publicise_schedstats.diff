Take the common functions of schedstats out, and privatise those that are
scheduler design dependant.

Signed-off-by: Con Kolivas <kernel@kolivas.org>


Index: xx-sources/include/linux/sched.h
===================================================================
--- xx-sources.orig/include/linux/sched.h	2005-01-14 21:03:14.833700688 +0000
+++ xx-sources/include/linux/sched.h	2005-01-14 21:03:37.522251504 +0000
@@ -33,6 +33,7 @@
 #include <linux/pid.h>
 #include <linux/percpu.h>
 #include <linux/topology.h>
+#include <linux/seq_file.h>
 
 struct exec_domain;
 
@@ -387,21 +388,6 @@
 struct backing_dev_info;
 struct reclaim_state;
 
-#ifdef CONFIG_SCHEDSTATS
-struct sched_info {
-	/* cumulative counters */
-	unsigned long	cpu_time,	/* time spent on the cpu */
-			run_delay,	/* time spent waiting on a runqueue */
-			pcnt;		/* # of timeslices run on this cpu */
-
-	/* timestamps */
-	unsigned long	last_arrival,	/* when we last ran on a cpu */
-			last_queued;	/* when we were last queued to run */
-};
-
-extern struct file_operations proc_schedstat_operations;
-#endif
-
 enum idle_type
 {
 	SCHED_IDLE,
@@ -410,6 +396,8 @@
 	MAX_IDLE_TYPES
 };
 
+#include <linux/schedstats.h>
+
 /*
  * sched-domains (multiprocessor balancing) declarations:
  */
@@ -535,7 +523,7 @@
 
 	int static_prio;	/* A commonality between cpu schedulers */
 	union cpusched u;
-  
+
 	unsigned long policy;
 	cpumask_t cpus_allowed;
 
@@ -768,6 +756,7 @@
 extern void set_user_nice(task_t *p, long nice);
 extern int task_prio(const task_t *p);
 extern int task_nice(const task_t *p);
+extern int is_idle_task(const task_t *p);
 extern int task_curr(const task_t *p);
 extern int idle_cpu(int cpu);
 extern int sched_setscheduler(struct task_struct *, int, struct sched_param *);
Index: xx-sources/include/linux/schedstats.h
===================================================================
--- xx-sources.orig/include/linux/schedstats.h	2005-01-14 01:38:42.771666504 +0000
+++ xx-sources/include/linux/schedstats.h	2005-01-14 21:03:37.522251504 +0000
@@ -0,0 +1,78 @@
+#ifndef _LINUX_SCHEDSTATS_H
+#define _LINUX_SCHEDSTATS_H
+
+#ifdef CONFIG_SCHEDSTATS
+struct sched_info {
+	/* cumulative counters */
+	unsigned long	cpu_time,	/* time spent on the cpu */
+			run_delay,	/* time spent waiting on a runqueue */
+			pcnt;		/* # of timeslices run on this cpu */
+
+	/* timestamps */
+	unsigned long	last_arrival,	/* when we last ran on a cpu */
+			last_queued;	/* when we were last queued to run */
+};
+
+typedef struct schedstat_per_cpu_data schedstat_pcd_t;
+
+struct schedstat_per_cpu_data {
+	/* latency stats */
+	struct sched_info rq_sched_info;
+
+	/* sys_sched_yield() stats */
+	unsigned long yld_exp_empty;
+	unsigned long yld_act_empty;
+	unsigned long yld_both_empty;
+	unsigned long yld_cnt;
+
+	/* schedule() stats */
+	unsigned long sched_noswitch;
+	unsigned long sched_switch;
+	unsigned long sched_cnt;
+	unsigned long sched_goidle;
+
+	/* pull_task() stats */
+	unsigned long pt_gained[MAX_IDLE_TYPES];
+	unsigned long pt_lost[MAX_IDLE_TYPES];
+
+	/* active_load_balance() stats */
+	unsigned long alb_cnt;
+	unsigned long alb_lost;
+	unsigned long alb_gained;
+	unsigned long alb_failed;
+
+	/* try_to_wake_up() stats */
+	unsigned long ttwu_cnt;
+	unsigned long ttwu_attempts;
+	unsigned long ttwu_moved;
+
+	/* wake_up_new_task() stats */
+	unsigned long wunt_cnt;
+	unsigned long wunt_moved;
+
+	/* sched_migrate_task() stats */
+	unsigned long smt_cnt;
+
+	/* sched_balance_exec() stats */
+	unsigned long sbe_cnt;
+};
+
+extern struct file_operations proc_schedstat_operations;
+extern DEFINE_PER_CPU(struct schedstat_per_cpu_data, schedstat_pcd_data);
+
+#define cpu_sspcd(cpu)		(&per_cpu(schedstat_pcd_data, (cpu)))
+#define task_sspcd(cpu)		(cpu_sspcd(task_cpu(cpu)))
+
+extern void sched_info_switch(task_t *prev, task_t *next);
+extern void sched_info_queued(task_t *t);
+
+# define schedstat_inc(sspcd, field)	sspcd->field++;
+# define schedstat_add(sspcd, field, amt)	sspcd->field += amt;
+#else /* !CONFIG_SCHEDSTATS */
+# define schedstat_inc(sspcd, field)	do { } while (0);
+# define schedstat_add(sspcd, field, amt)	do { } while (0);
+# define sched_info_queued(t)		do { } while (0)
+# define sched_info_switch(t, next)	do { } while (0)
+#endif
+
+#endif /* _LINUX_SCHEDSTATS_H */
Index: xx-sources/include/linux/scheduler.h
===================================================================
--- xx-sources.orig/include/linux/scheduler.h	2005-01-14 21:02:43.257501000 +0000
+++ xx-sources/include/linux/scheduler.h	2005-01-14 21:03:37.523251352 +0000
@@ -37,6 +37,7 @@
 	void (*set_user_nice)(task_t *, long);
 	long (*rr_get_interval)(pid_t, struct timespec __user *);
 	long (*yield)(void);
+	int (*is_idle_task)(const task_t *);
 	int (*task_curr)(const task_t *);
 	int (*task_nice)(const task_t *);
 	int (*task_prio)(const task_t *);
@@ -48,6 +49,9 @@
 	int (*set_cpus_allowed)(task_t *, cpumask_t);
 	void (*wait_task_inactive)(task_t *);
 	void (*cpu_attach_domain)(struct sched_domain *, int);
+#ifdef CONFIG_SCHEDSTATS
+	void (*show_schedstat_sd)(struct seq_file *, int);
+#endif
 #endif
 };
 
Index: xx-sources/kernel/sched.c
===================================================================
--- xx-sources.orig/kernel/sched.c	2005-01-14 21:03:14.835700384 +0000
+++ xx-sources/kernel/sched.c	2005-01-14 21:04:02.783411224 +0000
@@ -249,45 +249,7 @@
 #endif
 
 #ifdef CONFIG_SCHEDSTATS
-	/* latency stats */
-	struct sched_info rq_sched_info;
-
-	/* sys_sched_yield() stats */
-	unsigned long yld_exp_empty;
-	unsigned long yld_act_empty;
-	unsigned long yld_both_empty;
-	unsigned long yld_cnt;
-
-	/* schedule() stats */
-	unsigned long sched_noswitch;
-	unsigned long sched_switch;
-	unsigned long sched_cnt;
-	unsigned long sched_goidle;
-
-	/* pull_task() stats */
-	unsigned long pt_gained[MAX_IDLE_TYPES];
-	unsigned long pt_lost[MAX_IDLE_TYPES];
-
-	/* active_load_balance() stats */
-	unsigned long alb_cnt;
-	unsigned long alb_lost;
-	unsigned long alb_gained;
-	unsigned long alb_failed;
-
-	/* try_to_wake_up() stats */
-	unsigned long ttwu_cnt;
-	unsigned long ttwu_attempts;
-	unsigned long ttwu_moved;
-
-	/* wake_up_new_task() stats */
-	unsigned long wunt_cnt;
-	unsigned long wunt_moved;
-
-	/* sched_migrate_task() stats */
-	unsigned long smt_cnt;
-
-	/* sched_balance_exec() stats */
-	unsigned long sbe_cnt;
+	schedstat_pcd_t *sspcd;
 #endif
 };
 
@@ -344,105 +306,6 @@
 	spin_unlock_irqrestore(&rq->lock, *flags);
 }
 
-#ifdef CONFIG_SCHEDSTATS
-/*
- * bump this up when changing the output format or the meaning of an existing
- * format, so that tools can adapt (or abort)
- */
-#define SCHEDSTAT_VERSION 10
-
-static int show_schedstat(struct seq_file *seq, void *v)
-{
-	int cpu;
-	enum idle_type itype;
-
-	seq_printf(seq, "version %d\n", SCHEDSTAT_VERSION);
-	seq_printf(seq, "timestamp %lu\n", jiffies);
-	for_each_online_cpu(cpu) {
-		runqueue_t *rq = cpu_rq(cpu);
-#ifdef CONFIG_SMP
-		struct sched_domain *sd;
-		int dcnt = 0;
-#endif
-
-		/* runqueue-specific stats */
-		seq_printf(seq,
-		    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu "
-		    "%lu %lu %lu %lu %lu %lu %lu %lu %lu %lu",
-		    cpu, rq->yld_both_empty,
-		    rq->yld_act_empty, rq->yld_exp_empty,
-		    rq->yld_cnt, rq->sched_noswitch,
-		    rq->sched_switch, rq->sched_cnt, rq->sched_goidle,
-		    rq->alb_cnt, rq->alb_gained, rq->alb_lost,
-		    rq->alb_failed,
-		    rq->ttwu_cnt, rq->ttwu_moved, rq->ttwu_attempts,
-		    rq->wunt_cnt, rq->wunt_moved,
-		    rq->smt_cnt, rq->sbe_cnt, rq->rq_sched_info.cpu_time,
-		    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcnt);
-
-		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++)
-			seq_printf(seq, " %lu %lu", rq->pt_gained[itype],
-						    rq->pt_lost[itype]);
-		seq_printf(seq, "\n");
-
-#ifdef CONFIG_SMP
-		/* domain-specific stats */
-		for_each_domain(cpu, sd) {
-			char mask_str[NR_CPUS];
-
-			cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
-			seq_printf(seq, "domain%d %s", dcnt++, mask_str);
-			for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES;
-						itype++) {
-				seq_printf(seq, " %lu %lu %lu %lu %lu",
-				    sd->lb_cnt[itype],
-				    sd->lb_failed[itype],
-				    sd->lb_imbalance[itype],
-				    sd->lb_nobusyq[itype],
-				    sd->lb_nobusyg[itype]);
-			}
-			seq_printf(seq, " %lu %lu %lu %lu\n",
-			    sd->sbe_pushed, sd->sbe_attempts,
-			    sd->ttwu_wake_affine, sd->ttwu_wake_balance);
-		}
-#endif
-	}
-	return 0;
-}
-
-static int schedstat_open(struct inode *inode, struct file *file)
-{
-	unsigned int size = PAGE_SIZE * (1 + num_online_cpus() / 32);
-	char *buf = kmalloc(size, GFP_KERNEL);
-	struct seq_file *m;
-	int res;
-
-	if (!buf)
-		return -ENOMEM;
-	res = single_open(file, show_schedstat, NULL);
-	if (!res) {
-		m = file->private_data;
-		m->buf = buf;
-		m->size = size;
-	} else
-		kfree(buf);
-	return res;
-}
-
-struct file_operations proc_schedstat_operations = {
-	.open    = schedstat_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
-	.release = single_release,
-};
-
-# define schedstat_inc(rq, field)	rq->field++;
-# define schedstat_add(rq, field, amt)	rq->field += amt;
-#else /* !CONFIG_SCHEDSTATS */
-# define schedstat_inc(rq, field)	do { } while (0);
-# define schedstat_add(rq, field, amt)	do { } while (0);
-#endif
-
 /*
  * rq_lock - lock a given runqueue and disable interrupts.
  */
@@ -474,112 +337,6 @@
 #define cpu_and_siblings_are_idle(A) idle_cpu(A)
 #endif
 
-#ifdef CONFIG_SCHEDSTATS
-/*
- * Called when a process is dequeued from the active array and given
- * the cpu.  We should note that with the exception of interactive
- * tasks, the expired queue will become the active queue after the active
- * queue is empty, without explicitly dequeuing and requeuing tasks in the
- * expired queue.  (Interactive tasks may be requeued directly to the
- * active queue, thus delaying tasks in the expired queue from running;
- * see scheduler_tick()).
- *
- * This function is only called from sched_info_arrive(), rather than
- * dequeue_task(). Even though a task may be queued and dequeued multiple
- * times as it is shuffled about, we're really interested in knowing how
- * long it was from the *first* time it was queued to the time that it
- * finally hit a cpu.
- */
-static inline void sched_info_dequeued(task_t *t)
-{
-	t->sched_info.last_queued = 0;
-}
-
-/*
- * Called when a task finally hits the cpu.  We can now calculate how
- * long it was waiting to run.  We also note when it began so that we
- * can keep stats on how long its timeslice is.
- */
-static inline void sched_info_arrive(task_t *t)
-{
-	unsigned long now = jiffies, diff = 0;
-	struct runqueue *rq = task_rq(t);
-
-	if (t->sched_info.last_queued)
-		diff = now - t->sched_info.last_queued;
-	sched_info_dequeued(t);
-	t->sched_info.run_delay += diff;
-	t->sched_info.last_arrival = now;
-	t->sched_info.pcnt++;
-
-	if (!rq)
-		return;
-
-	rq->rq_sched_info.run_delay += diff;
-	rq->rq_sched_info.pcnt++;
-}
-
-/*
- * Called when a process is queued into either the active or expired
- * array.  The time is noted and later used to determine how long we
- * had to wait for us to reach the cpu.  Since the expired queue will
- * become the active queue after active queue is empty, without dequeuing
- * and requeuing any tasks, we are interested in queuing to either. It
- * is unusual but not impossible for tasks to be dequeued and immediately
- * requeued in the same or another array: this can happen in sched_yield(),
- * set_user_nice(), and even load_balance() as it moves tasks from runqueue
- * to runqueue.
- *
- * This function is only called from enqueue_task(), but also only updates
- * the timestamp if it is already not set.  It's assumed that
- * sched_info_dequeued() will clear that stamp when appropriate.
- */
-static inline void sched_info_queued(task_t *t)
-{
-	if (!t->sched_info.last_queued)
-		t->sched_info.last_queued = jiffies;
-}
-
-/*
- * Called when a process ceases being the active-running process, either
- * voluntarily or involuntarily.  Now we can calculate how long we ran.
- */
-static inline void sched_info_depart(task_t *t)
-{
-	struct runqueue *rq = task_rq(t);
-	unsigned long diff = jiffies - t->sched_info.last_arrival;
-
-	t->sched_info.cpu_time += diff;
-
-	if (rq)
-		rq->rq_sched_info.cpu_time += diff;
-}
-
-/*
- * Called when tasks are switched involuntarily due, typically, to expiring
- * their time slice.  (This may also be called when switching to or from
- * the idle task.)  We are only called when prev != next.
- */
-static inline void sched_info_switch(task_t *prev, task_t *next)
-{
-	struct runqueue *rq = task_rq(prev);
-
-	/*
-	 * prev now departs the cpu.  It's not interesting to record
-	 * stats about how efficient we were at scheduling the idle
-	 * process, however.
-	 */
-	if (prev != rq->idle)
-		sched_info_depart(prev);
-
-	if (next != rq->idle)
-		sched_info_arrive(next);
-}
-#else
-#define sched_info_queued(t)		do { } while (0)
-#define sched_info_switch(t, next)	do { } while (0)
-#endif /* CONFIG_SCHEDSTATS */
-
 /*
  * Adding/removing a task to/from a priority array:
  */
@@ -996,7 +753,7 @@
 #endif
 
 	rq = task_rq_lock(p, &flags);
-	schedstat_inc(rq, ttwu_cnt);
+	schedstat_inc(rq->sspcd, ttwu_cnt);
 	old_state = p->state;
 	if (!(old_state & state))
 		goto out;
@@ -1069,10 +826,10 @@
 
 	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
 out_set_cpu:
-	schedstat_inc(rq, ttwu_attempts);
+	schedstat_inc(rq->sspcd, ttwu_attempts);
 	new_cpu = wake_idle(new_cpu, p);
 	if (new_cpu != cpu) {
-		schedstat_inc(rq, ttwu_moved);
+		schedstat_inc(rq->sspcd, ttwu_moved);
 		set_task_cpu(p, new_cpu);
 		task_rq_unlock(rq, &flags);
 		/* might preempt at this point */
@@ -1202,7 +959,7 @@
 
 	BUG_ON(p->state != TASK_RUNNING);
 
-	schedstat_inc(rq, wunt_cnt);
+	schedstat_inc(rq->sspcd, wunt_cnt);
 	/*
 	 * We decrease the sleep average of forking parents
 	 * and children as well, to keep max-interactive tasks
@@ -1254,7 +1011,7 @@
 		if (TASK_PREEMPTS_CURR(p, rq))
 			resched_task(rq->curr);
 
-		schedstat_inc(rq, wunt_moved);
+		schedstat_inc(rq->sspcd, wunt_moved);
 		/*
 		 * Parent and child are on different CPUs, now get the
 		 * parent runqueue to update the parent's ->u.ingosched.sleep_avg:
@@ -1558,7 +1315,7 @@
 	    || unlikely(cpu_is_offline(dest_cpu)))
 		goto out;
 
-	schedstat_inc(rq, smt_cnt);
+	schedstat_inc(rq->sspcd, smt_cnt);
 	/* force the process onto the specified CPU */
 	if (migrate_task(p, dest_cpu, &req)) {
 		/* Need to wait for migration thread (might exit: take ref). */
@@ -1586,7 +1343,7 @@
 	struct sched_domain *tmp, *sd = NULL;
 	int new_cpu, this_cpu = get_cpu();
 
-	schedstat_inc(this_rq(), sbe_cnt);
+	schedstat_inc(this_rq()->sspcd, sbe_cnt);
 	/* Prefer the current CPU if there's only this task running */
 	if (this_rq()->nr_running <= 1)
 		goto out;
@@ -1734,8 +1491,8 @@
 	 * so we can safely collect pull_task() stats here rather than
 	 * inside pull_task().
 	 */
-	schedstat_inc(this_rq, pt_gained[idle]);
-	schedstat_inc(busiest, pt_lost[idle]);
+	schedstat_inc(this_rq->sspcd, pt_gained[idle]);
+	schedstat_inc(busiest->sspcd, pt_lost[idle]);
 
 	pull_task(busiest, array, tmp, this_rq, dst_array, this_cpu);
 	pulled++;
@@ -2092,7 +1849,7 @@
 	cpumask_t visited_cpus;
 	int cpu;
 
-	schedstat_inc(busiest_rq, alb_cnt);
+	schedstat_inc(busiest_rq->sspcd, alb_cnt);
 	/*
 	 * Search for suitable CPUs to push tasks to in successively higher
 	 * domains with SD_LOAD_BALANCE set.
@@ -2127,10 +1884,10 @@
 				double_lock_balance(busiest_rq, target_rq);
 				if (move_tasks(target_rq, cpu, busiest_rq,
 						1, sd, SCHED_IDLE)) {
-					schedstat_inc(busiest_rq, alb_lost);
-					schedstat_inc(target_rq, alb_gained);
+					schedstat_inc(busiest_rq->sspcd, alb_lost);
+					schedstat_inc(target_rq->sspcd, alb_gained);
 				} else {
-					schedstat_inc(busiest_rq, alb_failed);
+					schedstat_inc(busiest_rq->sspcd, alb_failed);
 				}
 				spin_unlock(&target_rq->lock);
 			}
@@ -2649,7 +2406,7 @@
 		dump_stack();
 	}
 
-	schedstat_inc(rq, sched_cnt);
+	schedstat_inc(rq->sspcd, sched_cnt);
 	now = sched_clock();
 	if (likely(now - prev->u.ingosched.timestamp < NS_MAX_SLEEP_AVG))
 		run_time = now - prev->u.ingosched.timestamp;
@@ -2715,14 +2472,14 @@
 		/*
 		 * Switch the active and expired arrays.
 		 */
-		schedstat_inc(rq, sched_switch);
+		schedstat_inc(rq->sspcd, sched_switch);
 		rq->active = rq->expired;
 		rq->expired = array;
 		array = rq->active;
 		rq->expired_timestamp = 0;
 		rq->best_expired_prio = MAX_PRIO;
 	} else
-		schedstat_inc(rq, sched_noswitch);
+		schedstat_inc(rq->sspcd, sched_noswitch);
 
 	idx = sched_find_first_bit(array->bitmap);
 	queue = array->queue + idx;
@@ -2742,7 +2499,7 @@
 	next->u.ingosched.activated = 0;
 switch_tasks:
 	if (next == rq->idle)
-		schedstat_inc(rq, sched_goidle);
+		schedstat_inc(rq->sspcd, sched_goidle);
 	prefetch(next);
 	clear_tsk_need_resched(prev);
 	rcu_qsctr_inc(task_cpu(prev));
@@ -3015,7 +2772,7 @@
 	prio_array_t *array = current->u.ingosched.array;
 	prio_array_t *target = rq->expired;
 
-	schedstat_inc(rq, yld_cnt);
+	schedstat_inc(rq->sspcd, yld_cnt);
 	/*
 	 * We implement yielding by moving the task into the expired
 	 * queue.
@@ -3027,11 +2784,11 @@
 		target = rq->active;
 
 	if (current->u.ingosched.array->nr_active == 1) {
-		schedstat_inc(rq, yld_act_empty);
+		schedstat_inc(rq->sspcd, yld_act_empty);
 		if (!rq->expired->nr_active)
-			schedstat_inc(rq, yld_both_empty);
+			schedstat_inc(rq->sspcd, yld_both_empty);
 	} else if (!rq->expired->nr_active)
-		schedstat_inc(rq, yld_exp_empty);
+		schedstat_inc(rq->sspcd, yld_exp_empty);
 
 	if (array != target) {
 		dequeue_task(current, array);
@@ -3146,6 +2903,33 @@
 }
 
 #ifdef CONFIG_SMP
+#ifdef CONFIG_SCHEDSTATS
+static void ingo_show_schedstat_sd(struct seq_file *seq, int cpu)
+{
+	enum idle_type itype;
+	struct sched_domain *sd;
+	int dcnt = 0;
+
+	for_each_domain(cpu, sd) {
+		char mask_str[NR_CPUS];
+
+		cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
+		seq_printf(seq, "domain%d %s", dcnt++, mask_str);
+		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++) {
+			seq_printf(seq, " %lu %lu %lu %lu %lu",
+				sd->lb_cnt[itype],
+				sd->lb_failed[itype],
+				sd->lb_imbalance[itype],
+				sd->lb_nobusyq[itype],
+				sd->lb_nobusyg[itype]);
+		}
+		seq_printf(seq, " %lu %lu %lu %lu\n",
+			sd->sbe_pushed, sd->sbe_attempts,
+			sd->ttwu_wake_affine, sd->ttwu_wake_balance);
+	}
+}
+#endif
+
 /*
  * This is how migration works:
  *
@@ -3992,6 +3776,9 @@
 			// delimiter for bitsearch
 			__set_bit(MAX_PRIO, array->bitmap);
 		}
+#ifdef CONFIG_SCHEDSTATS
+		rq->sspcd = cpu_sspcd(i);
+#endif
 	}
 
 	/*
@@ -4146,6 +3933,11 @@
 }
 #endif
 
+static int ingo_is_idle_task(const task_t *p)
+{
+	return p == task_rq(p)->idle;
+}
+
 #ifdef CONFIG_MAGIC_SYSRQ
 void normalize_rt_tasks(void)
 {
@@ -4201,6 +3993,7 @@
 	.set_user_nice		= ingo_set_user_nice,
 	.rr_get_interval	= ingo_sys_sched_rr_get_interval,
 	.yield			= ingo_sys_sched_yield,
+	.is_idle_task		= ingo_is_idle_task,
 	.task_curr		= ingo_task_curr,
 	.task_nice		= ingo_task_nice,
 	.task_prio		= ingo_task_prio,
@@ -4215,5 +4008,8 @@
 #ifdef CONFIG_HOTPLUG_CPU
 	.sched_idle_next	= ingo_sched_idle_next,
 #endif	
+#ifdef CONFIG_SCHEDSTATS
+	.show_schedstat_sd	= ingo_show_schedstat_sd,
+#endif
 #endif
 };
Index: xx-sources/kernel/scheduler.c
===================================================================
--- xx-sources.orig/kernel/scheduler.c	2005-01-14 21:03:14.835700384 +0000
+++ xx-sources/kernel/scheduler.c	2005-01-14 21:03:37.526250896 +0000
@@ -969,6 +969,185 @@
 }
 #endif
 
+#ifdef CONFIG_SCHEDSTATS
+/*
+ * bump this up when changing the output format or the meaning of an existing
+ * format, so that tools can adapt (or abort)
+ */
+#define SCHEDSTAT_VERSION 10
+
+DEFINE_PER_CPU(struct schedstat_per_cpu_data, schedstat_pcd_data);
+
+#ifdef CONFIG_SMP
+static void show_schedstat_sd(struct seq_file *seq, int cpu);
+#else
+static inline void show_schedstat_sd(struct seq_file *seq, int cpu)
+{
+}
+#endif
+
+int show_schedstat(struct seq_file *seq, void *v)
+{
+	int cpu;
+	enum idle_type itype;
+
+	seq_printf(seq, "version %d\n", SCHEDSTAT_VERSION);
+	seq_printf(seq, "timestamp %lu\n", jiffies);
+	for_each_online_cpu(cpu) {
+		schedstat_pcd_t *sspcd = cpu_sspcd(cpu);
+
+		/* schedstat_per_cpu-specific stats */
+		seq_printf(seq,
+		    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu "
+		    "%lu %lu %lu %lu %lu %lu %lu %lu %lu %lu",
+		    cpu, sspcd->yld_both_empty,
+		    sspcd->yld_act_empty, sspcd->yld_exp_empty,
+		    sspcd->yld_cnt, sspcd->sched_noswitch,
+		    sspcd->sched_switch, sspcd->sched_cnt, sspcd->sched_goidle,
+		    sspcd->alb_cnt, sspcd->alb_gained, sspcd->alb_lost,
+		    sspcd->alb_failed,
+		    sspcd->ttwu_cnt, sspcd->ttwu_moved, sspcd->ttwu_attempts,
+		    sspcd->wunt_cnt, sspcd->wunt_moved,
+		    sspcd->smt_cnt, sspcd->sbe_cnt, sspcd->rq_sched_info.cpu_time,
+		    sspcd->rq_sched_info.run_delay, sspcd->rq_sched_info.pcnt);
+
+		for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES; itype++)
+			seq_printf(seq, " %lu %lu", sspcd->pt_gained[itype],
+						    sspcd->pt_lost[itype]);
+		seq_printf(seq, "\n");
+
+		/* domain-specific stats */
+		show_schedstat_sd(seq, cpu);
+	}
+	return 0;
+}
+
+/*
+ * Called when a process is dequeued from the active array and given
+ * the cpu.  We should note that with the exception of interactive
+ * tasks, the expired queue will become the active queue after the active
+ * queue is empty, without explicitly dequeuing and requeuing tasks in the
+ * expired queue.  (Interactive tasks may be requeued directly to the
+ * active queue, thus delaying tasks in the expired queue from running;
+ * see scheduler_tick()).
+ *
+ * This function is only called from sched_info_arrive(), rather than
+ * dequeue_task(). Even though a task may be queued and dequeued multiple
+ * times as it is shuffled about, we're really interested in knowing how
+ * long it was from the *first* time it was queued to the time that it
+ * finally hit a cpu.
+ */
+static inline void sched_info_dequeued(task_t *t)
+{
+	t->sched_info.last_queued = 0;
+}
+
+/*
+ * Called when a task finally hits the cpu.  We can now calculate how
+ * long it was waiting to run.  We also note when it began so that we
+ * can keep stats on how long its timeslice is.
+ */
+static inline void sched_info_arrive(task_t *t)
+{
+	unsigned long now = jiffies, diff = 0;
+	schedstat_pcd_t *sspcd = task_sspcd(t);
+
+	if (t->sched_info.last_queued)
+		diff = now - t->sched_info.last_queued;
+	sched_info_dequeued(t);
+	t->sched_info.run_delay += diff;
+	t->sched_info.last_arrival = now;
+	t->sched_info.pcnt++;
+
+	if (!sspcd)
+		return;
+
+	sspcd->rq_sched_info.run_delay += diff;
+	sspcd->rq_sched_info.pcnt++;
+}
+
+/*
+ * Called when a process is queued into either the active or expired
+ * array.  The time is noted and later used to determine how long we
+ * had to wait for us to reach the cpu.  Since the expired queue will
+ * become the active queue after active queue is empty, without dequeuing
+ * and requeuing any tasks, we are interested in queuing to either. It
+ * is unusual but not impossible for tasks to be dequeued and immediately
+ * requeued in the same or another array: this can happen in sched_yield(),
+ * set_user_nice(), and even load_balance() as it moves tasks from runqueue
+ * to runqueue.
+ *
+ * This function is only called from enqueue_task(), but also only updates
+ * the timestamp if it is already not set.  It's assumed that
+ * sched_info_dequeued() will clear that stamp when appropriate.
+ */
+void sched_info_queued(task_t *t)
+{
+	if (!t->sched_info.last_queued)
+		t->sched_info.last_queued = jiffies;
+}
+
+/*
+ * Called when a process ceases being the active-running process, either
+ * voluntarily or involuntarily.  Now we can calculate how long we ran.
+ */
+static inline void sched_info_depart(task_t *t)
+{
+	schedstat_pcd_t *sspcd = task_sspcd(t);
+	unsigned long diff = jiffies - t->sched_info.last_arrival;
+
+	t->sched_info.cpu_time += diff;
+
+	if (sspcd)
+		sspcd->rq_sched_info.cpu_time += diff;
+}
+
+/*
+ * Called when tasks are switched involuntarily due, typically, to expiring
+ * their time slice.  (This may also be called when switching to or from
+ * the idle task.)  We are only called when prev != next.
+ */
+void sched_info_switch(task_t *prev, task_t *next)
+{
+	/*
+	 * prev now departs the cpu.  It's not interesting to record
+	 * stats about how efficient we were at scheduling the idle
+	 * process, however.
+	 */
+	if (!is_idle_task(prev))
+		sched_info_depart(prev);
+
+	if (!is_idle_task(next))
+		sched_info_arrive(next);
+}
+
+static int schedstat_open(struct inode *inode, struct file *file)
+{
+	unsigned int size = PAGE_SIZE * (1 + num_online_cpus() / 32);
+	char *buf = kmalloc(size, GFP_KERNEL);
+	struct seq_file *m;
+	int res;
+
+	if (!buf)
+		return -ENOMEM;
+	res = single_open(file, show_schedstat, NULL);
+	if (!res) {
+		m = file->private_data;
+		m->buf = buf;
+		m->size = size;
+	} else
+		kfree(buf);
+	return res;
+}
+
+struct file_operations proc_schedstat_operations = {
+	.open    = schedstat_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+#endif
+
 extern struct sched_drv ingo_sched_drv;
 
 struct sched_drv *scheduler =
@@ -1138,6 +1317,11 @@
 	return scheduler->sched_setscheduler(p, policy, param);
 }
 
+int is_idle_task(const task_t *task)
+{
+	return scheduler->is_idle_task(task);
+}
+
 int task_curr(const task_t *task)
 {
 	return scheduler->task_curr(task);
@@ -1177,3 +1361,12 @@
 {
 	scheduler->tail(task);
 }
+
+#ifdef CONFIG_SCHEDSTATS
+#ifdef CONFIG_SMP
+static void show_schedstat_sd(struct seq_file *seq, int cpu)
+{
+	scheduler->show_schedstat_sd(seq, cpu);
+}
+#endif
+#endif
