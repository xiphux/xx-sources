The task_struct needs to be flexible enough to have private entries unique
to each cpu scheduler design. Remove the variables that are likely to
differ between designs and put them in a separate per-scheduler struct
enclosed within a common union. 

Remove the private data from INIT_TASK and place it into sched_init().

Update all entries in sched.c to reflect the new task_struct design.

Signed-off-by: Con Kolivas <kernel@kolivas.org>

Index: xx-sources/include/linux/init_task.h
===================================================================
--- xx-sources.orig/include/linux/init_task.h	2005-01-14 17:36:21.000000000 +0000
+++ xx-sources/include/linux/init_task.h	2005-01-14 20:58:44.145851496 +0000
@@ -73,14 +73,10 @@
 	.usage		= ATOMIC_INIT(2),				\
 	.flags		= 0,						\
 	.lock_depth	= -1,						\
-	.prio		= MAX_PRIO-20,					\
-	.static_prio	= MAX_PRIO-20,					\
 	.policy		= SCHED_NORMAL,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
-	.run_list	= LIST_HEAD_INIT(tsk.run_list),			\
-	.time_slice	= HZ,						\
 	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
 	.ptrace_children= LIST_HEAD_INIT(tsk.ptrace_children),		\
 	.ptrace_list	= LIST_HEAD_INIT(tsk.ptrace_list),		\
@@ -115,5 +111,4 @@
 	.private_pages	= LIST_HEAD_INIT(tsk.private_pages),		\
 	.private_pages_count = 0,					\
 }
-
 #endif
Index: xx-sources/include/linux/sched.h
===================================================================
--- xx-sources.orig/include/linux/sched.h	2005-01-14 20:58:08.954201440 +0000
+++ xx-sources/include/linux/sched.h	2005-01-14 20:58:44.146851344 +0000
@@ -540,17 +540,13 @@
 
 	int lock_depth;		/* Lock depth */
 
-	int prio, static_prio;
-	struct list_head run_list;
-	prio_array_t *array;
-
-	unsigned long sleep_avg;
-	unsigned long long timestamp, last_ran;
-	int activated;
-
+	int static_prio;	/* A commonality between cpu schedulers */
+	union {
+		struct cpusched_ingo ingosched;
+	} u;
+  
 	unsigned long policy;
 	cpumask_t cpus_allowed;
-	unsigned int time_slice, first_time_slice;
 
 #ifdef CONFIG_SCHEDSTATS
 	struct sched_info sched_info;
Index: xx-sources/include/linux/scheduler.h
===================================================================
--- xx-sources.orig/include/linux/scheduler.h	2005-01-14 20:58:08.954201440 +0000
+++ xx-sources/include/linux/scheduler.h	2005-01-14 20:58:44.146851344 +0000
@@ -40,4 +40,16 @@
 #endif
 };
 
+struct cpusched_ingo {
+	int prio;
+	struct list_head run_list;
+	prio_array_t *array;
+	unsigned int time_slice;
+	unsigned int first_time_slice;
+	unsigned long sleep_avg;
+	unsigned long timestamp;
+	unsigned long long last_ran;
+	int activated;
+};
+
 #endif
Index: xx-sources/kernel/sched.c
===================================================================
--- xx-sources.orig/kernel/sched.c	2005-01-14 20:58:08.956201136 +0000
+++ xx-sources/kernel/sched.c	2005-01-14 20:59:48.278101912 +0000
@@ -47,6 +47,7 @@
 #include <linux/sysctl.h>
 #include <linux/syscalls.h>
 #include <linux/times.h>
+#include <linux/list.h>
 #include <linux/ltt-events.h>
 #include <asm/tlb.h>
 
@@ -125,7 +126,7 @@
  */
 
 #define CURRENT_BONUS(p) \
-	(NS_TO_JIFFIES((p)->sleep_avg) * MAX_BONUS / \
+	(NS_TO_JIFFIES((p)->u.ingosched.sleep_avg) * MAX_BONUS / \
 		MAX_SLEEP_AVG)
 
 #define GRANULARITY	(10 * HZ / 1000 ? : 1)
@@ -146,14 +147,14 @@
 	(SCALE(TASK_NICE(p), 40, MAX_BONUS) + INTERACTIVE_DELTA)
 
 #define TASK_INTERACTIVE(p) \
-	((p)->prio <= (p)->static_prio - DELTA(p))
+	((p)->u.ingosched.prio <= (p)->static_prio - DELTA(p))
 
 #define INTERACTIVE_SLEEP(p) \
 	(JIFFIES_TO_NS(MAX_SLEEP_AVG * \
 		(MAX_BONUS / 2 + DELTA((p)) + 1) / MAX_BONUS - 1))
 
 #define TASK_PREEMPTS_CURR(p, rq) \
-	((p)->prio < (rq)->curr->prio)
+	((p)->u.ingosched.prio < (rq)->curr->u.ingosched.prio)
 
 /*
  * task_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]
@@ -174,7 +175,7 @@
 	else
 		return SCALE_PRIO(DEF_TIMESLICE, p->static_prio);
 }
-#define task_hot(p, now, sd) ((long long) ((now) - (p)->last_ran)	\
+#define task_hot(p, now, sd) ((long long) ((now) - (p)->u.ingosched.last_ran)	\
 				< (long long) (sd)->cache_hot_time)
 
 /*
@@ -293,7 +294,7 @@
 
 static int ingo_rt_task(const task_t *p)
 {
-	return (unlikely((p)->prio < MAX_RT_PRIO));
+	return (unlikely((p)->u.ingosched.prio < MAX_RT_PRIO));
 }
 
 /*
@@ -576,18 +577,18 @@
 static void dequeue_task(struct task_struct *p, prio_array_t *array)
 {
 	array->nr_active--;
-	list_del(&p->run_list);
-	if (list_empty(array->queue + p->prio))
-		__clear_bit(p->prio, array->bitmap);
+	list_del(&p->u.ingosched.run_list);
+	if (list_empty(array->queue + p->u.ingosched.prio))
+		__clear_bit(p->u.ingosched.prio, array->bitmap);
 }
 
 static void enqueue_task(struct task_struct *p, prio_array_t *array)
 {
 	sched_info_queued(p);
-	list_add_tail(&p->run_list, array->queue + p->prio);
-	__set_bit(p->prio, array->bitmap);
+	list_add_tail(&p->u.ingosched.run_list, array->queue + p->u.ingosched.prio);
+	__set_bit(p->u.ingosched.prio, array->bitmap);
 	array->nr_active++;
-	p->array = array;
+	p->u.ingosched.array = array;
 }
 
 /*
@@ -596,20 +597,20 @@
  */
 static void requeue_task(struct task_struct *p, prio_array_t *array)
 {
-	list_move_tail(&p->run_list, array->queue + p->prio);
+	list_move_tail(&p->u.ingosched.run_list, array->queue + p->u.ingosched.prio);
 }
 
 static inline void enqueue_task_head(struct task_struct *p, prio_array_t *array)
 {
-	list_add(&p->run_list, array->queue + p->prio);
-	__set_bit(p->prio, array->bitmap);
+	list_add(&p->u.ingosched.run_list, array->queue + p->u.ingosched.prio);
+	__set_bit(p->u.ingosched.prio, array->bitmap);
 	array->nr_active++;
-	p->array = array;
+	p->u.ingosched.array = array;
 }
 
 static void ingo_set_oom_timeslice(task_t *p)
 {
-	p->time_slice = HZ;
+	p->u.ingosched.time_slice = HZ;
 }
 
 /*
@@ -631,7 +632,7 @@
 	int bonus, prio;
 
 	if (rt_task(p))
-		return p->prio;
+		return p->u.ingosched.prio;
 
 	bonus = CURRENT_BONUS(p) - MAX_BONUS / 2;
 
@@ -663,7 +664,7 @@
 
 static void recalc_task_prio(task_t *p, unsigned long long now)
 {
-	unsigned long long __sleep_time = now - p->timestamp;
+	unsigned long long __sleep_time = now - p->u.ingosched.timestamp;
 	unsigned long sleep_time;
 
 	if (__sleep_time > NS_MAX_SLEEP_AVG)
@@ -678,9 +679,9 @@
 		 * prevent them suddenly becoming cpu hogs and starving
 		 * other processes.
 		 */
-		if (p->mm && p->activated != -1 &&
+		if (p->mm && p->u.ingosched.activated != -1 &&
 			sleep_time > INTERACTIVE_SLEEP(p)) {
-				p->sleep_avg = JIFFIES_TO_NS(MAX_SLEEP_AVG -
+				p->u.ingosched.sleep_avg = JIFFIES_TO_NS(MAX_SLEEP_AVG -
 						DEF_TIMESLICE);
 		} else {
 			/*
@@ -694,12 +695,12 @@
 			 * limited in their sleep_avg rise as they
 			 * are likely to be waiting on I/O
 			 */
-			if (p->activated == -1 && p->mm) {
-				if (p->sleep_avg >= INTERACTIVE_SLEEP(p))
+			if (p->u.ingosched.activated == -1 && p->mm) {
+				if (p->u.ingosched.sleep_avg >= INTERACTIVE_SLEEP(p))
 					sleep_time = 0;
-				else if (p->sleep_avg + sleep_time >=
+				else if (p->u.ingosched.sleep_avg + sleep_time >=
 						INTERACTIVE_SLEEP(p)) {
-					p->sleep_avg = INTERACTIVE_SLEEP(p);
+					p->u.ingosched.sleep_avg = INTERACTIVE_SLEEP(p);
 					sleep_time = 0;
 				}
 			}
@@ -712,14 +713,14 @@
 			 * task spends sleeping, the higher the average gets -
 			 * and the higher the priority boost gets as well.
 			 */
-			p->sleep_avg += sleep_time;
+			p->u.ingosched.sleep_avg += sleep_time;
 
-			if (p->sleep_avg > NS_MAX_SLEEP_AVG)
-				p->sleep_avg = NS_MAX_SLEEP_AVG;
+			if (p->u.ingosched.sleep_avg > NS_MAX_SLEEP_AVG)
+				p->u.ingosched.sleep_avg = NS_MAX_SLEEP_AVG;
 		}
 	}
 
-	p->prio = effective_prio(p);
+	p->u.ingosched.prio = effective_prio(p);
 }
 
 /*
@@ -748,7 +749,7 @@
 	 * This checks to make sure it's not an uninterruptible task
 	 * that is now waking up.
 	 */
-	if (!p->activated) {
+	if (!p->u.ingosched.activated) {
 		/*
 		 * Tasks which were woken up by interrupts (ie. hw events)
 		 * are most likely of interactive nature. So we give them
@@ -757,16 +758,16 @@
 		 * on a CPU, first time around:
 		 */
 		if (in_interrupt())
-			p->activated = 2;
+			p->u.ingosched.activated = 2;
 		else {
 			/*
 			 * Normal first-time wakeups get a credit too for
 			 * on-runqueue time, but it will be weighted down:
 			 */
-			p->activated = 1;
+			p->u.ingosched.activated = 1;
 		}
 	}
-	p->timestamp = now;
+	p->u.ingosched.timestamp = now;
 
 	__activate_task(p, rq);
 }
@@ -777,8 +778,8 @@
 static void deactivate_task(struct task_struct *p, runqueue_t *rq)
 {
 	rq->nr_running--;
-	dequeue_task(p, p->array);
-	p->array = NULL;
+	dequeue_task(p, p->u.ingosched.array);
+	p->u.ingosched.array = NULL;
 }
 
 /*
@@ -851,7 +852,7 @@
 	 * If the task is not on a runqueue (and not running), then
 	 * it is sufficient to simply update the task's cpu field.
 	 */
-	if (!p->array && !task_running(rq, p)) {
+	if (!p->u.ingosched.array && !task_running(rq, p)) {
 		set_task_cpu(p, dest_cpu);
 		return 0;
 	}
@@ -882,7 +883,7 @@
 repeat:
 	rq = task_rq_lock(p, &flags);
 	/* Must be off runqueue entirely, not preempted. */
-	if (unlikely(p->array)) {
+	if (unlikely(p->u.ingosched.array)) {
 		/* If it's preempted, we yield.  It could be a while. */
 		preempted = !task_running(rq, p);
 		task_rq_unlock(rq, &flags);
@@ -991,7 +992,7 @@
 	if (!(old_state & state))
 		goto out;
 
-	if (p->array)
+	if (p->u.ingosched.array)
 		goto out_running;
 
 	cpu = task_cpu(p);
@@ -1070,7 +1071,7 @@
 		old_state = p->state;
 		if (!(old_state & state))
 			goto out;
-		if (p->array)
+		if (p->u.ingosched.array)
 			goto out_running;
 
 		this_cpu = smp_processor_id();
@@ -1085,7 +1086,7 @@
 		 * Tasks on involuntary sleep don't earn
 		 * sleep_avg beyond just interactive state.
 		 */
-		p->activated = -1;
+		p->u.ingosched.activated = -1;
 	}
 
 	/*
@@ -1129,8 +1130,8 @@
 	 * event cannot wake it up and insert it on the runqueue either.
 	 */
 	p->state = TASK_RUNNING;
-	INIT_LIST_HEAD(&p->run_list);
-	p->array = NULL;
+	INIT_LIST_HEAD(&p->u.ingosched.run_list);
+	p->u.ingosched.array = NULL;
 	spin_lock_init(&p->switch_lock);
 #ifdef CONFIG_SCHEDSTATS
 	memset(&p->sched_info, 0, sizeof(p->sched_info));
@@ -1150,21 +1151,21 @@
 	 * resulting in more scheduling fairness.
 	 */
 	local_irq_disable();
-	p->time_slice = (current->time_slice + 1) >> 1;
+	p->u.ingosched.time_slice = (current->u.ingosched.time_slice + 1) >> 1;
 	/*
 	 * The remainder of the first timeslice might be recovered by
 	 * the parent if the child exits early enough.
 	 */
-	p->first_time_slice = 1;
-	current->time_slice >>= 1;
-	p->timestamp = sched_clock();
-	if (unlikely(!current->time_slice)) {
+	p->u.ingosched.first_time_slice = 1;
+	current->u.ingosched.time_slice >>= 1;
+	p->u.ingosched.timestamp = sched_clock();
+	if (unlikely(!current->u.ingosched.time_slice)) {
 		/*
 		 * This case is rare, it happens when the parent has only
 		 * a single jiffy left from its timeslice. Taking the
 		 * runqueue lock is not a problem.
 		 */
-		current->time_slice = 1;
+		current->u.ingosched.time_slice = 1;
 		preempt_disable();
 		scheduler_tick();
 		local_irq_enable();
@@ -1199,10 +1200,10 @@
 	 * from forking tasks that are max-interactive. The parent
 	 * (current) is done further down, under its lock.
 	 */
-	p->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(p) *
+	p->u.ingosched.sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(p) *
 		CHILD_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
 
-	p->prio = effective_prio(p);
+	p->u.ingosched.prio = effective_prio(p);
 
 	if (likely(cpu == this_cpu)) {
 		if (!(clone_flags & CLONE_VM)) {
@@ -1211,13 +1212,13 @@
 			 * do child-runs-first in anticipation of an exec. This
 			 * usually avoids a lot of COW overhead.
 			 */
-			if (unlikely(!current->array))
+			if (unlikely(!current->u.ingosched.array))
 				__activate_task(p, rq);
 			else {
-				p->prio = current->prio;
-				list_add_tail(&p->run_list, &current->run_list);
-				p->array = current->array;
-				p->array->nr_active++;
+				p->u.ingosched.prio = current->u.ingosched.prio;
+				list_add_tail(&p->u.ingosched.run_list, &current->u.ingosched.run_list);
+				p->u.ingosched.array = current->u.ingosched.array;
+				p->u.ingosched.array->nr_active++;
 				rq->nr_running++;
 			}
 			set_need_resched();
@@ -1238,7 +1239,7 @@
 		 * Not the local CPU - must adjust timestamp. This should
 		 * get optimised away in the !CONFIG_SMP case.
 		 */
-		p->timestamp = (p->timestamp - this_rq->timestamp_last_tick)
+		p->u.ingosched.timestamp = (p->u.ingosched.timestamp - this_rq->timestamp_last_tick)
 					+ rq->timestamp_last_tick;
 		__activate_task(p, rq);
 		if (TASK_PREEMPTS_CURR(p, rq))
@@ -1247,12 +1248,12 @@
 		schedstat_inc(rq, wunt_moved);
 		/*
 		 * Parent and child are on different CPUs, now get the
-		 * parent runqueue to update the parent's ->sleep_avg:
+		 * parent runqueue to update the parent's ->u.ingosched.sleep_avg:
 		 */
 		task_rq_unlock(rq, &flags);
 		this_rq = task_rq_lock(current, &flags);
 	}
-	current->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(current) *
+	current->u.ingosched.sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(current) *
 		PARENT_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
 	task_rq_unlock(this_rq, &flags);
 }
@@ -1276,14 +1277,14 @@
 	 * the sleep_avg of the parent as well.
 	 */
 	rq = task_rq_lock(p->parent, &flags);
-	if (p->first_time_slice) {
-		p->parent->time_slice += p->time_slice;
-		if (unlikely(p->parent->time_slice > task_timeslice(p)))
-			p->parent->time_slice = task_timeslice(p);
-	}
-	if (p->sleep_avg < p->parent->sleep_avg)
-		p->parent->sleep_avg = p->parent->sleep_avg /
-		(EXIT_WEIGHT + 1) * EXIT_WEIGHT + p->sleep_avg /
+	if (p->u.ingosched.first_time_slice) {
+		p->parent->u.ingosched.time_slice += p->u.ingosched.time_slice;
+		if (unlikely(p->parent->u.ingosched.time_slice > task_timeslice(p)))
+			p->parent->u.ingosched.time_slice = task_timeslice(p);
+	}
+	if (p->u.ingosched.sleep_avg < p->parent->u.ingosched.sleep_avg)
+		p->parent->u.ingosched.sleep_avg = p->parent->u.ingosched.sleep_avg /
+		(EXIT_WEIGHT + 1) * EXIT_WEIGHT + p->u.ingosched.sleep_avg /
 		(EXIT_WEIGHT + 1);
 	task_rq_unlock(rq, &flags);
 }
@@ -1612,7 +1613,7 @@
 	set_task_cpu(p, this_cpu);
 	this_rq->nr_running++;
 	enqueue_task(p, this_array);
-	p->timestamp = (p->timestamp - src_rq->timestamp_last_tick)
+	p->u.ingosched.timestamp = (p->u.ingosched.timestamp - src_rq->timestamp_last_tick)
 				+ this_rq->timestamp_last_tick;
 	/*
 	 * Note that idle threads have a prio of MAX_PRIO, for this test
@@ -1708,7 +1709,7 @@
 	head = array->queue + idx;
 	curr = head->prev;
 skip_queue:
-	tmp = list_entry(curr, task_t, run_list);
+	tmp = list_entry(curr, task_t, u.ingosched.run_list);
 
 	curr = curr->prev;
 
@@ -2395,7 +2396,7 @@
 	}
 
 	/* Task might have expired already, but not scheduled off yet */
-	if (p->array != rq->active) {
+	if (p->u.ingosched.array != rq->active) {
 		set_tsk_need_resched(p);
 		goto out;
 	}
@@ -2412,9 +2413,9 @@
 		 * RR tasks need a special form of timeslice management.
 		 * FIFO tasks have no timeslices.
 		 */
-		if ((p->policy == SCHED_RR) && !--p->time_slice) {
-			p->time_slice = task_timeslice(p);
-			p->first_time_slice = 0;
+		if ((p->policy == SCHED_RR) && !--p->u.ingosched.time_slice) {
+			p->u.ingosched.time_slice = task_timeslice(p);
+			p->u.ingosched.first_time_slice = 0;
 			set_tsk_need_resched(p);
 
 			/* put it at the end of the queue: */
@@ -2422,12 +2423,12 @@
 		}
 		goto out_unlock;
 	}
-	if (!--p->time_slice) {
+	if (!--p->u.ingosched.time_slice) {
 		dequeue_task(p, rq->active);
 		set_tsk_need_resched(p);
-		p->prio = effective_prio(p);
-		p->time_slice = task_timeslice(p);
-		p->first_time_slice = 0;
+		p->u.ingosched.prio = effective_prio(p);
+		p->u.ingosched.time_slice = task_timeslice(p);
+		p->u.ingosched.first_time_slice = 0;
 
 		if (!rq->expired_timestamp)
 			rq->expired_timestamp = jiffies;
@@ -2455,9 +2456,9 @@
 		 * delta range with at least TIMESLICE_GRANULARITY to requeue.
 		 */
 		if (TASK_INTERACTIVE(p) && !((task_timeslice(p) -
-			p->time_slice) % TIMESLICE_GRANULARITY(p)) &&
-			(p->time_slice >= TIMESLICE_GRANULARITY(p)) &&
-			(p->array == rq->active)) {
+			p->u.ingosched.time_slice) % TIMESLICE_GRANULARITY(p)) &&
+			(p->u.ingosched.time_slice >= TIMESLICE_GRANULARITY(p)) &&
+			(p->u.ingosched.array == rq->active)) {
 
 			requeue_task(p, rq->active);
 			set_tsk_need_resched(p);
@@ -2548,7 +2549,7 @@
 	BUG_ON(!array->nr_active);
 
 	p = list_entry(array->queue[sched_find_first_bit(array->bitmap)].next,
-		task_t, run_list);
+		task_t, u.ingosched.run_list);
 
 	for_each_cpu_mask(i, sibling_map) {
 		runqueue_t *smt_rq = cpu_rq(i);
@@ -2562,7 +2563,7 @@
 		 * task from using an unfair proportion of the
 		 * physical cpu's resources. -ck
 		 */
-		if (((smt_curr->time_slice * (100 - sd->per_cpu_gain) / 100) >
+		if (((smt_curr->u.ingosched.time_slice * (100 - sd->per_cpu_gain) / 100) >
 			task_timeslice(p) || rt_task(smt_curr)) &&
 			p->mm && smt_curr->mm && !rt_task(p))
 				ret = 1;
@@ -2572,7 +2573,7 @@
 		 * or wake it up if it has been put to sleep for priority
 		 * reasons.
 		 */
-		if ((((p->time_slice * (100 - sd->per_cpu_gain) / 100) >
+		if ((((p->u.ingosched.time_slice * (100 - sd->per_cpu_gain) / 100) >
 			task_timeslice(smt_curr) || rt_task(p)) &&
 			smt_curr->mm && p->mm && !rt_task(smt_curr)) ||
 			(smt_curr == smt_rq->idle && smt_rq->nr_running))
@@ -2641,8 +2642,8 @@
 
 	schedstat_inc(rq, sched_cnt);
 	now = sched_clock();
-	if (likely(now - prev->timestamp < NS_MAX_SLEEP_AVG))
-		run_time = now - prev->timestamp;
+	if (likely(now - prev->u.ingosched.timestamp < NS_MAX_SLEEP_AVG))
+		run_time = now - prev->u.ingosched.timestamp;
 	else
 		run_time = NS_MAX_SLEEP_AVG;
 
@@ -2716,20 +2717,20 @@
 
 	idx = sched_find_first_bit(array->bitmap);
 	queue = array->queue + idx;
-	next = list_entry(queue->next, task_t, run_list);
+	next = list_entry(queue->next, task_t, u.ingosched.run_list);
 
-	if (!rt_task(next) && next->activated > 0) {
-		unsigned long long delta = now - next->timestamp;
+	if (!rt_task(next) && next->u.ingosched.activated > 0) {
+		unsigned long long delta = now - next->u.ingosched.timestamp;
 
-		if (next->activated == 1)
+		if (next->u.ingosched.activated == 1)
 			delta = delta * (ON_RUNQUEUE_WEIGHT * 128 / 100) / 128;
 
-		array = next->array;
+		array = next->u.ingosched.array;
 		dequeue_task(next, array);
-		recalc_task_prio(next, next->timestamp + delta);
+		recalc_task_prio(next, next->u.ingosched.timestamp + delta);
 		enqueue_task(next, array);
 	}
-	next->activated = 0;
+	next->u.ingosched.activated = 0;
 switch_tasks:
 	if (next == rq->idle)
 		schedstat_inc(rq, sched_goidle);
@@ -2737,14 +2738,14 @@
 	clear_tsk_need_resched(prev);
 	rcu_qsctr_inc(task_cpu(prev));
 
-	prev->sleep_avg -= run_time;
-	if ((long)prev->sleep_avg <= 0)
-		prev->sleep_avg = 0;
-	prev->timestamp = prev->last_ran = now;
+	prev->u.ingosched.sleep_avg -= run_time;
+	if ((long)prev->u.ingosched.sleep_avg <= 0)
+		prev->u.ingosched.sleep_avg = 0;
+	prev->u.ingosched.timestamp = prev->u.ingosched.last_ran = now;
 
 	sched_info_switch(prev, next);
 	if (likely(prev != next)) {
-		next->timestamp = now;
+		next->u.ingosched.timestamp = now;
 		rq->nr_switches++;
 		rq->curr = next;
 		++*switch_count;
@@ -2811,15 +2812,15 @@
 		p->static_prio = NICE_TO_PRIO(nice);
 		goto out_unlock;
 	}
-	array = p->array;
+	array = p->u.ingosched.array;
 	if (array)
 		dequeue_task(p, array);
 
-	old_prio = p->prio;
+	old_prio = p->u.ingosched.prio;
 	new_prio = NICE_TO_PRIO(nice);
 	delta = new_prio - old_prio;
 	p->static_prio = NICE_TO_PRIO(nice);
-	p->prio += delta;
+	p->u.ingosched.prio += delta;
 
 	if (array) {
 		enqueue_task(p, array);
@@ -2844,7 +2845,7 @@
  */
 static int ingo_task_prio(const task_t *p)
 {
-	return p->prio - MAX_RT_PRIO;
+	return p->u.ingosched.prio - MAX_RT_PRIO;
 }
 
 /**
@@ -2886,13 +2887,13 @@
 /* Actually do priority change: must hold rq lock. */
 static void __setscheduler(struct task_struct *p, int policy, int prio)
 {
-	BUG_ON(p->array);
+	BUG_ON(p->u.ingosched.array);
 	p->policy = policy;
 	p->rt_priority = prio;
 	if (policy != SCHED_NORMAL)
-		p->prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
+		p->u.ingosched.prio = MAX_USER_RT_PRIO-1 - p->rt_priority;
 	else
-		p->prio = p->static_prio;
+		p->u.ingosched.prio = p->static_prio;
 }
 
 /**
@@ -2948,10 +2949,10 @@
 		task_rq_unlock(rq, &flags);
 		goto recheck;
 	}
-	array = p->array;
+	array = p->u.ingosched.array;
 	if (array)
 		deactivate_task(p, rq);
-	oldprio = p->prio;
+	oldprio = p->u.ingosched.prio;
 	__setscheduler(p, policy, param->sched_priority);
 	if (array) {
 		__activate_task(p, rq);
@@ -2961,7 +2962,7 @@
 		 * this runqueue and our priority is higher than the current's
 		 */
 		if (task_running(rq, p)) {
-			if (p->prio > oldprio)
+			if (p->u.ingosched.prio > oldprio)
 				resched_task(rq->curr);
 		} else if (TASK_PREEMPTS_CURR(p, rq))
 			resched_task(rq->curr);
@@ -3002,7 +3003,7 @@
 static long ingo_sys_sched_yield(void)
 {
 	runqueue_t *rq = this_rq_lock();
-	prio_array_t *array = current->array;
+	prio_array_t *array = current->u.ingosched.array;
 	prio_array_t *target = rq->expired;
 
 	schedstat_inc(rq, yld_cnt);
@@ -3016,7 +3017,7 @@
 	if (rt_task(current))
 		target = rq->active;
 
-	if (current->array->nr_active == 1) {
+	if (current->u.ingosched.array->nr_active == 1) {
 		schedstat_inc(rq, yld_act_empty);
 		if (!rq->expired->nr_active)
 			schedstat_inc(rq, yld_both_empty);
@@ -3116,9 +3117,9 @@
 	runqueue_t *rq = cpu_rq(cpu);
 	unsigned long flags;
 
-	idle->sleep_avg = 0;
-	idle->array = NULL;
-	idle->prio = MAX_PRIO;
+	idle->u.ingosched.sleep_avg = 0;
+	idle->u.ingosched.array = NULL;
+	idle->u.ingosched.prio = MAX_PRIO;
 	idle->state = TASK_RUNNING;
 	set_task_cpu(idle, cpu);
 
@@ -3220,15 +3221,16 @@
 		goto out;
 
 	set_task_cpu(p, dest_cpu);
-	if (p->array) {
+	if (p->u.ingosched.array) {
 		/*
 		 * Sync timestamp with rq_dest's before activating.
 		 * The same thing could be achieved by doing this step
 		 * afterwards, and pretending it was a local activate.
 		 * This way is cleaner and logically correct.
 		 */
-		p->timestamp = p->timestamp - rq_src->timestamp_last_tick
-				+ rq_dest->timestamp_last_tick;
+		p->u.ingosched.timestamp = p->u.ingosched.timestamp -
+			rq_src->timestamp_last_tick +
+			rq_dest->timestamp_last_tick;
 		deactivate_task(p, rq_src);
 		activate_task(p, rq_dest, 0);
 		if (TASK_PREEMPTS_CURR(p, rq_dest))
@@ -3459,7 +3461,7 @@
 			while (!list_empty(list))
 				migrate_dead(dead_cpu,
 					     list_entry(list->next, task_t,
-							run_list));
+							u.ingosched.run_list));
 		}
 	}
 }
@@ -3997,6 +3999,11 @@
 	runqueue_t *rq;
 	int i, j, k;
 
+	init_task.u.ingosched.prio = MAX_PRIO - 20;
+	init_task.static_prio = MAX_PRIO - 20;
+	INIT_LIST_HEAD(&init_task.u.ingosched.run_list);
+	init_task.u.ingosched.time_slice = HZ;
+
 	for (i = 0; i < NR_CPUS; i++) {
 		prio_array_t *array;
 
@@ -4194,7 +4201,7 @@
 
 		rq = task_rq_lock(p, &flags);
 
-		array = p->array;
+		array = p->u.ingosched.array;
 		if (array)
 			deactivate_task(p, task_rq(p));
 		__setscheduler(p, SCHED_NORMAL, 0);
